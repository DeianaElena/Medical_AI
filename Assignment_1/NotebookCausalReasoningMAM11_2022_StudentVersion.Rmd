---
title: "Casual Inference Notebook"
output: html_notebook
---

In this assignment we will exercise causal inference on the RHC (Right Heart Catheterization) dataset This dataset was used in Connors et al. (1996): The effectiveness of RHC in the initial care of critically ill patients. J American Medical Association 276:889-897. The dataset pertains to day 1 of hospitalization, i.e., the "treatment" variable swang1 is whether or not a patient received a RHC (also called the Swan-Ganz catheter) on the first day in which the patient qualified for a specific research study called SUPPORT.

What should you submit? You need to submit: 1. This Notebook (in .Rmd format) after filling in all your answers inside this notebook. You are required to do an action (write or fill-in code or write documentation) at the places marked by an arrow (=\>). This arrow appears within the comments (and it will be preceeded by "\#") or outside the comments in the text (without "\#"). 2. The Notebook in html format with all the results.

==\> Student names: Elena Deiana, Ramon Cremers ==\> Student numbers: 14110822, 14090945 \# Open part

## Understand the datase:

Please Open <https://hbiostat.org/data/repo/rhc.html> and take a good look at the variables. Make sure you inspect the variables included in the dataset (they appear in a table below the text on the website). You can refer to the table whenever needed.

Packages: install packages if needed

```{r}
# install.packages("tableone")
# install.packages("Matching")
# install.packages("tidyverse") #added
# install.packages('rlang')     #added
# install.packages('Metrics')   #added
# install.packages('ggplot2')   #added

# Recommended: add "dplyr" if you have used it before to make efficient preprocessing.
```

## Load packages

```{r}
library(tableone) # This library allows us to calculate summary statistics about the variables including the SMD (standardized mean difference).
library(Matching) # This library will do the matching
library('dplyr')  #added
library(Metrics)  #added
library(ggplot2)  #added
```

## Read database

```{r}
# Download rhc dataset from CANVAS and read it
rhc <- read.csv("rhc.csv")
set.seed(14)
```

## Understanding the dataset

Tip: Look at the data, use the "View" command on the dataset "rhc"

```{r}
print("===================================================")
# => how many rows does rhc contain?
nr <- nrow(rhc)   #5735 rows
sprintf('The dataset contains %s rows', nr)
# => What are the variable names?
nc <- ncol(rhc)    #74 column 
sprintf('The %s variable names are the following below:', nc)
names(rhc)
print("===================================================")
```

For the assignment the TREATMENT variable is swang1 and the OUTCOME variable is death.

## Preprocess

If you look at the values of the dichotomous variables you will see that they are factors (in other words, categorical variables). We would like to work with numeric variables in this exercise so we need to perform some conversions.

Let's start with the treatment and outcome variables. Create an "outcome.died" and "swang.treatment" variables with the numeric values 0 and 1.

```{r}
rhc$outcome.died <- as.numeric(rhc$death=='Yes')  # Again you might want, instead, to do all preprocessing with dplyr if you prefer
rhc$treatment.swang <- as.numeric(rhc$swang1=='RHC')
```

### Check

Let's take a quick look into the dataset using these variables

```{r}
print("===================================================")
# => What is the mean of outcome.died in the whole data set?
m1 <- mean(rhc$outcome.died)
sprintf('The mean of outcome.died: %s', m1)

# => How many subjects are treated?
s1 <- sum(rhc$swang1=='RHC')
sprintf('The number of treated subjects: %s', s1)

# =>  What is the mean mortality (outcome.died) in this treated group?
m2 <- mean(rhc$swang1=='RHC' & rhc$outcome.died == 1)
sprintf('The mean mortality of treated group: %s ', m2)

# => How many subjects are not treated (i.e. in the control group)?
s3 <- sum(rhc$swang1=='No RHC')
sprintf('The number of not treated subjects: %s', s3)


# => What is the mean mortality in this control group?
m3 <- mean(rhc$swang1=='No RHC' & rhc$outcome.died == 1)
sprintf('Mean mortality for not treated: %s', m3)

print("===================================================")
```

The variable cat1 is a factor (categorical variable) containing the primary disease category for the patient.

```{r}
print("===================================================")
# => How many different categories are in Cat1? Tip: use the levels() function on this variable.
rhc$cat1 <- as.factor(rhc$cat1)  #from chr (character) to factor
ncat <- nlevels(rhc$cat1) 
sprintf("The number of categories in 'cat1' is %s.", ncat)
 
print("===================================================")
```

We would like to "translate" Cat1 into a set of variables each with the numeric values of 0/1. For example for the value of "ARF" we will create a variable "ARF" that will be 1 if rhc\$cat1=='ARF'.

```{r}
rhc$ARF <- as.numeric(rhc$cat1=='ARF')
print("===================================================")
# => Do the same (as we did for ARF) also for all other values of cat1 but ignore the category "Missing".
cat1_list <- list(levels(rhc$cat1))
print(cat1_list)

rhc$CHF <- as.numeric(rhc$cat1=='CHF')
rhc$Cirrhosis <- as.numeric(rhc$cat1=='Cirrhosis')
rhc$Colon_Cancer <- as.numeric(rhc$cat1=='Colon Cancer')
rhc$Coma <- as.numeric(rhc$cat1=='Coma')
rhc$COPD <- as.numeric(rhc$cat1=='COPD')
rhc$Lung_Cancer <- as.numeric(rhc$cat1=='Lung Cancer')
rhc$MOSF_Malignancy <- as.numeric(rhc$cat1=='MOSF w/Malignancy')
rhc$MOSF_Sepsis <- as.numeric(rhc$cat1=='MOSF w/Sepsis')

print("===================================================")
```

## Create data frame with variables of interest

==\> Create a data frame and call it rhc.small with the following variables of interest: All the underlying categories of "cat1" (ARF etc) that you created before, "cardiohx", "chfhx", "dementhx", "psychhx", "chrpulhx", "renalhx", "liverhx", "gibledhx", "malighx", "immunhx", "transhx", "amihx", "age",\
"female", "edu", "das2d3pc", "aps1", "scoma1", "meanbp1", wblc1", "hrt1", "resp1", "temp1", "pafi1", "alb1", "hema1", "bili1", "crea1", "sod1", "pot1", "paco21", "ph1", "wtkilo1", "dnr1", "resp", "card", "neuro", "gastr", "renal", "meta", "hema", "seps", "trauma", "ortho", "adld3p", "urin1", "treatment.swang", "outcome.died".

```{r}
print("===================================================")
print("We already added the 'female' column here instead of later. This way we did not have to alter the original rhc data and also did not have to import the 'sex' column into 'rhc.small'.")
rhc.small <- rhc
rhc.small$female <- as.numeric(rhc.small$sex=='Female')
rhc.small <- rhc.small[,c("ARF", "CHF", "Cirrhosis", "Colon_Cancer", "Coma", "COPD", "Lung_Cancer", "MOSF_Malignancy", "MOSF_Sepsis", "cardiohx", "chfhx", "dementhx", "psychhx",  "chrpulhx", "renalhx", "liverhx",  "gibledhx", "malighx",  "immunhx",  "transhx",  "amihx", "age", "female", "edu", "das2d3pc", "aps1", "scoma1", "meanbp1", "wblc1", "hrt1", "resp1", "temp1", "pafi1", "alb1", "hema1", "bili1", "crea1", "sod1", "pot1",     "paco21", "ph1", "wtkilo1", "dnr1", "resp", "card", "neuro", "gastr",  "renal", "meta", "hema", "seps", "trauma",  "ortho", "adld3p", "urin1", "treatment.swang", "outcome.died")]
print("===================================================")
```

Make sure you take a look at the definition of variables (on the URL given above) with unclear names to you. This data frame includes now all candidate variables from which we will select later for matching. However, before we jump into variable selection let us continue data class conversion.

Check classes of these variables by using the str command

```{r}
str(rhc.small)   #to check classes of all variables
```

If you have "labelled" variables like labelled int and labelled num it means that the datset was imported from SPSS. In such case change such variables into numeric variables. If you do not see such variables then skip this step.

```{r}
#rhc.small$age <- as.numeric(rhc.small$age) # only if in your version of the data "age" was a "labelled" variable
print("===================================================")
# => Change all labelled variables that include numbers (integer or numeric) into numeric.
print("We have not labelled variables, so we can skip this step.")
print("===================================================")
```

For each categorical variable with 2 values (such as sex) change it into a numeric variable by selecting one of the values value (such as "female") to be 1 and the other zero.

```{r}

## rhc.small$female <- as.numeric(rhc.small$sex=='Female')
print("===================================================")
print("We already created the female column earlier because we were required to put it into rhc.small")
#rhc.small$resp<-ifelse(df1$resp=="Yes",1,0)
rhc.small$resp <- as.numeric(rhc.small$resp=='Yes')
rhc.small$card <- as.numeric(rhc.small$card=='Yes')
rhc.small$gastr <- as.numeric(rhc.small$gastr=='Yes')
rhc.small$renal <- as.numeric(rhc.small$renal=='Yes')
rhc.small$hema <- as.numeric(rhc.small$hema=='Yes')
rhc.small$seps <- as.numeric(rhc.small$seps=='Yes')
rhc.small$trauma <- as.numeric(rhc.small$trauma=='Yes')
rhc.small$neuro <- as.numeric(rhc.small$neuro=='Yes')
rhc.small$meta <- as.numeric(rhc.small$meta=='Yes')
rhc.small$ortho <- as.numeric(rhc.small$ortho=='Yes')
rhc.small$dnr1 <- as.numeric(rhc.small$dnr1=='Yes')

# => Change any factor with two values into a 0/1 variable by selecting a value (such as "female" above) to be 1 and the other value will be zero.
print("===================================================")
```

Before we start the analysis we need to be aware of the following issue: If you encounter a variable with missing values then you might run into a problem with the Match() function later on. For this exercise you can follow this advice:

1)  If the proportion of missing is above 15% then just exclude the whole variable from the data frame and hence from the analysis, it is likely that the information it had still resides in the combination of the many other variables.

2)  If you have less than 15% missing variables try to impute the values of the variable. In the extreme case you could apply multiple imputations and repeat the analysis on each imputation set and then pool the results. However for this exercise you can follow a simple strategy such as imputing the value with mean/median/mode. This would be a limitation but imputing missing values is not the focus of our exercise on causal inference.

=\> Inspect the proportion of missing values for each variable and follow the advice above.

```{r}
print("===================================================")
missing <- colMeans(is.na(rhc.small))*100 ##check percentage of NA per column
print(missing)
exclude <- vector(mode = "list", length = 0)
for(colname in names(missing)){
  if(missing[[colname]] > 15){ ## if more than 15% of data is missing:
    cat(missing[[colname]], colname, '\n')
    exclude <- append(exclude, colname) ## contains "adld3p" and "urin1"
  }
}
print('Columns to be removed:')
print(exclude)
rhc.small <- rhc.small[, !colnames(rhc.small) %in% exclude]

print("There are no other columns which have any NA's, so step 2 about imputing doesn't apply")
print("===================================================")
```

Now we are ready to start the analysis.

## Selecting variables to control for

According to the disjunctive cause criterion we want to find variables (to control for) that are predictive of treatment and/or the outcome. In this assignment we will use the criterion to select all (pre-treatment) variables that are predictive of the outcome. This will include true confounders (that also affect the treatment, and risk factors that only affect the outcome). All our variables in rhc.small, except the outcome of course, are pre-treatment variables (none is measured AFTER the intervention) so they are all good candidates for the selection.

The truth is that the best way to select these variables is to use clinical knowledge. However, in this assignment we will do this in a data-driven and practical way. Specifically, we will operationalize "predictive of the outcome" as the following criterion: a variable is predictive of the outcome if its univariate association with the outcome is significant at the 0.1 level. Because our outcome is binary we can translate this criterion to: For every candidate variable in rhc.small fit a logistic regression model to predict outcome (outcome.died) using only the candidate variable as a predictor and retain the variable if the p-value of its association with the outcome is ≤ 0.1, and otherwise discard the variable. Note: Do not include the treatment in the selection.

```{r}
print("===================================================")
# => Fit logistic regression models for each variable separately with the outcome and retain those with an association having a p-value of ≤ 0.1. Tip: you may want to automate the whole process!
xvars <- vector(mode = "list", length = 0)

for(i in seq_along(rhc.small)){
  if(names(rhc.small[i]) == 'treatment.swang' || names(rhc.small[i]) == 'outcome.died'){
    next
  }
  mod <- as.formula(sprintf("outcome.died ~ %s", names(rhc.small[i])))
  model <- glm(formula = mod, family = binomial, data = rhc.small)
  pValue <- coef(summary(model))[,'Pr(>|z|)']
  pValue <- pValue[2]
  if(pValue <= 0.1){
    xvars <- append(xvars, names(rhc.small[i]))
  }
}

# => Put the names of the selected variables (without the treatment and outcome variables) in the variable xvars by writing xvars <- c("ARF", ...) or do it automatically as described in the tip.

# => Put these selected variables in a data.frame called 'rhc.selected' that also includes the treatment and outcome variables to this data frame.
idx <- match(xvars, names(rhc.small))
rhc.selected <- rhc.small[,idx] 
rhc.selected$treatment.swang <- rhc.small$treatment.swang   #adding treatment variables
rhc.selected$outcome.died <- rhc.small$outcome.died         #adding outcome variables
print("===================================================")
```

## Inspect balance before matching

## look at "table 1"

```{r}
xvars <- unlist(xvars)  #to convert it into a vector instead of a list
```

```{r}
print("===================================================")
#=> look at "table 1", complete the command by filling in the "..."

table1<- CreateTableOne(vars=xvars, strata='treatment.swang', data=rhc.selected, test=FALSE) 
## include standardized mean difference (SMD)
#print(table1, smd=TRUE)
## For which variables we do not have a good balance? Look at the slides for a guideline. 

print("As mentioned in slide 102, variables that have a serious imbalance are the ones with SMD > 0.2") 
addmargins(table(ExtractSmd(table1) > 0.2)) #counting number of variables with SMD > 0.2 (=12)    

#creating a dataframe to extract those variables

df_smd <- data.frame(variable = rownames(ExtractSmd(table1)),
                    Unmatched = as.numeric(ExtractSmd(table1)))
print('12 variables have smd > 0.2:')
df_smd$variable[df_smd$Unmatched>0.2]

print("===================================================")
```

## Greedy matching on Mahalanobis distance

Let us now do matching in the hope that the balance will get better so we can have a more valid inference.

```{r}
print("===================================================")
# We want to match the treated with the untreated subjects. Find 1 on 1 match.
# # => Complete the command
treatment <- as.numeric(rhc.selected$treatment.swang==1) 
greedymatch <- Match(Tr=treatment, M=1,X=rhc.selected[xvars], replace=FALSE) 

# read the documentation for Match() by typing ?Match.
# => What does replace=FALSE mean?
print(" 'replace=FALSE' means that the order of matches matters as they will be founded in the same way the data are ordered so this introduces more bias.")
# => What is inside X?
print('Inside X there is the matrix containing the variables we wish to match on.')
rhc.selected[xvars]
print("===================================================")
```

```{r}
?Match #useful to read the documentation for Match()
```

```{r}
print("===================================================")
greedymatch$index.treated # => What do you think this is?
print('This is who are the treated and their ID variable')
greedymatch$index.control # => What do you think this is?
print('This is who are the controls and their ID variable.')
print("===================================================")
```

=\> Are all treated subjects matched to a control one?

```{r}
length(greedymatch$index.treated)
length(greedymatch$index.control)
print("Yes, when checking their length we can see that treated group matched the control ones.")
```

```{r}
matched <- rhc.selected[unlist(greedymatch[c("index.treated","index.control")]), ]
matched[1,]  # This is the first treated subject
print("===================================================")
matched[length(greedymatch$index.treated)+1,] # => What do you think this subject is?
print("untreated. the index = length gives us the last treated person in the list, when our index exceeds the list length it continues with the people where treated = 0") 
print("===================================================")
```

Get table 1 for matched data with standardized differences

```{r}
print("===================================================")
# => Complete the command to create table 1 after matching
matchedtab1 <-  CreateTableOne(vars=xvars, strata='treatment.swang', data=matched, test=FALSE) 
# => Print the balance with smd. 
print(matchedtab1, smd = TRUE)
# => After matching how many "bad" variables with SMD > 0.2 are there?
addmargins(table(ExtractSmd(matchedtab1) > 0.2)) ## 3

print("There are 3 'bad' variables: meanbp1, aps1 and crea1")
print("===================================================")
```

## Outcome analysis

```{r}
print("===================================================")
# => Put here the vector of outcomes y (from matched$outcome.died) of those in the treatment group
y_trt <- matched$outcome.died[matched$treatment==1]
# Put here the vector of outcomes y (from matched$outcome.died) of those in the control group
y_con <- matched$outcome.died[matched$treatment==0]
print("===================================================")
```

## Let's do a paired t-test to see if there are differences in death between the matched groups. This is not the best test because the data is binary but we want to just get a feeling of the differences.

```{r}
diffy <- y_trt-y_con # pairwise difference
t.test(diffy)
```

=\> Look at the p-value. Is it below 0.05?

## We want to use the McNemar test, which is the best choice for paired (matched) binary data

```{r}
print('The p-value is below 0.05.')
table(y_trt,y_con)
# In this table you will see the 2x2 table:  
#         y_con
# ytreat     0     1
#       0    a     b
#       1    c     d

# Enter these a, b, c, and d numbers in the following McNemar test
# print("===================================================")
mcnemar.test(matrix(c(362,336,531,955 ),2,2)) # => Complete the command by filling the a, b, c, and d numbers in the correct order.

# => Is the p-value less than 0.05? What does this result mean?
print("The p-value is below 0.05. This means we reject H0, the treatment did have an effect")
# print("===================================================")
```

## The last way we want to analyse the outcome is with logistic regression

```{r}
gmodel1 <- glm(outcome.died ~ treatment.swang, family=binomial, data=matched)
summary(gmodel1)
exp(coef(gmodel1)) ## 1.47 odds ratio for treatment         
print("===================================================")
# => Is the coefficient of treatment significant? you can use summary(gmodel1) or confint(gmodel1)
print("the coefficient of treatment is significant, since it has a p-value of 9.38e-10")
# =>  What is the odds ratio that is associated with treatment? You can easily derive it from the coefficient (Google otherwise how to get the odds ratio from the coefficient)
print("the odds ratio associated with treatment, calculated by exp(coefficient), is 1.47")
# =>  If the coefficient is positive, what does that mean in terms of odds ratio?
print("A positive coefficient implies that the odds ratio is above 1, in this context meaning treatment is associated with higher outcome of death.")
# => If the coefficient is negative, what does that mean in terms of odds ratio?
print("A negative coefficient implies that the odds ratio is below 1, in this context meaning treatment is associated with lower outcome of death.")

```

```{r}
# The great thing about modelling is that we can now again adjust for the variables with the worst SMD. Adjust for the variable with the worst SMD that you encountered before. To adjust you just add the name of the variable in the logistic regression formula. => Complete the command
gmodel2 <- glm(outcome.died ~ treatment.swang + aps1, family=binomial, data=matched)
exp(coef(gmodel2))
# => What is the odds ratio for treatment now?
print("When adjusting the variable 'aps1', the odds ratio of the treatment is 1.2485")
# print("===================================================")
```

## Propensity score matching

Fit a propensity score model to predict treatment. Use logistic regression with glm

```{r}
print("===================================================")
# => Use the variables we found before in xvars in the first assignment to predict treatment
form <- reformulate(xvars, response = "treatment.swang")
psmodel <- glm(formula=form , family=binomial, data=rhc.selected)  
print("===================================================")
summary(psmodel)   # show coefficients and more details
pscore <- psmodel$fitted.values # create propensity score
#print(pscore)

prs_df <- data.frame(pr_score = predict(psmodel, type = "response"),
                     treatment = psmodel$model$treatment.swang)
#tail(prs_df)
```

# lets check the overlap between the propensity scores for the two groups.

```{r}
print("===================================================")
#  => plot the density of the propensity scores of the treated and untreated. Tip: you might want to use par(mfrow=c(2,1)) to create 2 panels (2 rows, 1 column) and then plotting the first hist, with say, ylim=c(0,650) and col='darkblue', and the second histogram with ylim=c(650, 0) (note that we flipped the order) and col='red'. This will plot the blue histogram as a normal histogram and the red one will be plotted upside down underneath the blue one so you can easily see the overlap.

gotTreatment <- prs_df$pr_score[prs_df$treatment==1]
gotNoTreatment <- prs_df$pr_score[prs_df$treatment==0]

# plot(density(gotTreatment))
# plot(density(gotNoTreatment))

#density hist
par(mfrow=c(2,1))
hist(gotTreatment, main='Treatment group', xlab='Propensity Score', ylim=c(0,3), col='darkblue', 
     prob = TRUE)   #shows density instead of frequency
lines(density(gotTreatment),
      lwd = 2,
      col = "orange")

hist(gotNoTreatment, main='Control group', xlab='Propensity Score', ylim=c(3,0), col='red', 
     prob = TRUE)   #shows density instead of frequency
lines(density(gotNoTreatment),
      lwd = 2,
      col = "orange")

# => What do you think about the overlap? 
print("This overlap is almost everywhere. Although there is little overlap on the high propensity score, the positivity assumption is still reasonable because control group always have a chance of getting treatment.")   

print("===================================================")
```

We want to do greedy matching on PS using Match with a caliper. Note that instead of the probability PS itself, we usually use the log odds (logit) of PS which is log(PS/(1-PS)) which is also equal to log(PS)-log(1-PS) . So let us first write a function (of one line) to calculate the logit.

```{r}
print("===================================================")
# => Complete this function to calculate the log odds
logit <- function(p) {
  return(log(p/(1-p)))
  } 
print("===================================================")
```

# Now it is time to check the positivity assumption

```{r}
print("===================================================")
# => Check the overlap between the propensity scores for the two groups. 
logitTreatment <- logit(gotTreatment)
logitNoTreatment <- logit(gotNoTreatment)

#frequency hist
# par(mfrow=c(2,1))
# hist(logitTreatment, main='Group with treatment', xlab='Log-odds of Propensity Score', xlim=c(-5,5), ylim=c(0,650), col='darkblue')
# hist(logitNoTreatment, main='Group without treatment', xlab='Log-odds of Propensity Score', xlim=c(-5,5),ylim=c(650,0), col='red')

#density hist
par(mfrow=c(2,1))
hist(logitTreatment, main='Treatment group', xlab='Log-odds of Propensity Score', xlim=c(-5,5), ylim=c(0,0.6), col='darkblue', 
     prob = TRUE)   #shows density instead of frequency
lines(density(logitTreatment),
      lwd = 2,
      col = "orange")

hist(logitNoTreatment, main='Control group', xlab='Log-odds of Propensity Score', xlim=c(-5,5),ylim=c(0.6,0), col='red', 
     prob = TRUE)   #shows density instead of frequency
lines(density(logitNoTreatment),
      lwd = 2,
      col = "orange")


#What do you think about the overlap?
print("There is better overlap compared to the previous plot, therefore the positivity assumption is still reasonable.")     
print("===================================================")
```

Now we want to use Match() with rhc.selected\$treatment.swang as treatement, with 1 matched control per treated subject, with X as the logit(pscore), with no replacement and with a caliper of 0.2

```{r}
print("===================================================")
# => complete the command

psmatch <- Match(Tr=rhc.selected$treatment.swang, M=1, X=logit(pscore), replace=FALSE, caliper = .2) 
matched <- rhc.selected[unlist(psmatch[c("index.treated","index.control")]), ]
print("===================================================")
```

## Get standardized differences. Note: xvars is the same as in the first assignment.

```{r}
print("===================================================")
# => Complete the command
print(xvars)
matchedtab1 <- CreateTableOne(vars=xvars, strata = "treatment.swang", data=matched, test = FALSE) 
# => print the matching table showing the SMD
print(matchedtab1, smd = TRUE)

# => Are there any variables with bad balance now?
addmargins(table(ExtractSmd(matchedtab1) > 0.2))
print("Now there aren't any variables with bad balance.")
print("===================================================")
```

## Outcome analysis after propensity score matching

```{r}
print("===================================================")
# => Put here the vector of outcomes y (from matched$outcome.died) of those in the treatment group
y_trt.ps <- matched$outcome.died[matched$treatment==1]   
# Put here the vector of outcomes y (from matched$outcome.died) of those in the control group 
y_con.ps <- matched$outcome.died[matched$treatment==0]
print("===================================================")
```

Perform a t-test

```{r}
# Perform paired t-test. Just to get a feeling of the result
diffy.ps <- y_trt.ps - y_con.ps # pairwise difference
t.test(diffy.ps)
print("===================================================")
# => Is the test significant?
print('The test is significant since p-value=2.855e-11 so below 0.05.')
print("===================================================")
```

Use the McNemar test as before. Enter the a, b, c, and d numbers you get from the 2x2 table in the McNemar test

```{r}
table(y_trt.ps,y_con.ps)  #added to visualize the 2x2 table
```

```{r}
print("===================================================")
# => Complete the command
mcnemar.test(matrix(c(955,531,336,362 ),2,2)) 
# => Is the p-value less than 0.05? What does this result mean?
print("The p-value is 4.44e-11 so less than 0.05.")               
# => Compare the difference in the means before and after matching.
print("The means before matching was 0.0893, While after it is 0.0588, therefore it is now different. This means that there is a smaller difference in the outcome from treated and control group.")          

print("===================================================")
```

Use logistic regression

```{r}
gmodel3 <- glm(outcome.died ~ treatment.swang, family="binomial", data = matched)
summary(gmodel3)
exp(coef(gmodel3))   
print("===================================================")
# => What is the odds ratio of treatment?
print("The odds ratio of treatment is 1.296")              
```

```{r}
# => Now correct also for the least well balanced variable
gmodel3 <- glm(outcome.died ~ treatment.swang + aps1, family="binomial", data = matched)
exp(coef(gmodel3))
# => What is the odds ratio of treatment now?
print("The odds ratio of treatment is now 1.312.")            
print("===================================================")
```

# Open part

Instead of the glm to calculate the propensity score use any machine learning technique you like such as a regression tree (using rpart), or random forests (randomForest) in R to create the propensity score model and to predict the probabilities on such a model. Note that you might need to tweak the model parameters in order to improve it. Compare your results to those you obtained by matching (via the Mahalanobis distance and the propensity score with logistic regression). Which approach do you prefer and why?

```{r}
#1. Propensity score using a different machine learning technique like Random Forest. 

# install.packages('randomForest')      #install package if need it
library(randomForest)

rhc.rf <- rhc.selected        #copying df to work on this
rhc.rf$treatment.swang <- as.factor(rhc.rf$treatment.swang)
rfFormula <- reformulate(xvars, response = "treatment.swang")
RFmodel <- randomForest(formula = rfFormula, data=rhc.rf, ntree=500, max_depth=1000)
forestScores <- predict(RFmodel, type="prob")
#summary(RFmodel)                   #if needed uncomment it
```

```{r}
pscoreRF <- data.frame(forestScores[,2])
comparisonDF <- data.frame("RandomForest"= forestScores[,2], "Treatment" = rhc.selected$treatment.swang)
```

```{r}
#Plotting the propensity scores for the two groups to check the overlap

gotTreatment2 <- comparisonDF$RandomForest[comparisonDF$Treatment==1]
gotNoTreatment2 <- comparisonDF$RandomForest[comparisonDF$Treatment==0]
# par(mfrow=c(2,1))
# hist(gotTreatment2, main='Group with treatment', xlab ='Propensity Score', ylim=c(0,650), col='darkblue')
# hist(gotNoTreatment2, main='Group without treatment', xlab ='Propensity Score', ylim=c(650,0), col='red')


#density hist
par(mfrow=c(2,1))
hist(gotTreatment2, main='Treatment group', xlab='Propensity Score', xlim=c(0,1), ylim=c(0,3), col='darkblue', 
     prob = TRUE)   #shows density instead of frequency
lines(density(gotTreatment2),
      lwd = 2,
      col = "orange")

hist(gotNoTreatment2, main='Control group', xlab='Propensity Score', xlim=c(0,1),ylim=c(3,0), col='red', 
     prob = TRUE)   #shows density instead of frequency
lines(density(gotNoTreatment2),
      lwd = 2,
      col = "orange")

```

```{r}
logitTreatment2 <- logit(gotTreatment2)
logitNoTreatment2 <- logit(gotNoTreatment2)

#density hist
par(mfrow=c(2,1))
hist(logitTreatment2, main='Treatment group', xlab='Log-odds of Propensity Score', xlim=c(-4,4), ylim=c(0,0.6), col='darkblue', 
     prob = TRUE)   #shows density instead of frequency
lines(density(logitTreatment2),
      lwd = 2,
      col = "orange")

hist(logitNoTreatment2, main='Control group', xlab='Log-odds of Propensity Score', xlim=c(-4,4),ylim=c(0.6,0), col='red', 
     prob = TRUE)   #shows density instead of frequency
lines(density(logitNoTreatment2),
      lwd = 2,
      col = "orange")
```

```{r}
pscoreRF_v <- c(pscoreRF[,1])   #creating a vector of the PS
```

```{r}
#matching PS 
psmatch2 <- Match(Tr=rhc.selected$treatment.swang, M=1, X=logit(pscoreRF_v), replace=FALSE, caliper = .2) 
matched2 <- rhc.selected[unlist(psmatch2[c("index.treated","index.control")]), ]
```

```{r}
matchedtab2 <- CreateTableOne(vars=xvars, strata = "treatment.swang", 
                            data=matched2, test = FALSE) 
# Printing the matching table to show the SMD
print(matchedtab2, smd = TRUE)

# Looking for bad balance
addmargins(table(ExtractSmd(matchedtab2) > 0.2))
print('There are no variables with bad balance (SMD>0.2).')
```

```{r}
#2. Compare the results to those previously obtained by matching (via the Mahalanobis distance and the propensity score with logistic regression).
```

```{r}
#calculating mean absolute error between two models

pscoreRF_v <- c(pscoreRF[,1])      ## creating a vector of the PS
predictedRF <- round(pscoreRF_v)   ## predicted values of random forest, propensity score
predictedPS <- round(prs_df$pr_score)
observed <- as.numeric(rhc$swang1=='RHC')
maeRF <- mae(predictedRF, observed)
maePS <- mae(predictedPS, observed)
barplot(c(maeRF, maePS), main="Mean Absolute Error",
   ylim=c(0.2,0.31), names=c('Random Forest method', 'Propensity score method'), col=c('green', 'yellow'))

```

```{r}
smd_dataPlot <- data.frame(variable   = rownames(ExtractSmd(matchedtab1)),
                       LR  = as.numeric(ExtractSmd(matchedtab1)),
                       RF    = as.numeric(ExtractSmd(matchedtab2)))
print(smd_dataPlot)
```

```{r}
print("When comparing the methods (PS method and RF), we can see from Mean Absolute error graph that we get a slightly lower value for Random Forest method. Additionally, when comparing their histograms of propensity score, we noticed a general good overlap with higher density for LR method and, on the other hand, less overlap for high propensity scores when using RF method. Finally, when checking the SMD table from both LR and RF, we noticed that LR has lower SMD so less risk to have 'bad' balance.")

```

```{r}
#3. Which approach do you prefer and why?

print("Although we got better MAE on random forest, we prefer the approach that uses logistic regression for different reasons:
      1. We got slightly better results;
      2. It seems easier to implement; 
      3. It is more popular therefore more documentation and explanation can be found.")         


```
