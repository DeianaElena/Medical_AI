{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-8elwvxUiHzt"
   },
   "source": [
    "The task in this notebook is mortality prediction on 48hrs using physiological variables.\n",
    "\n",
    "The positive class is in-hospital mortality and the negative survival.\n",
    "\n",
    "We define the preprocessing, training, and test classes in python and pytorch, as well as, hyperparameters.\n",
    "You have to complete the code and answer questions for the following exercises. Note: you can use the toy lstm model as an example to comple this notebook.\n",
    "\n",
    "## Exercises\n",
    "\n",
    "1. Understand the data and basic preprocessing\n",
    "   - Read the file `test_text_data_2/train/20/episode1_timeseries_264490.csv` (after having extracted data files from the original archive) using a DataFrame\n",
    "    - What do those data represent? E.g. how many patients? How many variables? Which specific patients? Can you name some specific variables and which kind of variables are (static, temporal, categorical, numerical, etc.)?\n",
    "   - Plot the glucose, heart rate, mean blood pressure, and glasgow coma scale total variables from such file\n",
    "   - On the basis of the previous plot as well as inspecting/displaying the whole data (from the DataFrame) what can you observe? Notably:\n",
    "      - Are there any missing values? For which variables? How could they be handled?\n",
    "      - What can you tell on the scales of the different numerical variables?\n",
    "      - Are there any uncommonly high/low values? What do they represent? How could they be handled?\n",
    "      - Do different elements in the serie (i.e. different measurements of each variable) occur at regular or irregular time intervals? Could this be a problem for a model? Why?\n",
    "      - Hint: you can also use the DataFrame [describe()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.describe.html) method.\n",
    "   - Compare different data preprocessing techniques for different issues\n",
    "    - Print the data after deleting missing values vs imputing them. What do you notice?\n",
    "      - Hint: for deleting missing values you can use the DataFrame [dropna()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.dropna.html) method\n",
    "      - After using `dropna()` on the whole dataframe try also to first remove the capillary refill rate and fraction inspired oxygen variables (`df[df.columns.difference(['Capillary refill rate', 'Fraction inspired oxygen'])]`) \n",
    "      - Use 3 imputation variants mode, mean, and last previous value known. Focus on the variable mean blood pressure for imputation (and comparison)\n",
    "        - Hint: you can use the Dataframe [fillna()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.fillna.html) and [ffill()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.ffill.html)\n",
    "      - **EXTRA** Try also the KNN imputation\n",
    "    - Change the value of mean blood pressure to 650 for the 11th element of the serie (`df.loc[10, 'Mean blood pressure']`) and to 6 for the 12th element of the serie (`df.loc[11, 'Mean blood pressure']`). What these 2 outliers may represent?\n",
    "    - Print the data after deleting outliers for the `Mean blood pressure` (consider outliers values >150 or < 53) vs capping them (both low and high values, at any quantile). What do you notice? Can you cap all the variables? When capping what is happening when you change the upper/lower quantile used to cap the data?\n",
    "2. Understand the code\n",
    "   - Is normalization used in the given code?\n",
    "   - Which imputation technique is used?\n",
    "   - How irregular time intervals are managed in the given code?\n",
    "   - Hint: look at the `timestep` and `imputation` arguments of the `train()` function as well as the `Discretizer` and `Normalizer` instantiation at the beginning of the `train()` function\n",
    "3. For the mortality prediction model \n",
    "   - Implement an architecture based on a LSTM\n",
    "    - We defined the constructor of the classifier (to be implemented by you) with some indications on the layers that you can use in the comments\n",
    "   - Implement the forward pass for the LSTM classifier\n",
    "      - Note: use the mean on the ouputs of the lstm to aggerate features for the classifier       \n",
    "   - Implement the training loop (epochs and minibatches)\n",
    "   - Print the test results for all the metrics\n",
    "   - Plot the training loss \n",
    "   - Plot AUC and calibration curves (for test data) \n",
    "4. Use the last hidden state of the LSTM as aggregation type instead of the mean\n",
    "  - Compare AUC-ROC scores on the test data between LSTM with mean and last hidden state. \n",
    "    - What do you observe when you use the last hidden state instead of the mean of all the outputs of the LSTM? Which one is better? \n",
    "5. Use a GRU layer instead of the LSTM\n",
    "  - Print test results for all metrics\n",
    "  - Plot the AUC and calibration curves (for test data)\n",
    "  - Plot training loss and compare it with LSTM.\n",
    "    - What do you observe, does the GRU converges faster? \n",
    "  - Compare AUC-ROC and AUC-PR scores on test data between LSTM and GRU models\n",
    "    - What do you observe when you use a different sequence model? Which one is better?\n",
    "  - **EXTRA** Plot the AUC and calibration curves for both models together\n",
    "6. Implement model selection for the validation data based on the AUC-ROC score for the LSTM model \n",
    "  - Print test results for all metrics with best model on AUC-ROC score\n",
    "  - Compare the AUC-ROC and AUC-PR scores with the test data for the LSTM model with and without model selection\n",
    "    - What do you observe when you use a model trained given the best performance score in validation?\n",
    "  - **EXTRA** Plot the AUC and calibration curves for both models together\n",
    "  - **EXTRA** Implement an early stopping strategy (with some patience parameter) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8bSMdxdvj2Zd"
   },
   "source": [
    "## Download data\n",
    "\n",
    "Save the MIMIC preprocess data in your google drive.\n",
    "The following code is going to upload it into colab from your drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YQ-CZ0Mph_pP",
    "outputId": "0481dc5f-5b7b-4795-d2fa-593068c6caea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ramon\\Desktop\\Medical_AI\\Assignment_2_mortality_prediction\n"
     ]
    }
   ],
   "source": [
    "#dowload csv files from gdrive\n",
    "#from pydrive.auth import GoogleAuth\n",
    "#from pydrive.drive import GoogleDrive\n",
    "#from google.colab import auth\n",
    "#from oauth2client.client import GoogleCredentials\n",
    "\n",
    "# Authenticate and create the PyDrive client.\n",
    "# This only needs to be done once per notebook.\n",
    "# ask google for authorization \n",
    "#auth.authenticate_user()\n",
    "#gauth = GoogleAuth()\n",
    "#gauth.credentials = GoogleCredentials.get_application_default()\n",
    "#drive = GoogleDrive(gauth)\n",
    "\n",
    "#from google.colab import drive\n",
    "import os\n",
    "print(os.getcwd())\n",
    "#drive.mount('/content/drive', force_remount=True)\n",
    "#%cd '/content/drive/MyDrive/medicalAI'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "71EfVOVVV5j4"
   },
   "outputs": [],
   "source": [
    "#download file from gdrive with the id for share file\n",
    "# create share link for the tar.gz file  and copy id for example:\n",
    "#https://drive.google.com/file/d/1E279yz7ZiZmok6qOYWlrWku1w7F13T5Q/view?usp=sharing\n",
    "#file_id = '1E279yz7ZiZmok6qOYWlrWku1w7F13T5Q' # URL id. \n",
    "#downloaded = drive.CreateFile({'id': file_id})\n",
    "#downloaded.GetContentFile('test_text_data_2.tar.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "F4k9lBdhWTsH"
   },
   "outputs": [],
   "source": [
    "#extract data\n",
    "!tar -xzf test_text_data_2.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "SoiEq4_gZvVT"
   },
   "outputs": [],
   "source": [
    "#upload the discretizer_config.json\n",
    "#this code loads the discretizer_config.json from your hard drive\n",
    "# the config files and mimic utils are in the practical-mortality-utils.zip file\n",
    "#from google.colab import files\n",
    "#uploaded = files.upload()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 73
    },
    "id": "eB4gprWnbcBg",
    "outputId": "668c805e-299e-4fae-9ea1-63ac4a0b90d3"
   },
   "outputs": [],
   "source": [
    "#upload the norm_start_time_zero.normalizer\n",
    "#from google.colab import files\n",
    "#uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kLq6mf6OkPT8"
   },
   "source": [
    "## upload utils \n",
    "**Upload mimic_utils_text.py**\n",
    "For reading csv, normalize data, and missing data imputation.\n",
    "The imputation techinique is using the previous value, there are other imputation methods avilable.\n",
    "\n",
    "To learn more on data normalization see [this tutorial](https://towardsdatascience.com/understand-data-normalization-in-machine-learning-8ff3062101f0) and for imputation see [here](https://medium.com/@Cambridge_Spark/tutorial-introduction-to-missing-data-imputation-4912b51c34eb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 73
    },
    "id": "UGurxaL2jjH9",
    "outputId": "96ce3589-44f5-468d-ae5f-4a8a97b68c75"
   },
   "outputs": [],
   "source": [
    "#upload mimic_utils_text.py\n",
    "#from google.colab import files\n",
    "#uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JueUrtG5EVKA"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GRIHWUoPkVV2"
   },
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "FXZ7HWxhjeOD"
   },
   "outputs": [],
   "source": [
    "#import main python and pytorch libraries\n",
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "from sklearn import metrics\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "import codecs\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import logging\n",
    "import tempfile\n",
    "import shutil\n",
    "import pickle\n",
    "import platform\n",
    "import json\n",
    "from datetime import datetime\n",
    "import random\n",
    "from mimic_utils_text import InHospitalMortalityReader, Discretizer, Normalizer, read_chunk\n",
    "\n",
    "# For the data understanding and preprocessing exercises\n",
    "import pandas as pd\n",
    "\n",
    "# Figures for ROC and calibration curve\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.calibration import calibration_curve\n",
    "import matplotlib.lines as mlines\n",
    "import matplotlib.transforms as mtransforms\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3S1iXwTTmr_l"
   },
   "source": [
    "## Pytorch Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "A8JTbfcomYc4"
   },
   "outputs": [],
   "source": [
    "# pytroch class for reading data into batches\n",
    "class MIMICDataset(Dataset):\n",
    "    \"\"\"\n",
    "       Loads time serie data into memory from a text file,\n",
    "       split by newlines. \n",
    "    \"\"\"\n",
    "    def __init__(self, reader, discretizer, normalizer, target_repl=False, batch_labels=False):\n",
    "        self.data = []\n",
    "        self.y  = []\n",
    "        N = reader.get_number_of_examples()\n",
    "        #if small_part:\n",
    "        #    N = 1000\n",
    "        # read data form cvs files\n",
    "        ret = read_chunk(reader, N)\n",
    "        # read into memory structured data X and labels y\n",
    "        data = ret[\"X\"]\n",
    "        ts = ret[\"t\"]\n",
    "        labels = ret[\"y\"]\n",
    "        names = ret[\"name\"]\n",
    "        # runs discretizer and normalization\n",
    "        data = [discretizer.transform(X, end=t)[0] for (X, t) in zip(data, ts)]\n",
    "        if normalizer is not None:\n",
    "            data = [normalizer.transform(X) for X in data]\n",
    "        self.data = np.array(data, dtype=np.float32)\n",
    "        self.T = self.data.shape[1]\n",
    "        if batch_labels:\n",
    "            self.y = np.array([[l] for l in labels], dtype=np.float32)\n",
    "        else:\n",
    "            self.y = np.array(labels, dtype=np.float32)\n",
    "        if target_repl:\n",
    "            self.y = self._extend_labels(self.y)\n",
    "\n",
    "    def _extend_labels(self, labels):\n",
    "        # (B,)\n",
    "        labels = labels.repeat(self.T, axis=1)  # (B, T)\n",
    "        return labels\n",
    "\n",
    "    def __len__(self):\n",
    "        # overide len to get number of instances\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # get features (physiological variables x) and label for a given instance index\n",
    "        return self.data[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ikv2wdqEmxrZ"
   },
   "source": [
    "## Evaluation metrics\n",
    "The main measure used are [accuracy](https://developers.google.com/machine-learning/crash-course/classification/accuracy), [ROC AUC and PR AUC](https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-imbalanced-classification/).\n",
    "\n",
    "To know more on evaluating prognostic models you can watch [this](https://www.youtube.com/watch?v=B4dFjWEpRnY&list=PLx7eKEyZ1cjTfr-SDKkwmuYmrrgryjTC0&index=2) video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "8qVNiqW1nJa7"
   },
   "outputs": [],
   "source": [
    "# eval metrics\n",
    "def print_metrics_binary(y_true, predictions, logging, verbose=1):\n",
    "    predictions = np.array(predictions)\n",
    "    if len(predictions.shape) == 1:\n",
    "        predictions = np.stack([1 - predictions, predictions]).transpose((1, 0))\n",
    "    cf = metrics.confusion_matrix(y_true, predictions.argmax(axis=1))\n",
    "    if verbose:\n",
    "        logging.info(\"confusion matrix:\")\n",
    "        logging.info(cf)\n",
    "    cf = cf.astype(np.float32)\n",
    "\n",
    "    acc = (cf[0][0] + cf[1][1]) / np.sum(cf)\n",
    "    prec0 = cf[0][0] / (cf[0][0] + cf[1][0])\n",
    "    prec1 = cf[1][1] / (cf[1][1] + cf[0][1])\n",
    "    rec0 = cf[0][0] / (cf[0][0] + cf[0][1])\n",
    "    rec1 = cf[1][1] / (cf[1][1] + cf[1][0])\n",
    "    auroc = metrics.roc_auc_score(y_true, predictions[:, 1])\n",
    "\n",
    "    (precisions, recalls, thresholds) = metrics.precision_recall_curve(y_true, predictions[:, 1])\n",
    "    auprc = metrics.auc(recalls, precisions)\n",
    "    minpse = np.max([min(x, y) for (x, y) in zip(precisions, recalls)])\n",
    "\n",
    "    if verbose:\n",
    "        logging.info(\"accuracy = {0:.3f}\".format(acc))\n",
    "        logging.info(\"precision class 0 = {0:.3f}\".format(prec0))\n",
    "        logging.info(\"precision class 1 = {0:.3f}\".format(prec1))\n",
    "        logging.info(\"recall class 0 = {0:.3f}\".format(rec0))\n",
    "        logging.info(\"recall class 1 = {0:.3f}\".format(rec1))\n",
    "        logging.info(\"AUC of ROC = {0:.3f}\".format(auroc))\n",
    "        logging.info(\"AUC of PRC = {0:.3f}\".format(auprc))\n",
    "       \n",
    "\n",
    "    return {\"acc\": acc,\n",
    "            \"prec0\": prec0,\n",
    "            \"prec1\": prec1,\n",
    "            \"rec0\": rec0,\n",
    "            \"rec1\": rec1,\n",
    "            \"auroc\": auroc,\n",
    "            \"auprc\": auprc}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q77q3NfHmmMU"
   },
   "source": [
    "## Understanding the data and basic preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "SCu2rMk3pCMm",
    "outputId": "9716a0f5-e81e-4260-aade-6d02d7964e8d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1d848e5dd60>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Add your code here (add any number of cell you may need)\n",
    "# Plot some variables\n",
    "df = pd.DataFrame()\n",
    "df = pd.read_csv('test_text_data_2/train/20/episode1_timeseries_264490.csv')\n",
    "x = df['Hours']\n",
    "y = df[['Glucose', 'Heart Rate', 'Mean blood pressure', 'Glascow coma scale total']]\n",
    "\n",
    "# plot glucose\n",
    "plt.plot(x,y['Glucose'], marker='x', label='Glucose')\n",
    "# Add more plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "9lLp18E_6IWD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This dataset contains 51 rows of data, and 18 columns/variables.\n",
      "Judging by the data, as well as the file structure of our dataset, this is data collected of one patient (patient 20)\n",
      "during their first stay (episode 1) at the ICU.\n",
      "We see that most data is numerical, with numerous \"rates\" such as eart- and respiratory rate being measured.\n",
      "There is some categorical data as well, where the category represents a scale (Glascow coma scale).\n",
      "These scales, which are numeric, are given an explanatory annotation in text form.\n",
      "The \"hours\" variable is temporal data. Height can be considered static data.\n",
      "Weight could potentially be numeric, but since we only see one measurement in this dataset we assume it is static.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hours</th>\n",
       "      <th>Capillary refill rate</th>\n",
       "      <th>Diastolic blood pressure</th>\n",
       "      <th>Fraction inspired oxygen</th>\n",
       "      <th>Glascow coma scale eye opening</th>\n",
       "      <th>Glascow coma scale motor response</th>\n",
       "      <th>Glascow coma scale total</th>\n",
       "      <th>Glascow coma scale verbal response</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>Heart Rate</th>\n",
       "      <th>Height</th>\n",
       "      <th>Mean blood pressure</th>\n",
       "      <th>Oxygen saturation</th>\n",
       "      <th>Respiratory rate</th>\n",
       "      <th>Systolic blood pressure</th>\n",
       "      <th>Temperature</th>\n",
       "      <th>Weight</th>\n",
       "      <th>pH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.743333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>215.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.126667</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>190.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.190000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>178.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.290000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>123.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.990000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>59.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>80.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>79.000000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>36.000002</td>\n",
       "      <td>93.300003</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2.240000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1 No Response</td>\n",
       "      <td>1 No Response</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0 ET/Trach</td>\n",
       "      <td>NaN</td>\n",
       "      <td>80.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>36.000002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2.406667</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>90.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2.490000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>62.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>80.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>35.900002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2.740000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>59.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>80.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>77.000000</td>\n",
       "      <td>97.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>36.000002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2.990000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>60.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>80.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>79.000000</td>\n",
       "      <td>97.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>35.900002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3.240000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>52.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>80.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>95.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>36.000002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>3.490000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>53.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>80.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>67.000000</td>\n",
       "      <td>96.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>36.000002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>3.740000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>78.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>36.100002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>3.990000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>42.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1 No Response</td>\n",
       "      <td>1 No Response</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0 ET/Trach</td>\n",
       "      <td>NaN</td>\n",
       "      <td>80.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>95.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>36.100002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>4.240000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>70.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>80.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>144.0</td>\n",
       "      <td>36.000002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>4.490000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>45.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>130.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>99.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>36.000002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>4.740000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>76.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>80.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>98.000000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>156.0</td>\n",
       "      <td>36.100002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>4.990000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>80.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>99.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>36.000002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>5.240000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>62.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>80.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>36.100002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>5.490000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>62.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>137.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>82.000000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>136.0</td>\n",
       "      <td>36.200002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>5.673333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>193.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>99.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>5.740000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>62.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>80.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>83.000000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>136.0</td>\n",
       "      <td>36.200002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>5.990000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>53.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>80.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>123.0</td>\n",
       "      <td>36.299999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>6.490000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>57.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>80.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>81.000000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>137.0</td>\n",
       "      <td>36.700001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>6.923333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>199.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>98.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>6.990000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>56.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>80.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>79.000000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>133.0</td>\n",
       "      <td>36.799999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>7.490000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>70.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>80.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>102.000000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>167.0</td>\n",
       "      <td>36.700001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>7.740000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>67.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>80.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>156.0</td>\n",
       "      <td>36.799999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>7.990000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>53.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4 Spontaneously</td>\n",
       "      <td>6 Obeys Commands</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.0 ET/Trach</td>\n",
       "      <td>217.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>68.000000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>8.240000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>47.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>80.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>37.099999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>8.990000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>62.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>193.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>161.0</td>\n",
       "      <td>37.200001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>9.240000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>62.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>80.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>89.000000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>161.0</td>\n",
       "      <td>37.099999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>9.990000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>60.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>181.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>81.000000</td>\n",
       "      <td>98.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>144.0</td>\n",
       "      <td>37.300000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>10.073333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>99.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>10.990000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>56.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>80.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>79.000000</td>\n",
       "      <td>96.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>145.0</td>\n",
       "      <td>37.200001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>11.223333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>150.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>11.240000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>52.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>164.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>71.000000</td>\n",
       "      <td>97.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>37.200001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>11.990000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>58.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4 Spontaneously</td>\n",
       "      <td>6 Obeys Commands</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5 Oriented</td>\n",
       "      <td>NaN</td>\n",
       "      <td>80.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>83.000000</td>\n",
       "      <td>96.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>151.0</td>\n",
       "      <td>37.200001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>12.990000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>55.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>131.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>79.000000</td>\n",
       "      <td>96.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>145.0</td>\n",
       "      <td>37.200001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>13.073333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>80.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>96.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>37.200001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>13.906667</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>77.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>13.990000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>59.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>128.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>85.000000</td>\n",
       "      <td>97.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>153.0</td>\n",
       "      <td>37.200001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>14.990000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>122.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>15.990000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4 Spontaneously</td>\n",
       "      <td>6 Obeys Commands</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5 Oriented</td>\n",
       "      <td>99.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>71.000000</td>\n",
       "      <td>97.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>131.0</td>\n",
       "      <td>37.200001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>16.990000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>45.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>97.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>95.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>109.0</td>\n",
       "      <td>37.099999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>17.990000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>54.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>76.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>82.000000</td>\n",
       "      <td>98.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>138.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>18.990000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>43.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>74.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>69.000000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>19.990000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>36.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4 Spontaneously</td>\n",
       "      <td>6 Obeys Commands</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5 Oriented</td>\n",
       "      <td>138.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>60.333302</td>\n",
       "      <td>98.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>109.0</td>\n",
       "      <td>36.000002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>20.990000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>38.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>67.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>63.333302</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>21.990000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>41.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>78.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>68.000000</td>\n",
       "      <td>99.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>22.990000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>41.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>80.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>62.666698</td>\n",
       "      <td>99.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Hours  Capillary refill rate  Diastolic blood pressure  \\\n",
       "0   -0.743333                    NaN                       NaN   \n",
       "1   -0.126667                    NaN                       NaN   \n",
       "2    0.190000                    NaN                       NaN   \n",
       "3    1.290000                    NaN                       NaN   \n",
       "4    1.990000                    NaN                      59.0   \n",
       "5    2.240000                    NaN                      50.0   \n",
       "6    2.406667                    NaN                       NaN   \n",
       "7    2.490000                    NaN                      62.0   \n",
       "8    2.740000                    NaN                      59.0   \n",
       "9    2.990000                    NaN                      60.0   \n",
       "10   3.240000                    NaN                      52.0   \n",
       "11   3.490000                    NaN                      53.0   \n",
       "12   3.740000                    NaN                      50.0   \n",
       "13   3.990000                    NaN                      42.0   \n",
       "14   4.240000                    NaN                      70.0   \n",
       "15   4.490000                    NaN                      45.0   \n",
       "16   4.740000                    NaN                      76.0   \n",
       "17   4.990000                    NaN                      51.0   \n",
       "18   5.240000                    NaN                      62.0   \n",
       "19   5.490000                    NaN                      62.0   \n",
       "20   5.673333                    NaN                       NaN   \n",
       "21   5.740000                    NaN                      62.0   \n",
       "22   5.990000                    NaN                      53.0   \n",
       "23   6.490000                    NaN                      57.0   \n",
       "24   6.923333                    NaN                       NaN   \n",
       "25   6.990000                    NaN                      56.0   \n",
       "26   7.490000                    NaN                      70.0   \n",
       "27   7.740000                    NaN                      67.0   \n",
       "28   7.990000                    NaN                      53.0   \n",
       "29   8.240000                    NaN                      47.0   \n",
       "30   8.990000                    NaN                      62.0   \n",
       "31   9.240000                    NaN                      62.0   \n",
       "32   9.990000                    NaN                      60.0   \n",
       "33  10.073333                    NaN                       NaN   \n",
       "34  10.990000                    NaN                      56.0   \n",
       "35  11.223333                    NaN                       NaN   \n",
       "36  11.240000                    NaN                      52.0   \n",
       "37  11.990000                    NaN                      58.0   \n",
       "38  12.990000                    NaN                      55.0   \n",
       "39  13.073333                    NaN                      50.0   \n",
       "40  13.906667                    NaN                       NaN   \n",
       "41  13.990000                    NaN                      59.0   \n",
       "42  14.990000                    NaN                       NaN   \n",
       "43  15.990000                    NaN                      50.0   \n",
       "44  16.990000                    NaN                      45.0   \n",
       "45  17.990000                    NaN                      54.0   \n",
       "46  18.990000                    NaN                      43.0   \n",
       "47  19.990000                    NaN                      36.0   \n",
       "48  20.990000                    NaN                      38.0   \n",
       "49  21.990000                    NaN                      41.0   \n",
       "50  22.990000                    NaN                      41.0   \n",
       "\n",
       "    Fraction inspired oxygen Glascow coma scale eye opening  \\\n",
       "0                        NaN                            NaN   \n",
       "1                        NaN                            NaN   \n",
       "2                        NaN                            NaN   \n",
       "3                        NaN                            NaN   \n",
       "4                        NaN                            NaN   \n",
       "5                        NaN                  1 No Response   \n",
       "6                        NaN                            NaN   \n",
       "7                        NaN                            NaN   \n",
       "8                        NaN                            NaN   \n",
       "9                        NaN                            NaN   \n",
       "10                       NaN                            NaN   \n",
       "11                       NaN                            NaN   \n",
       "12                       NaN                            NaN   \n",
       "13                       NaN                  1 No Response   \n",
       "14                       NaN                            NaN   \n",
       "15                       NaN                            NaN   \n",
       "16                       NaN                            NaN   \n",
       "17                       NaN                            NaN   \n",
       "18                       NaN                            NaN   \n",
       "19                       NaN                            NaN   \n",
       "20                       NaN                            NaN   \n",
       "21                       NaN                            NaN   \n",
       "22                       NaN                            NaN   \n",
       "23                       NaN                            NaN   \n",
       "24                       NaN                            NaN   \n",
       "25                       NaN                            NaN   \n",
       "26                       NaN                            NaN   \n",
       "27                       NaN                            NaN   \n",
       "28                       NaN                4 Spontaneously   \n",
       "29                       NaN                            NaN   \n",
       "30                       NaN                            NaN   \n",
       "31                       NaN                            NaN   \n",
       "32                       NaN                            NaN   \n",
       "33                       NaN                            NaN   \n",
       "34                       NaN                            NaN   \n",
       "35                       NaN                            NaN   \n",
       "36                       NaN                            NaN   \n",
       "37                       NaN                4 Spontaneously   \n",
       "38                       NaN                            NaN   \n",
       "39                       NaN                            NaN   \n",
       "40                       NaN                            NaN   \n",
       "41                       NaN                            NaN   \n",
       "42                       NaN                            NaN   \n",
       "43                       NaN                4 Spontaneously   \n",
       "44                       NaN                            NaN   \n",
       "45                       NaN                            NaN   \n",
       "46                       NaN                            NaN   \n",
       "47                       NaN                4 Spontaneously   \n",
       "48                       NaN                            NaN   \n",
       "49                       NaN                            NaN   \n",
       "50                       NaN                            NaN   \n",
       "\n",
       "   Glascow coma scale motor response  Glascow coma scale total  \\\n",
       "0                                NaN                       NaN   \n",
       "1                                NaN                       NaN   \n",
       "2                                NaN                       NaN   \n",
       "3                                NaN                       NaN   \n",
       "4                                NaN                       NaN   \n",
       "5                      1 No Response                       3.0   \n",
       "6                                NaN                       NaN   \n",
       "7                                NaN                       NaN   \n",
       "8                                NaN                       NaN   \n",
       "9                                NaN                       NaN   \n",
       "10                               NaN                       NaN   \n",
       "11                               NaN                       NaN   \n",
       "12                               NaN                       NaN   \n",
       "13                     1 No Response                       3.0   \n",
       "14                               NaN                       NaN   \n",
       "15                               NaN                       NaN   \n",
       "16                               NaN                       NaN   \n",
       "17                               NaN                       NaN   \n",
       "18                               NaN                       NaN   \n",
       "19                               NaN                       NaN   \n",
       "20                               NaN                       NaN   \n",
       "21                               NaN                       NaN   \n",
       "22                               NaN                       NaN   \n",
       "23                               NaN                       NaN   \n",
       "24                               NaN                       NaN   \n",
       "25                               NaN                       NaN   \n",
       "26                               NaN                       NaN   \n",
       "27                               NaN                       NaN   \n",
       "28                  6 Obeys Commands                      11.0   \n",
       "29                               NaN                       NaN   \n",
       "30                               NaN                       NaN   \n",
       "31                               NaN                       NaN   \n",
       "32                               NaN                       NaN   \n",
       "33                               NaN                       NaN   \n",
       "34                               NaN                       NaN   \n",
       "35                               NaN                       NaN   \n",
       "36                               NaN                       NaN   \n",
       "37                  6 Obeys Commands                      15.0   \n",
       "38                               NaN                       NaN   \n",
       "39                               NaN                       NaN   \n",
       "40                               NaN                       NaN   \n",
       "41                               NaN                       NaN   \n",
       "42                               NaN                       NaN   \n",
       "43                  6 Obeys Commands                      15.0   \n",
       "44                               NaN                       NaN   \n",
       "45                               NaN                       NaN   \n",
       "46                               NaN                       NaN   \n",
       "47                  6 Obeys Commands                      15.0   \n",
       "48                               NaN                       NaN   \n",
       "49                               NaN                       NaN   \n",
       "50                               NaN                       NaN   \n",
       "\n",
       "   Glascow coma scale verbal response  Glucose  Heart Rate  Height  \\\n",
       "0                                 NaN    215.0         NaN     NaN   \n",
       "1                                 NaN    190.0         NaN     NaN   \n",
       "2                                 NaN    178.0         NaN     NaN   \n",
       "3                                 NaN    123.0         NaN     NaN   \n",
       "4                                 NaN      NaN        80.0     NaN   \n",
       "5                        1.0 ET/Trach      NaN        80.0     NaN   \n",
       "6                                 NaN     90.0         NaN     NaN   \n",
       "7                                 NaN      NaN        80.0     NaN   \n",
       "8                                 NaN      NaN        80.0     NaN   \n",
       "9                                 NaN      NaN        80.0     NaN   \n",
       "10                                NaN      NaN        80.0     NaN   \n",
       "11                                NaN      NaN        80.0     NaN   \n",
       "12                                NaN      NaN        78.0     NaN   \n",
       "13                       1.0 ET/Trach      NaN        80.0     NaN   \n",
       "14                                NaN      NaN        80.0     NaN   \n",
       "15                                NaN    130.0        80.0     NaN   \n",
       "16                                NaN      NaN        80.0     NaN   \n",
       "17                                NaN      NaN        80.0     NaN   \n",
       "18                                NaN      NaN        80.0     NaN   \n",
       "19                                NaN    137.0        80.0     NaN   \n",
       "20                                NaN    193.0         NaN     NaN   \n",
       "21                                NaN      NaN        80.0     NaN   \n",
       "22                                NaN      NaN        80.0     NaN   \n",
       "23                                NaN      NaN        80.0     NaN   \n",
       "24                                NaN    199.0         NaN     NaN   \n",
       "25                                NaN      NaN        80.0     NaN   \n",
       "26                                NaN      NaN        80.0     NaN   \n",
       "27                                NaN      NaN        80.0     NaN   \n",
       "28                       1.0 ET/Trach    217.0        80.0     NaN   \n",
       "29                                NaN      NaN        80.0     NaN   \n",
       "30                                NaN    193.0        80.0     NaN   \n",
       "31                                NaN      NaN        80.0     NaN   \n",
       "32                                NaN    181.0        80.0     NaN   \n",
       "33                                NaN      NaN         NaN     NaN   \n",
       "34                                NaN      NaN        80.0     NaN   \n",
       "35                                NaN    150.0         NaN     NaN   \n",
       "36                                NaN    164.0        80.0     NaN   \n",
       "37                         5 Oriented      NaN        80.0     NaN   \n",
       "38                                NaN    131.0        80.0     NaN   \n",
       "39                                NaN      NaN        80.0     NaN   \n",
       "40                                NaN      NaN        77.0     NaN   \n",
       "41                                NaN    128.0        80.0     NaN   \n",
       "42                                NaN    122.0         NaN     NaN   \n",
       "43                         5 Oriented     99.0        80.0     NaN   \n",
       "44                                NaN     97.0        80.0     NaN   \n",
       "45                                NaN      NaN        76.0     NaN   \n",
       "46                                NaN      NaN        74.0     NaN   \n",
       "47                         5 Oriented    138.0        74.0     NaN   \n",
       "48                                NaN      NaN        67.0     NaN   \n",
       "49                                NaN      NaN        78.0     NaN   \n",
       "50                                NaN      NaN        80.0     NaN   \n",
       "\n",
       "    Mean blood pressure  Oxygen saturation  Respiratory rate  \\\n",
       "0                   NaN                NaN               NaN   \n",
       "1                   NaN                NaN               NaN   \n",
       "2                   NaN                NaN               NaN   \n",
       "3                   NaN                NaN               NaN   \n",
       "4             79.000000              100.0              12.0   \n",
       "5             65.000000              100.0              11.0   \n",
       "6                   NaN                NaN               NaN   \n",
       "7             75.000000              100.0              10.0   \n",
       "8             77.000000               97.0              10.0   \n",
       "9             79.000000               97.0              10.0   \n",
       "10            65.000000               95.0              10.0   \n",
       "11            67.000000               96.0              10.0   \n",
       "12            62.000000              100.0              10.0   \n",
       "13            52.000000               95.0              10.0   \n",
       "14            92.000000              100.0              10.0   \n",
       "15            57.000000               99.0              10.0   \n",
       "16            98.000000              100.0              10.0   \n",
       "17            65.000000               99.0              12.0   \n",
       "18            86.000000              100.0              11.0   \n",
       "19            82.000000              100.0              20.0   \n",
       "20                  NaN               99.0               NaN   \n",
       "21            83.000000              100.0              18.0   \n",
       "22            72.000000              100.0              25.0   \n",
       "23            81.000000              100.0              26.0   \n",
       "24                  NaN               98.0               NaN   \n",
       "25            79.000000              100.0              16.0   \n",
       "26           102.000000              100.0              16.0   \n",
       "27            92.000000              100.0              17.0   \n",
       "28            68.000000              100.0              22.0   \n",
       "29            60.000000              100.0              16.0   \n",
       "30            90.000000              100.0              11.0   \n",
       "31            89.000000              100.0              18.0   \n",
       "32            81.000000               98.0              19.0   \n",
       "33                  NaN               99.0               NaN   \n",
       "34            79.000000               96.0               0.0   \n",
       "35                  NaN                NaN               NaN   \n",
       "36            71.000000               97.0              16.0   \n",
       "37            83.000000               96.0              16.0   \n",
       "38            79.000000               96.0              15.0   \n",
       "39            70.000000               96.0              23.0   \n",
       "40                  NaN                NaN               NaN   \n",
       "41            85.000000               97.0              19.0   \n",
       "42                  NaN                NaN               NaN   \n",
       "43            71.000000               97.0              18.0   \n",
       "44            61.000000               95.0              11.0   \n",
       "45            82.000000               98.0              20.0   \n",
       "46            69.000000              100.0              13.0   \n",
       "47            60.333302               98.0              22.0   \n",
       "48            63.333302                NaN              19.0   \n",
       "49            68.000000               99.0              27.0   \n",
       "50            62.666698               99.0              21.0   \n",
       "\n",
       "    Systolic blood pressure  Temperature     Weight    pH  \n",
       "0                       NaN          NaN        NaN  7.43  \n",
       "1                       NaN          NaN        NaN  7.43  \n",
       "2                       NaN          NaN        NaN  7.38  \n",
       "3                       NaN          NaN        NaN  7.34  \n",
       "4                     121.0    36.000002  93.300003   NaN  \n",
       "5                     103.0    36.000002        NaN   NaN  \n",
       "6                       NaN          NaN        NaN  7.43  \n",
       "7                     130.0    35.900002        NaN   NaN  \n",
       "8                     126.0    36.000002        NaN   NaN  \n",
       "9                     129.0    35.900002        NaN   NaN  \n",
       "10                    103.0    36.000002        NaN   NaN  \n",
       "11                    107.0    36.000002        NaN   NaN  \n",
       "12                     99.0    36.100002        NaN   NaN  \n",
       "13                     81.0    36.100002        NaN   NaN  \n",
       "14                    144.0    36.000002        NaN   NaN  \n",
       "15                     90.0    36.000002        NaN   NaN  \n",
       "16                    156.0    36.100002        NaN   NaN  \n",
       "17                    108.0    36.000002        NaN   NaN  \n",
       "18                    140.0    36.100002        NaN   NaN  \n",
       "19                    136.0    36.200002        NaN   NaN  \n",
       "20                      NaN          NaN        NaN  7.48  \n",
       "21                    136.0    36.200002        NaN   NaN  \n",
       "22                    123.0    36.299999        NaN   NaN  \n",
       "23                    137.0    36.700001        NaN   NaN  \n",
       "24                      NaN          NaN        NaN  7.46  \n",
       "25                    133.0    36.799999        NaN   NaN  \n",
       "26                    167.0    36.700001        NaN   NaN  \n",
       "27                    156.0    36.799999        NaN   NaN  \n",
       "28                    121.0    37.000000        NaN   NaN  \n",
       "29                    100.0    37.099999        NaN   NaN  \n",
       "30                    161.0    37.200001        NaN   NaN  \n",
       "31                    161.0    37.099999        NaN   NaN  \n",
       "32                    144.0    37.300000        NaN   NaN  \n",
       "33                      NaN          NaN        NaN  7.38  \n",
       "34                    145.0    37.200001        NaN   NaN  \n",
       "35                      NaN          NaN        NaN   NaN  \n",
       "36                    128.0    37.200001        NaN  7.36  \n",
       "37                    151.0    37.200001        NaN   NaN  \n",
       "38                    145.0    37.200001        NaN   NaN  \n",
       "39                    129.0    37.200001        NaN   NaN  \n",
       "40                      NaN          NaN        NaN   NaN  \n",
       "41                    153.0    37.200001        NaN   NaN  \n",
       "42                      NaN          NaN        NaN   NaN  \n",
       "43                    131.0    37.200001        NaN   NaN  \n",
       "44                    109.0    37.099999        NaN   NaN  \n",
       "45                    138.0          NaN        NaN   NaN  \n",
       "46                    121.0          NaN        NaN   NaN  \n",
       "47                    109.0    36.000002        NaN   NaN  \n",
       "48                    114.0          NaN        NaN   NaN  \n",
       "49                    122.0          NaN        NaN   NaN  \n",
       "50                    106.0          NaN        NaN   NaN  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "print(f'This dataset contains {df.shape[0]} rows of data, and {df.shape[1]} columns/variables.')\n",
    "print('Judging by the data, as well as the file structure of our dataset, this is data collected of one patient (patient 20)\\n' \n",
    "       'during their first stay (episode 1) at the ICU.')\n",
    "print('We see that most data is numerical, with numerous \"rates\" such as eart- and respiratory rate being measured.\\n' \n",
    "      'There is some categorical data as well, where the category represents a scale (Glascow coma scale).\\n' \n",
    "      'These scales, which are numeric, are given an explanatory annotation in text form.')\n",
    "print('The \"hours\" variable is temporal data. Height can be considered static data.\\n' \n",
    "      'Weight could potentially be numeric, but since we only see one measurement in this dataset we assume it is static.')\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "P1q8XK30ith4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We see that there are 3 columns which have no data, name \"Capillary refill rate\", \"Fraction inspired oxygen\" and \"height\". We can not utilize these columns\n",
      "Other columns such as \"Mean Blood Pressure\" have a few NANs, which could be imputed\n",
      "The variables use different scales, since they represent different things.\n",
      "A variable like heartrate will have a larger scale and more variance than for example body temperature.\n",
      "There is some data which has a large variance, such as glucose for example. Since they represent measurements we believe they should simply be handled as they are.\n",
      "They happen irregularly, as can be seen by the NaN values for different columns at different times.\n",
      "We see for example that Mean blood Pressure and temperature get almost hourly measurements. Coma scale measurements are much less frequent.\n",
      "This could be a problem since, for example, variables which could be strongly correlated may not be measured at the same time.\n",
      "In such a case these correlations could get lost, even when using a method such as imputation.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hours</th>\n",
       "      <th>Capillary refill rate</th>\n",
       "      <th>Diastolic blood pressure</th>\n",
       "      <th>Fraction inspired oxygen</th>\n",
       "      <th>Glascow coma scale total</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>Heart Rate</th>\n",
       "      <th>Height</th>\n",
       "      <th>Mean blood pressure</th>\n",
       "      <th>Oxygen saturation</th>\n",
       "      <th>Respiratory rate</th>\n",
       "      <th>Systolic blood pressure</th>\n",
       "      <th>Temperature</th>\n",
       "      <th>Weight</th>\n",
       "      <th>pH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>51.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>40.00000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>8.568431</td>\n",
       "      <td>NaN</td>\n",
       "      <td>54.500000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.333333</td>\n",
       "      <td>153.750000</td>\n",
       "      <td>79.121951</td>\n",
       "      <td>NaN</td>\n",
       "      <td>75.058333</td>\n",
       "      <td>98.476190</td>\n",
       "      <td>15.250000</td>\n",
       "      <td>127.82500</td>\n",
       "      <td>36.545715</td>\n",
       "      <td>93.300003</td>\n",
       "      <td>7.41000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>6.160175</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.055385</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.887841</td>\n",
       "      <td>39.837664</td>\n",
       "      <td>2.461657</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.734453</td>\n",
       "      <td>1.742413</td>\n",
       "      <td>5.673827</td>\n",
       "      <td>20.86942</td>\n",
       "      <td>0.539794</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.04717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-0.743333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>67.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>95.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>81.00000</td>\n",
       "      <td>35.900002</td>\n",
       "      <td>93.300003</td>\n",
       "      <td>7.34000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.865000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>126.750000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>97.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>109.00000</td>\n",
       "      <td>36.000002</td>\n",
       "      <td>93.300003</td>\n",
       "      <td>7.38000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>6.990000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>54.500000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>144.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>76.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>129.00000</td>\n",
       "      <td>36.299999</td>\n",
       "      <td>93.300003</td>\n",
       "      <td>7.43000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>12.490000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>60.500000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>190.750000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>82.250000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>144.00000</td>\n",
       "      <td>37.150000</td>\n",
       "      <td>93.300003</td>\n",
       "      <td>7.43000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>22.990000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>76.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>217.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>102.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>167.00000</td>\n",
       "      <td>37.300000</td>\n",
       "      <td>93.300003</td>\n",
       "      <td>7.48000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Hours  Capillary refill rate  Diastolic blood pressure  \\\n",
       "count  51.000000                    0.0                 40.000000   \n",
       "mean    8.568431                    NaN                 54.500000   \n",
       "std     6.160175                    NaN                  9.055385   \n",
       "min    -0.743333                    NaN                 36.000000   \n",
       "25%     3.865000                    NaN                 50.000000   \n",
       "50%     6.990000                    NaN                 54.500000   \n",
       "75%    12.490000                    NaN                 60.500000   \n",
       "max    22.990000                    NaN                 76.000000   \n",
       "\n",
       "       Fraction inspired oxygen  Glascow coma scale total     Glucose  \\\n",
       "count                       0.0                  6.000000   20.000000   \n",
       "mean                        NaN                 10.333333  153.750000   \n",
       "std                         NaN                  5.887841   39.837664   \n",
       "min                         NaN                  3.000000   90.000000   \n",
       "25%                         NaN                  5.000000  126.750000   \n",
       "50%                         NaN                 13.000000  144.000000   \n",
       "75%                         NaN                 15.000000  190.750000   \n",
       "max                         NaN                 15.000000  217.000000   \n",
       "\n",
       "       Heart Rate  Height  Mean blood pressure  Oxygen saturation  \\\n",
       "count   41.000000     0.0            40.000000          42.000000   \n",
       "mean    79.121951     NaN            75.058333          98.476190   \n",
       "std      2.461657     NaN            11.734453           1.742413   \n",
       "min     67.000000     NaN            52.000000          95.000000   \n",
       "25%     80.000000     NaN            65.000000          97.000000   \n",
       "50%     80.000000     NaN            76.000000          99.000000   \n",
       "75%     80.000000     NaN            82.250000         100.000000   \n",
       "max     80.000000     NaN           102.000000         100.000000   \n",
       "\n",
       "       Respiratory rate  Systolic blood pressure  Temperature     Weight  \\\n",
       "count         40.000000                 40.00000    35.000000   1.000000   \n",
       "mean          15.250000                127.82500    36.545715  93.300003   \n",
       "std            5.673827                 20.86942     0.539794        NaN   \n",
       "min            0.000000                 81.00000    35.900002  93.300003   \n",
       "25%           10.000000                109.00000    36.000002  93.300003   \n",
       "50%           16.000000                129.00000    36.299999  93.300003   \n",
       "75%           19.000000                144.00000    37.150000  93.300003   \n",
       "max           27.000000                167.00000    37.300000  93.300003   \n",
       "\n",
       "            pH  \n",
       "count  9.00000  \n",
       "mean   7.41000  \n",
       "std    0.04717  \n",
       "min    7.34000  \n",
       "25%    7.38000  \n",
       "50%    7.43000  \n",
       "75%    7.43000  \n",
       "max    7.48000  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEGCAYAAACevtWaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nOzdeVyU1f7A8c8BRjZBRRAXVNTc2UQUFRcMl+qaaertpqVm3crKyrZr9tO8lqXtm2V6M1tMr93UtDT3XVNx38UFFRdAEGRfz++PgQkEBBQcmPm+Xy9eM3Pmmef5Do5fzpznPN+jtNYIIYSwLDbmDkAIIUTFk+QuhBAWSJK7EEJYIEnuQghhgSS5CyGEBbIzdwAA7u7u2tvb29xhCCFEtbJnz56rWmuP4p6rEsnd29ub8PBwc4chhBDVilLqXEnPybCMEEJYIEnuQghhgSS5CyGEBaoSY+5CVFVZWVlERUWRnp5u7lCEFXNwcMDLywuDwVDm10hyF+ImoqKicHFxwdvbG6WUucMRVkhrTVxcHFFRUTRr1qzMr5NhGSFuIj09nbp160piF2ajlKJu3brl/vYoyV2IUkhiF+Z2K59BSe5CCGGBJLkLUQ1ER0czfPhwmjdvTseOHenatStLlixh48aNDBgwwNzhiSqoWif3UzFJPPFdOOfiUswdihCVRmvNoEGD6NmzJ2fOnGHPnj0sXLiQqKgoc4cmqrBqndwdDLasPRbNbwcvmzsUISrN+vXrqVGjBk8//bSprWnTpowbN67QdlOmTOGDDz4wPfbx8SEyMhKA77//Hj8/P/z9/Xn00UcBOHfuHGFhYfj5+REWFsb58+cB+Pnnn/Hx8cHf35+ePXsCkJOTw6uvvkqnTp3w8/Pj66+/rsy3LCpAtZ4K6VXHiY5N67D8wCWe7X2XucMRFu7fy49w9NL1Ct1nu4auvHl/+5tuc+TIEQIDA2/5GEeOHGHatGls27YNd3d34uPjAXjuuecYOXIko0aNYu7cuTz//PMsXbqUqVOnsmrVKho1akRCQgIA33zzDbVq1WL37t1kZGQQEhJCv379yjU1T9xZ1brnDjDArwHHryRxKibJ3KEIcUc8++yz+Pv706lTpzJtv379eoYOHYq7uzsAbm5uAOzYsYPhw4cD8Oijj7J161YAQkJCGD16NHPmzCEnJweA1atX8/333xMQEEBwcDBxcXFERERU9FsTFaha99wB/ubbgKm/HWX5gcuM7+ti7nCEBSuth11Z2rdvzy+//GJ6PHPmTK5evUpQUFCh7ezs7MjNzTU9zp8XrbUu01S6/G1mzZrFzp07+f333wkICGD//v1orfn888/p379/RbwlcQdU+557PVcHujSry/KDl9BamzscISrc3XffTXp6Ol999ZWpLTU1tch23t7e7N27F4C9e/dy9uxZAMLCwli0aBFxcXEApmGZbt26sXDhQgDmz59P9+7dATh9+jTBwcFMnToVd3d3Lly4QP/+/fnqq6/IysoC4OTJk6SkyESGqqza99wB7vdvyMQlhzh6+TrtG9YydzhCVCilFEuXLmX8+PG89957eHh44OzszIwZMwptN2TIENPQSadOnWjVqhVg7Pm/8cYb9OrVC1tbWzp06MC8efP47LPPGDNmDO+//z4eHh58++23ALz66qtERESgtSYsLAx/f3/8/PyIjIwkMDAQrTUeHh4sXbr0jv8uRNmpqtDbDQoK0rezWEd8Siadpq3lyZ7N+dc9bSowMmHtjh07Rtu2bc0dhhDFfhaVUnu01kHFbV/th2UA3Jxr0P0ud5YfkKEZIYQAC0nuYByaibqWxv4LCeYORQghzM5iknu/9p7UsLVh+QG5oEkIISwmubs6GOjV2oPfD10iN1eGZoQQ1q3U5K6UaqyU2qCUOqaUOqKUeiGv3U0ptUYpFZF3W6fAa15XSp1SSp1QSt2xibH3+zck+noGuyPj79QhhRCiSipLzz0beFlr3RboAjyrlGoHTADWaa1bAuvyHpP33D+A9sA9wJdKKdvKCP5GYW3q4WCwYfnBS3ficEIIUWWVmty11pe11nvz7icBx4BGwAPAd3mbfQcMyrv/ALBQa52htT4LnAI6V3TgxXG2tyOsrScrD10hOye39BcIUQ3UrFmz0ON58+bx3HPPVci+IyMj+emnn0p8ztHRkYCAANq1a8fIkSNNFzGVZOPGjWzfvr1CYhO3p1xj7kopb6ADsBPw1FpfBuMfAKBe3maNgAsFXhaV13bjvp5USoUrpcJjY2PLH3kJ7vdrSFxKJjvOxFXYPoWwRNnZ2TdN7gAtWrRg//79HDp0iKioKBYtWnTTfUpyrzrKnNyVUjWBX4AXtdY3K41XXBGLImc4tdaztdZBWusgDw+PsoZRqtDWHtS0t2P5ARmaEZYvNjaWIUOG0KlTJzp16sS2bdsA2LVrF926daNDhw5069aNEydOAMZe/7Bhw7j//vvp168fEyZMYMuWLQQEBPDxxx+XeBxbW1s6d+7MxYsXAVi+fDnBwcF06NCBPn36EB0dTWRkJLNmzeLjjz8mICCALVu2lBifqHxlKj+glDJgTOzztdaL85qjlVINtNaXlVINgJi89iigcYGXewF3LNM6GGzp186TPw5f4e1BvtSws5gJQcLcVk6AK4cqdp/1feHe6TfdJC0tjYCAANPj+Ph4Bg4cCMALL7zA+PHj6d69O+fPn6d///4cO3aMNm3asHnzZuzs7Fi7di0TJ040FR/bsWMHBw8exM3NjY0bN/LBBx/w22+/3TSG9PR0du7cyaeffgpA9+7d+fPPP1FK8Z///If33nuPDz/8kKeffpqaNWvyyiuvADB8+PBi4xOVr9Tkroyl4r4BjmmtPyrw1DJgFDA97/bXAu0/KaU+AhoCLYFdFRl0ae73b8jifRfZEhFLWFvPO3loISqco6Mj+/fvNz2eN28e+eU61q5dy9GjR03PXb9+naSkJBITExk1ahQREREopQqNlfft29dU9rc0p0+fJiAggIiICIYOHYqfnx8AUVFRPPTQQ1y+fJnMzMwS67qXFJ+Li1RwrWxl6bmHAI8Ch5RS+Z+wiRiT+iKl1OPAeWAYgNb6iFJqEXAU40ybZ7XWORUe+c0Cvsud2k4Glh+4JMldVJxSetjmkJuby44dO3B0dCzUPm7cOHr37s2SJUuIjIwkNDTU9Jyzs3OZ958/5n758mVCQ0NZtmwZAwcOZNy4cbz00ksMHDiQjRs3MmXKlHLFJypfWWbLbNVaK621n9Y6IO9nhdY6TmsdprVumXcbX+A107TWLbTWrbXWKyv3LRRVw86Ge9rXZ83RaNKz7ujflWrrj8NXOHwx0dxhiHLq168fX3zxhelxfg8/MTGRRo2M8xjmzZtX4utdXFxISip9oZsGDRowffp03n333SL7/+6770zb3bi/kuITlc9iB6Tv929ISmYOG47HlL6xYMLigywKv1D6hqJK+eyzzwgPD8fPz4927doxa9YsAF577TVef/11QkJCTKspFcfPzw87Ozv8/f1vekIVYNCgQaSmprJlyxamTJnCsGHD6NGjh2mFJ4D777+fJUuWmE6olhSfqHwWUfK3ODm5muB31tG5WR2+HNGxQvdtiVr930oeC/Hm9XulvG1BUvJXVBVWWfK3OLY2ivt867PuWAzJGdnmDqdKy8rJJTM7F+caFrF2ixACC07uAAP9G5KRncuqw1fMHUqVlppp/NruVOOOVIkQQtwBFp3cOzatQxM3J5bsu2juUKq0NFNyl567EJbCopO7UorBHRqx7fRVLiemmTucKisl0zhs5WwvPXchLIVFJ3eABwMboTUs3SflCEqSmiE9dyEsjcUn96Z1nQlqWofFe6NkfdUSpOb13GXMXQjLYfHJHWBwYCMiYpI5culm9c6sl5xQrdqUUjz66KOmx9nZ2Xh4eDBgwIBKPW5oaCjFTVGuyJLDN5YzFhXHKpL7AN+G1LC14Ze9UeYOpUpKlROqVZqzszOHDx8mLc143mjNmjWmq0Ot2c0uzqrOx6ooVpHcazkZ6NOuHsv2XyJLFvEoIkWGZaq8e++9l99//x2ABQsW8PDDD5ueS0lJYcyYMXTq1IkOHTrw66/GGn6RkZH06NGDwMBAAgMDTXXWN27cSGhoKEOHDqVNmzaMGDGixCHLH3/8kW7duuHj48OuXUXr/507d46wsDD8/PwICwvj/PnzN20/e/YsXbt2pVOnTkyaNKnYY0ZGRtKmTRtGjRqFn58fQ4cOJTU1FQBvb2+mTp1K9+7d+fnnn1m9ejVdu3YlMDCQYcOGkZycDMCECRNo164dfn5+pgqVP//8Mz4+Pvj7+9OzZ0+g6LeQAQMGsHHjRsD4rWLy5MkEBwezY8cOfvzxRzp37kxAQABPPfVUlU/4VtNVe7CDFysOXWHzSakUeaPUjPzZMlbzcbglM3bN4Hj88QrdZxu3Nvyr879K3e4f//gHU6dOZcCAARw8eJAxY8awZcsWAKZNm8bdd9/N3LlzSUhIoHPnzvTp04d69eqxZs0aHBwciIiI4OGHHzYNs+zbt48jR47QsGFDQkJC2LZtG927dy9y3JSUFLZv387mzZsZM2YMhw8fLvT8c889x8iRIxk1ahRz587l+eefZ+nSpSW2v/DCC4wdO5aRI0cyc+bMEt/viRMn+OabbwgJCWHMmDF8+eWXpiTt4ODA1q1buXr1Kg8++CBr167F2dmZGTNm8NFHH/Hcc8+xZMkSjh8/jlKKhIQEAKZOncqqVato1KiRqe1mUlJS8PHxYerUqRw7dowZM2awbds2DAYDzzzzDPPnz2fkyJGl7sdcrKLnDtCrtQduzjVYLHPei0jNkjH3qs7Pz4/IyEgWLFjAfffdV+i51atXM336dAICAggNDSU9PZ3z58+TlZXFP//5T3x9fRk2bFih0rudO3fGy8sLGxsbAgICiIyMLPa4+d8QevbsyfXr14skxR07djB8+HAAHn30UbZu3XrT9m3btpn2WfA8wo0aN25MSEgIAI888ojp9QAPPfQQAH/++SdHjx4lJCSEgIAAvvvuO86dO4erqysODg488cQTLF68GCcnJwBCQkIYPXo0c+bMKVOv29bWliFDhgCwbt069uzZQ6dOnQgICGDdunWcOXOm1H2Yk9V01Qy2Ngz0b8hPu86TmJZFLUeDuUOqMlIzcrBRYC8Lm9xUWXrYlWngwIG88sorbNy4kbi4v5aR1Frzyy+/0Lp160LbT5kyBU9PTw4cOEBubi4ODg6m5+zt7U33bW1tyc4uvkSHcTmHkh+Xtn1x7aXto7Tj5pcs1lrTt29fFixYUOT1u3btYt26dSxcuJAvvviC9evXM2vWLHbu3Mnvv/9OQEAA+/fvx87Ojtzcv4Zq09PTTfcdHBywtbU1HWvUqFGmqpjVgVX9b34goCGZ2blSKfIGKZnZONewK9N/OmE+Y8aMYfLkyfj6+hZq79+/P59//rlp3Hzfvn2AsSxvgwYNsLGx4YcffrilMeL//ve/AGzdupVatWpRq1atQs9369aNhQsXAjB//nzT0E5J7SEhIYXaS3L+/Hl27NgBGM8xFDdk1KVLF7Zt28apU6cASE1N5eTJkyQnJ5OYmMh9993HJ598YiozfPr0aYKDg5k6dSru7u5cuHABb29v9u/fT25uLhcuXCj2vAJAWFgY//vf/4iJMeaO+Ph4zp07V9qvz6yspucO4OdVmzpOBjafjGVQB5ltkC8tMwdHGZKp8ry8vHjhhReKtE+aNIkXX3wRPz8/tNZ4e3vz22+/8cwzzzBkyBB+/vlnevfuXa5FOvLVqVOHbt26cf36debOnVvk+c8++4wxY8bw/vvv4+HhwbfffnvT9k8//ZThw4fz6aefmoY8itO2bVu+++47nnrqKVq2bMnYsWOLbOPh4cG8efN4+OGHycjIAODtt9/GxcWFBx54gPT0dLTWplLGr776KhEREWitCQsLw9/fH4BmzZrh6+uLj48PgYGBxcbTrl073n77bfr160dubi4Gg4GZM2fStGnTcvw27yyLLflbknEL9rHjdBy7JoZhYyM9VTD+Tg5fTGTDK6HmDqXKkZK/d15kZCQDBgwocvLW2knJ31L0auXB1eQMjl2RC5rypWZky8lUISyM1SX3ni2Nq8ZsPnnVzJFUHamZOZLcRZXh7e0tvfYKYHXJvZ6rA23qu7D5ZKy5Q6kyUjOz5epUISyM1SV3MA7NhJ+LJ0VWaAKMPXcp9yuEZbHK5N6zlQdZOZo/z8SVvrEVSM3MwdEgPXchLIlVJvcg7zo4GmxlaCZPSma29NyFsDBWmdzt7Wzp0tyNTZLcgfwTqtJzr6qio6MZPnw4zZs3p2PHjnTt2pUlS5YAxiJglV3619qUtwzxO++8U+o2CQkJfPnll5Vy/JJYZXIH49BMZFwq5+NSzR2KWWXl5JKZnSuzZaoorTWDBg2iZ8+enDlzhj179rBw4UKioqR8dVVR0cm9olh1cgfYFGHdvXdZqKNqW79+PTVq1ODpp582tTVt2pRx48YV2XbXrl1069aNDh060K1bN06cOAHAkSNHTKVq/fz8iIiIAOD777/Hz88Pf39/UxGv4kr15uTk0Lx5c7TWJCQkYGNjw+bNmwHo0aOH6fL/fDk5Obzyyiv4+vri5+fH559/DhiLb3Xo0AFfX1/GjBljuqrU29ubiRMn0rVrV4KCgti7dy/9+/enRYsWzJo1C4Dk5GTCwsIIDAzE19fXVNb4xuOOHj0aHx8ffH19TVemnjp1ij59+uDv709gYCCnT58u0/4A3n//fTp16oSfnx9vvvlmkecnTJhAWloaAQEBjBgxAoCPPvoIHx8ffHx8+OSTT0zbnT59moCAAF599dUyH/92WO138ebuzjSq7cjmk7E82qXqXkJc2dLykruU+y3Z9bQsUjKzUTM/JuNYxZb8tW/bhvoTJ5b4/JEjR0q8JP5Gbdq0YfPmzdjZ2bF27VomTpzIL7/8wqxZs3jhhRcYMWIEmZmZ5OTkcOTIEaZNm8a2bdtwd3cnPj4eKLmEb6tWrTh69Chnz56lY8eObNmyheDgYKKiorjrrrsKxTF79mzOnj3Lvn37sLOzIz4+nvT0dEaPHs26deto1aoVI0eO5KuvvuLFF18EjFUgd+zYwfjx4xk9ejTbtm0jPT2d9u3b8/TTT+Pg4MCSJUtwdXXl6tWrdOnShYEDBxaqh7R//34uXrxomiOfX8FyxIgRTJgwgcGDB5Oenk5ubi41atQodX+rV68mIiKCXbt2obVm4MCBbN682VQLHmD69Ol88cUXpvo1e/bs4dtvv2Xnzp1orQkODqZXr15Mnz6dw4cPm7bLzs4u9fi3y2p77koperbyYMfpOKtewEMW6ihdSmY2ccmZ5g4DgGeffRZ/f386depU5LnExESGDRuGj48P48eP58iRIwB07dqVd955hxkzZnDu3DkcHR1Zv349Q4cOxd3deFGfm5sbUHKp3h49erB582Y2b97M66+/ztatW9m9e3excaxdu5ann34aOzs7075PnDhBs2bNaNWqFQCjRo0y9f7BWPESwNfXl+DgYFxcXPDw8MDBwYGEhAS01kycOBE/Pz/69OnDxYsXiY6OLnTc5s2bc+bMGcaNG8cff/yBq6srSUlJXLx4kcGDBwPGSo9OTk5l2t/q1atZvXo1HTp0IDAwkOPHj5u+9ZRk69atDB48GGdnZ2rWrMmDDz5oqrtfUFmOf7usurvWq5UHC3adZ++5awQ3r2vucMwiTZbYK1V++aWb9bArS/v27fnll19Mj2fOnMnVq1cJCipaTmTSpEn07t2bJUuWEBkZSWhoKADDhw8nODiY33//nf79+/Of//wHrXW5Su/26NGDWbNmcenSJaZOncr777/Pxo0bC/Vi8xW379JqWOWXILaxsSlUjtjGxobs7Gzmz59PbGwse/bswWAw4O3tXag8LxiLnB04cIBVq1Yxc+ZMFi1aZBoWuVFZ9qe15vXXX+epp566aezleZ/lOf7tstqeO0C3u+pia6PYbMXj7vkXcknP/ebMVQ357rvvJj09na+++srUlr/k3I0SExNNa6vOmzfP1H7mzBmaN2/O888/z8CBAzl48CBhYWEsWrTIVBc+f1impFK9wcHBbN++HRsbGxwcHAgICODrr7+mR48eReLo168fs2bNMtWIj4+Pp02bNkRGRprG53/44Qd69epV5t9DYmIi9erVw2AwsGHDhmLL7V69epXc3FyGDBnCW2+9xd69e3F1dcXLy4ulS5cCkJGRQWpqapn2179/f+bOnWtauu/ixYumkr8FGQwGsrKyAOOiJkuXLiU1NZWUlBSWLFlCjx49cHFxISkpqVzv53ZZdXJ3dTAQ2KS2VdeZkROqpdNaozBPdldKsXTpUjZt2kSzZs3o3Lkzo0aNYsaMGUW2fe2113j99dcJCQkpVLv9v//9Lz4+PgQEBHD8+HFGjhxJ+/bteeONN+jVqxf+/v689NJLgLFU77fffoufnx8//PADn376KWDsWTdu3JguXboAxp58UlJSkdryAE888QRNmjQxnaz96aefcHBw4Ntvv2XYsGH4+vpiY2NT6CRxaUaMGEF4eDhBQUHMnz+fNm3aFNnm4sWLhIaGEhAQwOjRo00La/zwww989tln+Pn50a1bN65cuVKm/fXr14/hw4fTtWtXfH19GTp0aKEEne/JJ5/Ez8+PESNGEBgYyOjRo+ncuTPBwcE88cQTdOjQgbp16xISEoKPjw+vvvpqmY5/u6yu5O+NPl8XwUdrTxL+Rh/q1rQv/QUW5veDl3n2p72sHt+TVp4u5g6nyjl27Bgu9ZuSlJ5N2wau5g5HWDEp+VtOPVt5oDVsPWWdvff8E6qOBum5l6QK9H+EKDerT+4+jWpRx8lgtVerpuaNuctUyJuTFQhFdWP1yd3WRtG9pQebT14lN9f6umipWTLmXprcXPONuQsBZZ+FU5DVJ3cwLuBhraszpWbkYKPA3k4+CsVxcHAgNSlBxmaE2WitiYuLw8HBoVyvk+/i/FWKYPPJq7RvWKuUrS1LamYOzjXsKvTKOEvi5eXFij8P41ojhtyE8v3nEqKiODg44OXlVa7XlJrclVJzgQFAjNbaJ69tCvBPIH+geqLWekXec68DjwM5wPNa61XlisgMPAuszjQ2tIW5w7mjUjOzcZJyvyUyGAwsO51NdFI6v43rYO5whCizsnwXnwfcU0z7x1rrgLyf/MTeDvgH0D7vNV8qpapF5uhppaszpUi531Jl5WpsbWTYSlQvpX5itdabgfgy7u8BYKHWOkNrfRY4BXS+jfjumF5WujpTWma2nEwtRU5uLnY2Mmwlqpfb6Y48p5Q6qJSaq5Sqk9fWCLhQYJuovLYilFJPKqXClVLhsbHmn4ZoraszpWQYx9xFyeo41aCei/Vd4Caqt1tN7l8BLYAA4DLwYV57cd2bYqcZaK1na62DtNZBHh4etxhGxclfnWlzhHVdzJSamY2j9Nxv6ovhgXz1SEdzhyFEudxSctdaR2utc7TWucAc/hp6iQIaF9jUC7h0eyHeOT1beXD2agoX4q1ndabUzBxZP1UIC3RLyV0p1aDAw8HA4bz7y4B/KKXslVLNgJbArtsL8c4xrc5kRUMzsn6qEJapLFMhFwChgLtSKgp4EwhVSgVgHHKJBJ4C0FofUUotAo4C2cCzWuuc4vZbFRVcnekRK1mdKUVOqAphkUpN7lrrh4tp/uYm208Dpt1OUOaSvzrT8gOXyMrJxWBr+dPfpOcuhGWy/OxVTr1auZOckc3ec9fMHUqly87JJTM7F2fpuQthcSS536DbXe5WszpTftEwmS0jhOWR5H4DVwcDHRpbx+pMqRnG5C7lfoWwPJLci9GzlQeHLyUSl5xh7lAqVf5CHXJCVQjLI8m9GNayOlOaaf1U6bkLYWkkuRfD10pWZ8ovkiYnVIWwPJLci5G/OtOWiKu3tAJKdaGB+q4OuDgYzB2KEKKCSXIvQc+W7sQmZXDscpK5Q6k0XZrX5c+JYfh6WdcCJUJYA0nuJTCtzmQFUyKFEJZHknsJCq7OJIQQ1Y0k95vo2cqD8MhrVrc6kxCi+pPkfhM9W3qQmZNrdaszCSGqP0nuNxHkXQcHg40MzQghqh1J7jfhYLClS/O6Vrc6k6i6MrNz+XX/RTKzc80diqjiJLmXomdL61udSVRd64/H8MLC/YTMWM8na08Sm2TZJTLErZPkXgprXJ1JVF392nny/ZjO+DR05ZO1EYRMX89Li/ZzKCrR3KGJKkaKipSihYf1rc4kqi4bG+OCMj1beXAmNpnvd5zj5/ALLN57kaCmdRgd4s097etjZwULzYibk09AKfJXZ9p+Oo6sHBnnFFVHc4+aTBnYnh0Tw5g0oB0xSRk899M+ery3gS83nuJaSqa5QxRmJMm9DPJXZ9p3PsHcoQhRhKuDgce7N2PDK6H8Z2QQzT2cee+PE3R5dx0TfjnI8SvXzR2iMAMZlimDbne509jNkXjpCYkqzNZG0aedJ33aeXLiShLztkeyZF8UC3dfoHdrD756pCMOBqkAai1UVah6GBQUpMPDw80dhhAW51pKJt/vOMfHa0/yUt9WPB/W0twhiQqklNqjtQ4q7jkZlhHCgtVxrsELfVryN78GzNxwSqb0WhFJ7kJYgf/7W1tsbRT/Xn7E3KGIO0SSuxBWoEEtR17s05K1x2JYezTa3OGIO0CSuxBW4rGQZrSsV5Mpy4+QnpVj7nBEJZPkLoSVMNjaMPUBH6KupfHlhlPmDkdUMknuQliRri3qMiigIbM2neHs1RRzhyMqkSR3IazMxPvaYm9nw5vLjlj0AvDWTpK7EFamnqsD4/u2YvPJWFYduWLucEQlkeQuhBUa2bUpbeq7MHX5UVIzZRlJSyTJXQgrZGdrw9uDfLiUmM7n6+XkqiWS5C6ElQrydmNoRy/+s+UMp2KSzR2OqGCS3IWwYhPubYOjwZY3lx2Wk6sWRpK7EFbMvaY9r/ZvzbZTcfx+6LK5wxEVSJK7EFZueHBTfBq58tZvR0nOkJOrlkKSuxBWztZG8dYDPkRfz+CzdRHmDkdUEEnuQgg6NKnDP8DkN3UAACAASURBVDo1Zu7Ws5yMTjJ3OKIClJrclVJzlVIxSqnDBdrclFJrlFIRebd1Cjz3ulLqlFLqhFKqf2UFLqqP65nXuZws47lV3Wv3tKGmgx2TlsrJVUtQlp77POCeG9omAOu01i2BdXmPUUq1A/4BtM97zZdKKVnXy8q9/efbPPz7w2TlZpk7FHETbs41+Nc9bdh5Np5f918ydzjiNpWa3LXWm4H4G5ofAL7Lu/8dMKhA+0KtdYbW+ixwCuhcQbGKaigrJ4vNUZuJS49j5+Wd5g5HlOKhoMb4N67NtBXHuJ4uf4yrs1sdc/fUWl8GyLutl9feCLhQYLuovLYilFJPKqXClVLhsbGxtxiGqOp2R+8mJctYfXDl2ZVmjkaUxsZG8fYDPlxNzuC1nw+y8tBljl2+Tlqm1H+vbuwqeH+qmLZiB++01rOB2WBcILuC47B6qyNX09ilMW3rtjVrHBsvbMTB1oHejXuz7vw6JmVPwsHOwawxiZvz9arF2F4t+HLjaf4oUFisvqsD3u5ONHN3xruuM955t03rOuFgkNHXquZWk3u0UqqB1vqyUqoBEJPXHgU0LrCdFyCDd2bw9p9v06VBF97r9Z7ZYtBas+nCJro06MKgloNYGbmSLRe30LdpX7PFJMrmtXvaMDa0BefiUjl7NYXIqymcjTPerj4STVxKpmlbpaCBqwPe7s50b+nOM6F3mTFyke9Wk/syYBQwPe/21wLtPymlPgIaAi2BXbcbpCif7NxsEjISiE4171qZEQkRXEq5xD/9/knn+p1xc3Bj5dmVktyrCRcHAz6NauHTqFaR5xLTsjgXl5KX+FOJjEshMi6FC/FpZohUFKfU5K6UWgCEAu5KqSjgTYxJfZFS6nHgPDAMQGt9RCm1CDgKZAPPaq1lsO4OS8hIQKOJSY0pfeNKtPHCRgB6efXCzsaO/t79WRyxmOTMZGrWqGnW2MTtqeVowM+rNn5etc0diihBWWbLPKy1bqC1NmitvbTW32it47TWYVrrlnm38QW2n6a1bqG1bq21ljNoZnAt/RoAMakxZp2vvOnCJnzq+uDh5AHAfc3uIyMngw0XNpgtJiGshVyhaoHi041/azNzM0nMSDRLDFfTrnLo6iF6Ne5lavP38Kehc0NWnF1hlpiEsCaS3KuxE/En6LGwB1dSCi+Vlp/cAbONu2+J2oJGE9o41NSmlOKeZvfw56U/Sc6U+uFCVCZJ7tXYsfhjJGQkEHGtcLGngsndXOPuGy5soL5zfVrXaV2o3d/Dn2ydTeT1yFvar9aaqKQouTxeiFJIcq/G8pP41bSrxbaDeZJ7Rk4Gf17+k15evVCq8KUPTV2bAnDu+rlb2veSU0u4d/G9hP0cxuRtk1l7bi3ZuVKmVogbSXKvxuLTjEn8xgQenx6PSw0X43Npdz6577y8k7TstEJDMvm8XLxQKM5fP39L+/711K80dG5IoGcga8+tZfzG8Ty55skif+CEsHaS3Kux/B56bFrh8g3xafF4Onni5uBmlp77pgubcLRzpFP9TkWes7e1p4FzA84llb/nfjn5Mntj9jK45WA+6PUBm/+xmandpnIw9iB/X/539kTvqYjwhbAIktyrsbj0OKDosMy1jGu4Objh6eRZJLlHp0Sz/PTyShuz1lqzMWojIQ1DsLe1L3abJq5NbqnnvipyFWCcUglgZ2PH4JaDmX/ffBztHHl81ePMOzxPxuOFQJJ7tWbquafGFml3c3CjnlO9Isl9wfEFTNw6kdkHZ1dKTMfijxGTGlNoCuSNmro2JfJ6ZLmT8IqzK2hftz1NXJsUam/t1pqFAxbSu3FvPtzzIS9ueJGkTFlwQlg3Se7VWP6Ye3HDMm4Obng4eRRJ7qcTTgPwxf4vWH56eYXHtOnCJhSKHo16lLhNE5cmJGUmkZCRUOb9RiZGciz+GPc2u7fY511quPBR6Ee8EvQKm6I28dBvD3Ei/kS54xfCUkhyr6a01oXG3PN7wVk5WSRlJVHHoQ71nOoRnx5PZs5fRZ5OJ57m7sZ307l+ZyZvn8zuK7uN+0iNZezasWyO2nxbcW24sAE/Dz/qOtYtcZtbmTGz8uxKFIp7vG9cN+YvSilGtR/F3P5zycjOYMSKESyJWFL24G9DTm4OM3bN4N2d77L14lbSs9PvyHGFKIkk92rqeuZ1snU2jWo2MhUKg7+GavLH3OGvnn16djpRSVG0cWvDR6Ef0cSlCS9seIEzCWeo7VCbA7EHTOPaN1NS4opOieZY/LFiZ8kUlD+scj6pbOPuWmtWnF1BR8+OeDp7lrp9oGcgi+5fRIBHAJO3T+bN7W9WarLVWjNj9wx+PPYj/zv5P8auHUuPhT14Zu0zLDi+gKikqEo7dnWWlZvFG1vf4JeTv5g7FIskyb2ayj+Z2qpOK+CvBJ6f3Os61KWek3ENlfyhmbOJZ9FomtduTi37WnzZ50ta1WmFwcaAwcZAL69ebIradNN548tPL2fQr4O4lFy0kvOmqE0AhHqF3jR2r5pe2CibMvfcT1w7QeT1yBKHZIpT17EuX/f9mif9nmRxxGIeXfko1zOvl/n15fHD0R9YcHwBI9uNZNvD25jVZxZDWg0h8nok7+x8h3sX38vApQN5b/d77Li0o9A3qXLTGjIs43zC3ENzWXZ6GVN2TGHOwTnWeyI8p3Ku01BV4RcaFBSkw8PDzR3G7bl6iuTZPTFojWmOSPBT0GcK/DgUzm0r/z6b94Z+b0HdFsbHiVGwdgocX0F4DVseq1ebsddT+crVia9jE+mWkcV2ewNPedTiu5gEauZqhtSvw/tx17knLZPfHO15va4LS69co0V2gWKdI38F91as/dKf8e6ufBObSOeMokusnbSzZUS92vhkZTPnke3YObn99WRWOs9+489pgy0rr1wrdtWWgu6tXwefzGzejy89UX1Uy4kfajqy/nI8dXLL/3nd7GBgs3NN3njqSJGLqspk/TTY8UWxT611qMFLdV0IS8vkw/ikIr2lc3Y2bHGowVaHGuy2N5CpFI65muCMTHqkZ9EjPZMGORrqtYFmPY0/jbtADafiY7m4B779G4xYZNy2svw2Hg4sLP/rajWGln2NP026gV2NYjeLuBbB33/7O3c3vht7W3uWn1nO6PajeanjS7f2b1TVZSRB/BmIOw3xpyHuTN7taWjZDwZ/dUu7VUrt0VoHFfdcRa/EZLWOpcfwcCN3PqwTRJhDQ2Nj42Djbet7jf95C4jNSefTpKNsyYjmv+6h1Ld1LLzDrDTjf66ZwdBlLNRwhq2fABr8HiJeJ0PCblo3DYVru4i5KxScmhCXegES9+LWfgh1bGpA9EpivLtBzRacuX4Uu5RTNAkYCapAGqpZD+zs6db+YeyjV7KuaQCda/kVCicpN4uXrm6ips7m/fqh2BkKJ5/U3Ez+dHRgmJM3qrFvqb+vpvE7OOeQAS3+ftPtcrXmj9g1dLFzoU7HQTfdtiQ9gZ52DsZVJW6FVyfo9HiR5oOZ8UyI24avoRbvNgjBpkXR1Yia5v08AqTmZrM78ypbMqLZYohmo6Ox9vmntTpyd9xl2P45bP0YbGuAV2dj8m7eCxp1BFuDcYd7fzDeNvC/tfdSVt49jJ+58tAaoo/ArtnGP4Y1akLzUGOiv6sv1DKuuJmdm82kbZNwreHKG13eoLZ9bZwNzsw7Mo+kzCQmdZmErU01XNkpJxtijxWfwFNuuN7EpQG4tYA294F35fyRluReQVo0CMJga094/daEdf5X4ScLJIaMnAx+OPoDcw7OISMngxydw/a7Qniw5YNFd9rzVVj3Fmz/zPi4/WDoOxVqNyH++ELYuZvWvf8Ni+/laoue4PsE8Ue+g/C9uPWZiovBBfv564lp3BE6vcKp9c/TxC4HQ/93in0PTvdMp9v6VNbHH2NC37dMPSitNZM3vkRUbjrf9P8Gd8+ORV6rbWx5qfNrBNQLgLrtS/19Ndn5DvtPL0MXOE5xDsTs4/LKZYzrOgla3F/qfitFq37GnwIuJF1g3IpH8KjZkM/u+xGHm5xAzucE9Mr70VpzJvEMQ5cN5XCTQO4e9DxkJMP5HXB2E5zZBBvfhY3vgMEZmnY1JvvDv0D7QeBQdAGNCuXzoPHnVmQkw9nNcGoNRKyB478Z2+u1h5Z9mWefy5G4I3zQ6wPcHIzf/iYGT8TV3pXZB2eTnJXMu93fxZD/B606iD8D/30Uog//1VbT05jAW/UDt+bG+3VbGO+X9w/nLZDkXkFq2NbA38PfNPvkRlpr1p1fxwfhH3Ax+SK9G/fm5aCXGblyJOFXwotP7i71YdBM6Pos5GRCwwDTU/Hp8SgU9Z3r42JwMY2rX0u/hp2NHS4GF5RShea6n0k8YxqjL0lYkzA2XNjA0bijtHc3Junvj37P2vNrebnjy3QsJrEDOBucGdF2RKm/p3xNXZuSkpVCXHoc7o7uJW634swK7G3tubvJ3WXed2VLzEjkmbXPkJ2bzZd9vrzpzKCSKKVoUbsF9Z3rE5Wcd8LVvuZfQxoAqfEQudWY7M9uhjWTje0dHq2gd1JJ7Gsae6Rt7jP25mOPQ8RqiFjDmd2z+LKhB33Ts+i/az5ci4WGgSi3ZozrMA4Xgwsf7vmQ5KxkPg79GEc7x9KPZ24Ra+CXxwEFAz83fqtyaw72LmYNS5J7BQqqH8SX+78kMSORWvZ/9axOxJ/gvd3vsevKLu6qfRez+86ma8Ouxtd4BrE7ejda65J7sJ7tijTFpcVR2742djZ2uDu5m65SjU+Px83ezbSvek71iE6NJiMngwtJF0o9KdnLqxe2ypZ159fR3r09e6L38PGej+nTpA+j2o+6lV9LsZq45M2YuX6+xOSenZvN6nOr6enVE2dD5fd0yiIzJ5MXNrzAxeSLzO47m2a1mt3W/hrVbMTF5IvFP+nkBu0GGn8Arl+GxAvQuPNtHfOOUgrqtYV6bcnp+hyTVozAOfEsE2v5w5nNcPTXv7Z1bcRot+a4OLfh3xe38fSyv/NF8BRc6rUHQxVcVD03F7Z8CBumgacP/ONHqONt7qhMJLlXoCDPIDSaPdF7TD1NrTX/3vFvzied543gNxjaaih2Nn/92oPqB7H63GouJl/Ey8WrzMfKvwoVwMPRw3SVanx6PG6Of53orOdUj8NXDxOZGEmuzqVFrRY33W9th9p09OzI+vPrGd52OK9uepVGNRsxNWRqhZ7oKjjXPdAzsNhtdl3ZRXx6vKncgLlprZm0bRJ7ovcwvcd0guoXex6rXBq5NGLThU1l29i1gfGnmvrx2I8cjDvCjB4zcG+e16uPOQoxxwqdbBxy5gjONum8rjWP/z6cWVdicXNpaOwN120BdZqBoZw9eoOT8Y9kRfWm0xNhyVg48Tv4/h3u/7Tkk+BmIsm9Avl6+GJva094dLgpuSulmNZ9Gm4OboV68/k6eRqLa+2+srv8yT0viXs4ebA/Zj9gHJapY1/HtJ2nkyfrU9dzKuEUAC1q3zy5A9zd5G6m75rOU2ueIikzia/6fGWqMllRGtZsiJ2yu+lc95VnV1LTUJMeXiVf7XonfbH/C1acXcHzHZ7nb83/ViH7bFSzEXHpcaRlp1WPIYhbFJkYyef7Pqd3495/fXtUCjzbG39ucE/aNZxPLOWlg58xqkVb5tRoQf2EKDi8GNLLfmVzIav/D7qNg85PGoeOblXsCVg4HOLPwj0zjLPiquAMH0nuFcje1h4/Dz/CrxSe1nmzr+4taregjn0dwqPDGdxycJmPFZ8eTxs34wyc/J671pq49DgauzY2bVfPqR4ZORnsi9mHrbI19ZhvJqxJGNN3TefktZNM6z6N1m6tS31NednZ2NHIpVGJc90zczJZd24ddze5u8QCZHfSkoglzD44myEth/CE7xMVtt9GNY0zSC4nX6Z57eYVtt+qJCc3h8nbJ2Nva8+kLpPK9g3QsQ49Ah5jVgM/nlv3HCPVFeY89K3x85uWAOWt4R9/Bja9B+v+bZzJ0+156PzP8p/YPLoMlo41fnMYtRy8Q8r3+jtILmKqYJ08O3E8/niZL5hRShFUP6jIH4TSxKXFFRqWyczN5Hrm9ULDNYBpceo/L/9JY5fG1LAtft5xQfWd63OP9z2Mbj+agS0Gliuu8mjiUnJ1yC0Xt5CUlVSuC5cqy/ZL25m6YyrdGnbjjS5vVOjwVH5yN51UrSJ+O/MbM3bNuL0LrvIsOL6AfTH7+Ffnf5k+j2XV0bMj3/T/hvTsdEauHGmsF+RYG5zdy/fTuDM88j94fC00CIC1b8Kn/sbpp5mppQeSmwNr/w2LHgWPNvDkpiqd2EGSe4ULqm8cd98Xva/Mr+no2ZFLKZeKveqzOJk5mSRlJf2V3PP+w1xIukBadlqh5J5fguDc9XNlGpLJ936v93k56OUyb38rmro25XzS+WKvTFx5diV17OsQ3CC4UmMoi1PXTtGidgs+7PUhBpuKnZ6XPxRX4klVMzmTcIYfj/3I8N+HcybhzC3v5/z183y691N6evXk/ua3NpW1Xd12zLt3HgYbA4+tesw0BHlLGneCRxfDmNXGk6Cr/8+Y5HfMNF5bUpzUeJg/DLZ+BIGj4LEVpjn7VZkk9wrm6+6LwcZQ4pTI4uQvahEeXbbeu6l+TN6Ye/5sk5PXThrbHQqfUM1XnuR+JzRxbUJadlqRqpapWalsurCJft79KjyZ3oqR7Ufy099+omaN2xinLUFdh7r4uvviYFu1ZoM8H/g8M8NmEpMaw0O/PcTPJ38ud3mAXJ3L5O2TMdgYmNxl8m1942leqznf3/s9dezr8OSaJ1kSseT2yhU0CYaRS+GxP4yzeVZNNCb5P78qnOSvHILZoRC5xXjSdOBnYGf+YcKykORewRzsHIzj7mVM1AB31b6LWva1yvwHoWBxMPgrgeeXuC2U3B0LJPdSZsrcaU1djOP/ZxIL9ww3XNhAek56lRiSyVeW4axboZTip7/9VK7zLXdKT6+e/DLwFzrU68DUHVN5aeNLJGYklvn1/z3xX/ZE7+HVTq+WqeBbaRrWbMh3935HW7e2TN4+mTGrxhT57JRb064wahmMXgHureCPCfBpAOz8Gvb/BP/pCzlZ8NhK6Di62F0kpCewOGIxT695msf+eIzFEYtJyUq5vbgqgCT3ShDkGcSx+GNlXjDCRtkQ5Gkcd9das+H8BoYsG8KsA7OK3b5gcTAwjrmDscAWQB2Hv2bLGGwNpmRf1Xrufh5+ONk5Fakrv/LsSjydPOlQr4OZIhP5PJw8mNV3Fq8EvcLGqI08uOzBMnVCopKi+HjPx4Q0DGHQXbdWNqI47o7ufHvPt7zZ9U1OXDvBkGVDmLl/Jhk5Gbe3Y+8QGP0bjPrNON1y5WvGE6eNAuGpTeBVeNprYkYiSyKW8PTap+m9qDdvbn+Tc9fPEZcex5vb36T3ot5M2jaJvdF7zVYQTWbLVIJO9Tvx9cGv2Rezj55eZasbEeQZxLrz63hs1WPsid6Dk50TX+7/kiDPoCLzqePSjBUh85O7k8EJZ4MzJ+OLDsuAsWefkJGAdy3v23xnFatmjZo8cNcD/O/k/xjfcTzuju4kZiSy7dI2Hmn7CDZK+h5VgY2yYVT7UQTVD+Jfm//F46se5wnfJxgbMLbYYTOtNVO2T8FG2TCl25QKLwRmo2wY2moooY1D+SD8A2YdmMXKsyuZ1GXS7Z+jadYDvLsbrwiOOWYsHZJXBuF65nU2nN/AqshV7Li8g+xcY8ntke1H0t+7P23d2gJwIPYAS08tZeXZlSw9tRRvV28G3TWIgS0GlvuE8u2Q/z2VwM/DDzsbu3LNgOncwHjV4clrJ5nQeQJrhq3By8WLN7a+QXJmcqFtbxxzB2PvPSnL+E0hP+nna1SzEU1dm1aJKYU3Gt5mOFm5WSw6sQiANefWkJ2bXaWGZIRR+7rtWTRgEYNbDmbOoTmM/mM0F5IuFNnu55M/s/PKTl4Oepn6zvUrLR53R3em95jO132/Jlfn8sTqJ5i4ZaLp/8ctU8pYsK3L0yTlpLP89HKeW/ccvf7bi//b9n+cTjjNo20fZeHfFrLywZWM7ziednXboZRCKUVAvQCmdJvChr9v4K2Qt3BzcOOTvZ/Q9399GbduHOvOryMrt2jV1YomJX8ryeKIxbRxa0O7ukVLB5Tkz8t/0rpOa9Owyv6Y/Yz6YxT3N7+ft7u/bdruw/APWXB8AbtH7Db1ih774zHCo8Oxt7Uv1A5wKfkSadlpVW5YJt8za5/hSNwR1gxdw9i1Y4lJjWHZoGWWWfrVQvwR+QdTt08ll1z+r8v/MaD5AMA4X3/wssH4uPswp++cO/ZvmJ6dzuyDs/n2yLc4G5x5qeNLDLprULm//WXlZHHi2gkOxh5kx+UdbLu4jazcLDydPOnv3Z/+3v3xdfct9/uKTIxk6amlLDu9jNi0WNwc3BjYYiCD7xp8W9c33KzkryT3Ku6zvZ8x59AcPg79mD5N+wDwxtY32H1lN6uHrjZt99rm11h5diUNnBsUaq8Otl/czlNrn+LFwBf5dO+nPOX/FM8GPGvusEQpLiVf4vUtr7M3Zi/3N7+ficETeXnTy+yL2ceSB5aY5vDfSacTTjN1x1T2xuwlsF4gk7tOLrFTo7XmUsolDsUe4kDsAQ5dPcSxuGNk5hrn9td3rk+fJn3o790fPw+/ChkmzM7NZvul7SyJWMLGCxvJ1tkMbDGQad2n3dL+pJ57NTbWfyxbLm7hoz0fEdYkDKUUcelxRYZe8k+q3jjeXh10bdiV5rWa8/m+z9FoGZKpJhrWbMg3/b9hzqE5zDowiy0Xt5CQkcAbwW+YJbGDcdLAt/d8y9JTS/kw/EOGLh/KGJ8x/NP3n2TnZnM47jCHYg9x8OpBDsYeNA3h2Nva065uOx5u8zC+Hr74e/jj6eRZ4d887Gzs6OnVk55ePYlLi+O3M7/dUlXRMh2rUvYqKozB1sDwNsOZvH0yR+KO4OPuQ3xafJETM/nTIQvOlKkulFKMaDuCt/58izZubWheyzIvw7dEdjZ2jPUfS5cGXXh9y+u0q9uOv7e++QIslc1G2fBgywfp5dWLD8I/YPbB2Sw4voDkzGQ0xpEKb1dvujfqjq+7L34efrSs0/KOX1NR17FuhVZavZEk92ogrGkYU/+cyoqzK/Bx9yEuPa5IvZf8C5mqY88dYEDzAfxw9Aceav2QuUMRt6BDvQ6seHAFWusqM8uprmNd3u3xLgNbDDTNWvHz8MPH3afYIn6WRpJ7NeBaw5Xujbqz6uwqXu74cpH6MfDXsMyNwzXVhZPBieWDl5e+oaiybJQNpS6eawZdG3Y1rZ9gTarGn1hRqvua3UdMWgwbozaSnZtddMw9b5imOg7LCCEqniT3aqKXVy8c7RyZf2w+UHiOOxgLUA1sMZAejapG7XMhhHnJsEw14WRwIrRxKCvPrgSKjq0bbAy3PJ1KCGF5pOdejRRcbq66jq0LIe6M20ruSqlIpdQhpdR+pVR4XpubUmqNUioi71YGgStISMMQ03J31XVWjBDizqiInntvrXVAgaukJgDrtNYtgXV5j0UFMNga6Ne0H3Y2dtR2qG3ucIQQVVhljLk/AITm3f8O2Aj8qxKOY5XGdxzP/S3urxKLWAghqq7b7blrYLVSao9S6sm8Nk+t9WWAvNt6Jb5alFst+1p09Oxo7jCEEFXc7fbcQ7TWl5RS9YA1SqnjZX1h3h+DJwGaNGlym2EIIYQo6LZ67lrrS3m3McASoDMQrZRqAJB3G1PCa2drrYO01kEeHneugL0QQliDW07uSilnpZRL/n2gH3AYWAbkV8MZBfx6u0EKIYQon9sZlvEEluSVxLQDftJa/6GU2g0sUko9DpwHht1+mEIIIcrjlpO71voM4F9MexwQdjtBCSGEuD1yhaoQQlggSe5CCGGBJLkLIYQFkuQuhBAWSJK7EEJYIEnuQghhgSS5CyGEBZLkLoQQFkiSuxBCWCBJ7kIIYYEkuQshhAWS5C6EEBZIkrsQQlggSe5CCGGBJLkLIYQFkuQuhBAWSJK7EEJYIEnuQghhgSS5CyGEBZLkLoQQFkiSuxBCWCBJ7kIIYYEkuQshhAWS5C6EEBZIkrsQQlggSe5CCGGBJLkLIYQFkuQuhBAWSJK7EEJYIEnuQghhgSS5CyGEBZLkLoQQFkiSuxBCWCBJ7kIIYYEkuQshhAWS5C6EEBZIkrsQQlggSe5CCGGBKi25K6XuUUqdUEqdUkpNqKzjCCGEKKpSkrtSyhaYCdwLtAMeVkq1q4xjCSGEKKqyeu6dgVNa6zNa60xgIfBAJR1LCCHEDSoruTcCLhR4HJXXZqKUelIpFa6UCo+Nja2kMIQQwjpVVnJXxbTpQg+0nq21DtJaB3l4eFRSGEIIYZ0qK7lHAY0LPPYCLlXSsYQQQtygspL7bqClUqqZUqoG8A9gWSUdSwghxA3sKmOnWutspdRzwCrAFpirtT5SGccSQghRVKUkdwCt9QpgRWXtXwghRMnkClUhhLBASmtd+laVHYRSscC5Sj6MO3C1ko9Rlcn7l/dvze8fLPN30FRrXex0wyqR3O8EpVS41jrI3HGYi7x/ef/W/P7B+n4HMiwjhBAWSJK7EEJYIGtK7rPNHYCZyfu3btb+/sHKfgdWM+YuhBDWxJp67kIIYTUkuQshhAWyiuRu7atCKaUilVKHlFL7lVLh5o6nsiml5iqlYpRShwu0uSml1iilIvJu65gzxspUwvufopS6mPcZ2K+Uus+cMVYmpVRjpdQGpdQxpdQRpdQLee1W8xkAK0jusiqUSW+tdYCVzPOdB9xzQ9sEYJ3WuiWwLu+xpZpH0fcP8HHeZyAgrzyIpcoGXtZatwW6AM/m/Z+3ps+A5Sd3ZFUoq6O13gzE39D8APBd3v3vgEF3NKg7qIT3bzW01pe11nvz7icBxzAuFmQ1nwGwjuRe6qpQVkADq5VSCudUlQAAAsxJREFUe5RST5o7GDPx1FpfBuN/fqCemeMxh+eUUgfzhm0sekgin1LKG+gA7MTKPgPWkNxLXRXKCoRorQMxDk09q5Tqae6AxB33FdACCAAuAx+aN5zKp5SqCfwCvKi1vm7ueO40a0juVr8qlNb6Ut5tDLAE41CVtYlWSjUAyLuNMXM8d5TWOlprnaO1zgXmYOGfAaWUAWNin6+1XpzXbFWfAWtI7la9KpRSylkp5ZJ/H+gHHL75qyzSMmBU3v1RwK9mjOWOy09qeQZjwZ8BpZQCvgGOaa0/KvCUVX0GrOIK1bxpX5/w16pQ08wc0h2jlGqOsbcOxsVZfrL096+UWgCEYizxGg28CSwFFgFNgPPAMK21RZ50LOH9h2IcktFAJPBU/vizpVFKdQe2AIeA3LzmiRjH3a3iMwBWktyFEMLaWMOwjBBCWB1J7kIIYYEkuQshhAWS5C6EEBZIkrsQQlggSe7Caiilkm94PFop9YW54hGiMklyF+I25VUeFaJKkeQuBKCUaqqUWpdXWGudUqpJXvs8pdTQAtsl592G5tUM/wk4lHcl8O9KqQNKqcNKqYfM9FaEAIxXLAphLRyVUvsLPHbjr1IUXwDfa62/U0qNAT6j9JKwnQEfrfVZpdQQ4JLW+m8ASqlaFRy7EOUiPXdhTdIKLFYRAEwu8FxX4Ke8+z8A3cuwv11a67N59w8BfZRSM5RSPbTWiRUXthDlJ8ldiOLl1+XIJu//SV5BqhoFtkkxbaz1SaAjxiT/rlKq4B8OIe44Se5CGG3HWDEUYASwNe9+JMakDcaVfAzFvVgp1RBI1Vr/CHwABFZapEKUgYy5C2H0PDBXKfUqEAs8ltc+B/hVKbUL47qbKSW83hd4XymVC2QBYys5XiFuSqpCCiGEBZJhmf9vpw5kAAAAAAb5W9/jK4gAhuQOMCR3gCG5AwzJHWBI7gBDcgcYCsxBvGMR80o1AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.plot(x=\"Hours\", y=[\"Glucose\", \"Heart Rate\", 'Mean blood pressure', 'Glascow coma scale total'])\n",
    "\n",
    "## Are there any missing values? For which variables? How could they be handled?\n",
    "print('We see that there are 3 columns which have no data, name \"Capillary refill rate\", \"Fraction inspired oxygen\" and \"height\". We can not utilize these columns')\n",
    "print('Other columns such as \"Mean Blood Pressure\" have a few NANs, which could be imputed')\n",
    "## What can you tell on the scales of the different numerical variables?\n",
    "print('The variables use different scales, since they represent different things.')\n",
    "print('A variable like heartrate will have a larger scale and more variance than for example body temperature.')\n",
    "## Are there any uncommonly high/low values? What do they represent? How could they be handled?\n",
    "print('There is some data which has a large variance, such as glucose for example. Since they represent measurements we believe they should simply be handled as they are.')\n",
    "\n",
    "## Do different elements in the serie (i.e. different measurements of each variable) occur at regular or irregular time intervals? Could this be a problem for a model? Why?\n",
    "print('They happen irregularly, as can be seen by the NaN values for different columns at different times.')\n",
    "print('We see for example that Mean blood Pressure and temperature get almost hourly measurements. Coma scale measurements are much less frequent.')\n",
    "print('This could be a problem since, for example, variables which could be strongly correlated may not be measured at the same time.')\n",
    "print('In such a case these correlations could get lost, even when using a method such as imputation.')\n",
    "\n",
    "df.describe() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 396
    },
    "id": "RSK4R2Uk4NFB",
    "outputId": "292d5ccf-7a44-4978-87c1-5e3355ad539d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will be using the forward fill (copies previous value) method for MBP.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hours</th>\n",
       "      <th>Capillary refill rate</th>\n",
       "      <th>Diastolic blood pressure</th>\n",
       "      <th>Fraction inspired oxygen</th>\n",
       "      <th>Glascow coma scale total</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>Heart Rate</th>\n",
       "      <th>Height</th>\n",
       "      <th>Mean blood pressure</th>\n",
       "      <th>Oxygen saturation</th>\n",
       "      <th>Respiratory rate</th>\n",
       "      <th>Systolic blood pressure</th>\n",
       "      <th>Temperature</th>\n",
       "      <th>Weight</th>\n",
       "      <th>pH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>51.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>40.00000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>8.568431</td>\n",
       "      <td>NaN</td>\n",
       "      <td>54.500000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.333333</td>\n",
       "      <td>153.750000</td>\n",
       "      <td>79.121951</td>\n",
       "      <td>NaN</td>\n",
       "      <td>75.432623</td>\n",
       "      <td>98.476190</td>\n",
       "      <td>15.250000</td>\n",
       "      <td>127.82500</td>\n",
       "      <td>36.545715</td>\n",
       "      <td>93.300003</td>\n",
       "      <td>7.41000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>6.160175</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.055385</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.887841</td>\n",
       "      <td>39.837664</td>\n",
       "      <td>2.461657</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.154583</td>\n",
       "      <td>1.742413</td>\n",
       "      <td>5.673827</td>\n",
       "      <td>20.86942</td>\n",
       "      <td>0.539794</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.04717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-0.743333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>67.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>95.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>81.00000</td>\n",
       "      <td>35.900002</td>\n",
       "      <td>93.300003</td>\n",
       "      <td>7.34000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.865000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>126.750000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>97.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>109.00000</td>\n",
       "      <td>36.000002</td>\n",
       "      <td>93.300003</td>\n",
       "      <td>7.38000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>6.990000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>54.500000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>144.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>79.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>129.00000</td>\n",
       "      <td>36.299999</td>\n",
       "      <td>93.300003</td>\n",
       "      <td>7.43000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>12.490000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>60.500000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>190.750000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>82.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>144.00000</td>\n",
       "      <td>37.150000</td>\n",
       "      <td>93.300003</td>\n",
       "      <td>7.43000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>22.990000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>76.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>217.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>102.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>167.00000</td>\n",
       "      <td>37.300000</td>\n",
       "      <td>93.300003</td>\n",
       "      <td>7.48000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Hours  Capillary refill rate  Diastolic blood pressure  \\\n",
       "count  51.000000                    0.0                 40.000000   \n",
       "mean    8.568431                    NaN                 54.500000   \n",
       "std     6.160175                    NaN                  9.055385   \n",
       "min    -0.743333                    NaN                 36.000000   \n",
       "25%     3.865000                    NaN                 50.000000   \n",
       "50%     6.990000                    NaN                 54.500000   \n",
       "75%    12.490000                    NaN                 60.500000   \n",
       "max    22.990000                    NaN                 76.000000   \n",
       "\n",
       "       Fraction inspired oxygen  Glascow coma scale total     Glucose  \\\n",
       "count                       0.0                  6.000000   20.000000   \n",
       "mean                        NaN                 10.333333  153.750000   \n",
       "std                         NaN                  5.887841   39.837664   \n",
       "min                         NaN                  3.000000   90.000000   \n",
       "25%                         NaN                  5.000000  126.750000   \n",
       "50%                         NaN                 13.000000  144.000000   \n",
       "75%                         NaN                 15.000000  190.750000   \n",
       "max                         NaN                 15.000000  217.000000   \n",
       "\n",
       "       Heart Rate  Height  Mean blood pressure  Oxygen saturation  \\\n",
       "count   41.000000     0.0            47.000000          42.000000   \n",
       "mean    79.121951     NaN            75.432623          98.476190   \n",
       "std      2.461657     NaN            11.154583           1.742413   \n",
       "min     67.000000     NaN            52.000000          95.000000   \n",
       "25%     80.000000     NaN            66.000000          97.000000   \n",
       "50%     80.000000     NaN            79.000000          99.000000   \n",
       "75%     80.000000     NaN            82.000000         100.000000   \n",
       "max     80.000000     NaN           102.000000         100.000000   \n",
       "\n",
       "       Respiratory rate  Systolic blood pressure  Temperature     Weight  \\\n",
       "count         40.000000                 40.00000    35.000000   1.000000   \n",
       "mean          15.250000                127.82500    36.545715  93.300003   \n",
       "std            5.673827                 20.86942     0.539794        NaN   \n",
       "min            0.000000                 81.00000    35.900002  93.300003   \n",
       "25%           10.000000                109.00000    36.000002  93.300003   \n",
       "50%           16.000000                129.00000    36.299999  93.300003   \n",
       "75%           19.000000                144.00000    37.150000  93.300003   \n",
       "max           27.000000                167.00000    37.300000  93.300003   \n",
       "\n",
       "            pH  \n",
       "count  9.00000  \n",
       "mean   7.41000  \n",
       "std    0.04717  \n",
       "min    7.34000  \n",
       "25%    7.38000  \n",
       "50%    7.43000  \n",
       "75%    7.43000  \n",
       "max    7.48000  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Use 3 imputation variants mode, mean, and last previous value known. Focus on the variable mean blood pressure for imputation (and comparison)\n",
    "# Mode imputation\n",
    "df_imputed_mode = df.fillna(value={'Mean blood pressure':df['Mean blood pressure'].mode()[0]})\n",
    "df_imputed_mode\n",
    "\n",
    "# Impute using mean and the last previous value known\n",
    "df_imputed_mean = df.fillna(value={'Mean blood pressure':df['Mean blood pressure'].mean()})\n",
    "df_imputed_mean\n",
    "\n",
    "df_imputed_prev = df.copy(deep=True)\n",
    "df_imputed_prev['Mean blood pressure'] = df['Mean blood pressure'].ffill(axis=0)\n",
    "\n",
    "print('We will be using the forward fill (copies previous value) method for MBP.')\n",
    "df_imputed_prev.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 145
    },
    "id": "Aj6IP0nEFNux",
    "outputId": "3d6f80f3-f234-4d6a-caee-aee130dc6d79"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dropna() function results in an empty dataFrame since every row has atleast one NaN value\n",
      "Even when dropping columns which are fully NaN and using imputation on MBP, this holds true.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hours</th>\n",
       "      <th>Diastolic blood pressure</th>\n",
       "      <th>Glascow coma scale eye opening</th>\n",
       "      <th>Glascow coma scale motor response</th>\n",
       "      <th>Glascow coma scale total</th>\n",
       "      <th>Glascow coma scale verbal response</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>Heart Rate</th>\n",
       "      <th>Mean blood pressure</th>\n",
       "      <th>Oxygen saturation</th>\n",
       "      <th>Respiratory rate</th>\n",
       "      <th>Systolic blood pressure</th>\n",
       "      <th>Temperature</th>\n",
       "      <th>Weight</th>\n",
       "      <th>pH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Hours, Diastolic blood pressure, Glascow coma scale eye opening, Glascow coma scale motor response, Glascow coma scale total, Glascow coma scale verbal response, Glucose, Heart Rate, Mean blood pressure, Oxygen saturation, Respiratory rate, Systolic blood pressure, Temperature, Weight, pH]\n",
       "Index: []"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = df_imputed_prev.copy(deep=True)\n",
    "df2 = df2.drop(['Capillary refill rate', 'Fraction inspired oxygen', 'Height'], axis=1)\n",
    "print('The dropna() function results in an empty dataFrame since every row has atleast one NaN value')\n",
    "print('Even when dropping columns which are fully NaN and using imputation on MBP, this holds true.')\n",
    "df2.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SCOEVTigvlDg",
    "outputId": "8c296bc7-f232-44fc-b70b-b18a1c34b6a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These outliers may represent an error in measurement. Their values do not look realistic/probable.\n",
      "We see that capping values gives a lower std while also maintaining a higher count of data overal\n",
      "Also, when setting a smaller boundary for quantile while capping, the std decreases further as measurements become closer in value.\n"
     ]
    }
   ],
   "source": [
    "# Comparing deleting outliers vs capping them:\n",
    "# Change the value of mean blood pressure to 650 for the 11th element\n",
    "# and to 6 for the 12th element of the serie\n",
    "df.loc[10, 'Mean blood pressure'] = 650\n",
    "df.loc[11, 'Mean blood pressure'] = 6\n",
    "#df.describe()\n",
    "# What these 2 outliers may represent?\n",
    "print('These outliers may represent an error in measurement. Their values do not look realistic/probable.')\n",
    "## Print the data after deleting outliers for the Mean blood pressure (consider outliers values >150 or < 53) vs capping them (both low and high values, at any quantile). \n",
    "## What do you notice? Can you cap all the variables? When capping what is happening when you change the upper/lower quantile used to cap the data?\n",
    "\n",
    "\n",
    "dfDeleted = df.drop(df[(df['Mean blood pressure'] < 53) | (df['Mean blood pressure'] > 153)].index)\n",
    "dfDeleted.describe() ## mean 76.2, std  11.3, count 37\n",
    "\n",
    "thresh = df['Mean blood pressure'].quantile([.25,.75])\n",
    "df['Mean blood pressure'].clip(lower=thresh.loc[.25], upper=thresh.loc[.75]).describe()  ## mean 74.8, std 7.6, count 40\n",
    "\n",
    "thresh = df['Mean blood pressure'].quantile([.15,.85])\n",
    "df['Mean blood pressure'].clip(lower=thresh.loc[.15], upper=thresh.loc[.85]).describe()  ## mean 75.3, std 10.0, count 40\n",
    "\n",
    "thresh = df['Mean blood pressure'].quantile([.40,.60])\n",
    "df['Mean blood pressure'].clip(lower=thresh.loc[.40], upper=thresh.loc[.60]).describe()  ## mean 75.2, std 3.9, count 40\n",
    "\n",
    "print('We see that capping values gives a lower std while also maintaining a higher count of data overal')\n",
    "print('Also, when setting a smaller boundary for quantile while capping, the std decreases further as measurements become closer in value.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fpETeMXAMvha"
   },
   "source": [
    "*  **Is normalization used in the given code?**\n",
    "\n",
    "    The program uses the normalization hyperparameter \"none\", which the code then changes to \"norm_start_time_zero.normalizer\" in an if-statement. The code does seem to support other normalizing techniques however, given this if-statement:  \n",
    "            if normalizer is not None:\n",
    "            data = [normalizer.transform(X) for X in data]\n",
    "\n",
    "*   **Which imputation technique is used?**\n",
    "\n",
    "    The imputation technique used here is called forward fill. When there is missing data, the last known data is used to fill this missing value.\n",
    "\n",
    "\n",
    "*   **How irregular time intervals are managed in the given code?**\n",
    "\n",
    "    First, the starting times are set to 0, since our data contains negative times. Then, it creates a list of times in which an arrow is not found (???).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-2B6Q5Wwkvgk"
   },
   "source": [
    "## Training\n",
    "\n",
    "Here we define the model, loss and training loop.\n",
    "\n",
    "The model is an LSTM. You can learn more on LSTM [here](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "pye3W2HkjoOp"
   },
   "outputs": [],
   "source": [
    "#model\n",
    "class LSTMClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, tag_size, feat_size,hidden_size, emb_size,\n",
    "                 bidirectional=False, dropout=0.2, aggregation_type='mean', modelType = 'lstm'):\n",
    "        \"\"\"\n",
    "        constructor, here we define the hidden layers for our architecture       \n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # define if the rnn will be bidirectional\n",
    "        self.bidirectional = bidirectional\n",
    "        self.tag_size = tag_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.feat_size = feat_size\n",
    "        self.emb_size = emb_size\n",
    "        self.modelType = modelType\n",
    "\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # define the aggregation type of the features for the classifier for example, you can take the mean\n",
    "        self.aggregation_type = aggregation_type\n",
    "\n",
    "        #define an encoder linear layer \n",
    "        #self.encoder_layer = nn.TransformerEncoderLayer(d_model=feat_size, nhead=4)\n",
    "        \n",
    "        # Create a (bidirectional) LSTM to encode sequence\n",
    "        #self.lstm = nn.LSTM(feat_size, hidden_size, num_layers=1 ,bidirectional = self.bidirectional)\n",
    "\n",
    "        #self.hidden2out = nn.Linear(16, 1)\n",
    "        # The output of the LSTM doubles if we use a bidirectional encoder.\n",
    "        # we have the forward and backward hidden states \n",
    "\n",
    "        # define a combination linear layer to use in the aggregation\n",
    "        #self.comb = nn.Linear(hidden_size, hidden_size)\n",
    "        \n",
    "        # Create affine layer to project to the classes \n",
    "        #torch.nn.functional.affine_grid(theta, size)\n",
    "        \n",
    "        # dropout layer for regularizetion of a sequence\n",
    "\n",
    "        #non linear activation layer (ReLU)\n",
    "                \n",
    "        self.fc1 = nn.Linear(feat_size, feat_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dout = nn.Dropout(dropout)\n",
    "        if(modelType == 'lstm'):\n",
    "            self.lstm = nn.LSTM(feat_size, hidden_size, batch_first=True, \n",
    "                              bidirectional=bidirectional)\n",
    "        elif(modelType == 'gru'):\n",
    "            self.gru = nn.GRU(feat_size, hidden_size, batch_first=True, \n",
    "                              bidirectional=bidirectional)\n",
    "        encoding_size = hidden_size * 2 if self.bidirectional else hidden_size\n",
    "        self.fc2 = nn.Linear(encoding_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, 1)\n",
    " \n",
    "    def forward(self, x, seq_mask=None, seq_len=None):\n",
    "        h1 = self.fc1(x)\n",
    "        h1 = self.relu(h1)\n",
    "        # [B, T, H] batch, time, hidden or hidden * 2\n",
    "        if(self.modelType == 'lstm'):\n",
    "            outputs, (final, _) = self.lstm(h1)\n",
    "        elif(self.modelType == 'gru'):\n",
    "            outputs, final = self.gru(h1)\n",
    "            \n",
    "\n",
    "        if self.aggregation_type == 'mean':\n",
    "            # mean over hidden states of LSTM\n",
    "            outputs = self.dout(outputs)\n",
    "            h = self.relu(self.fc2(outputs))\n",
    "            #[B, H]\n",
    "            h = h.mean(dim=1)\n",
    "        elif self.aggregation_type == 'last_state':\n",
    "            # last hidden state of the lstm or concat of bidirectional forward and backward states\n",
    "            if self.bidirectional:\n",
    "                h_T_fwd = final[0]\n",
    "                h_T_bwd = final[1]\n",
    "                h = torch.cat([h_T_fwd, h_T_bwd], dim=-1)\n",
    "            else:\n",
    "                h = final[-1]\n",
    "            h = self.relu(self.fc2(h))\n",
    "            h = self.dout(h)\n",
    "        #[B, 1]\n",
    "        #h = self.dout(h) # maybe?\n",
    "        logits = self.out(h)\n",
    "        return logits\n",
    "        \n",
    "        #return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "BG63f4LBnFFf"
   },
   "outputs": [],
   "source": [
    "#eval model\n",
    "def eval_model(model, dataset, device):\n",
    "    model.eval() # the model will not update parameters or perfom dropout\n",
    "    sigmoid = nn.Sigmoid()\n",
    "    with torch.no_grad(): # the model will not update parameters\n",
    "        y_true = []\n",
    "        predictions = []\n",
    "        for data, labels in dataset:\n",
    "            data = data.to(device)\n",
    "            labels = labels.to(device)\n",
    "            logits = model(data)\n",
    "            probs = sigmoid(logits) #compute probabilities\n",
    "            #_, predicted = torch.max(probs.data, 1)\n",
    "            #y_hat_class = np.where(probs.data<0.5, 0, 1)\n",
    "            predictions += [p.item() for p in probs] #concatenate all predictions\n",
    "            y_true += [y.item() for y in labels] #concatenate all labels\n",
    "    #print(predictions)\n",
    "    #print(y_true)\n",
    "    results = print_metrics_binary(y_true, predictions, logging)\n",
    "    # return results, predcitions (probs), and labels\n",
    "    return results, predictions, y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "4WSyj4Minlc6"
   },
   "outputs": [],
   "source": [
    "def train(args):\n",
    "  mode = 'train'\n",
    "  hidden_size = args['dim']\n",
    "  dropout = args['dropout']\n",
    "  batch_size = args['batch_size']\n",
    "  learning_rate = args['lr']\n",
    "  num_epochs = args['epochs']\n",
    "  emb_size = args['emb_size']\n",
    "  aggregation_type = args['aggregation_type']\n",
    "  bidirectional_encoder = args['bidirectional'] \n",
    "  seed = args['seed']\n",
    "  steps = args['steps']\n",
    "  data = args['data']\n",
    "  notes = args['notes']\n",
    "  timestep = args['timestep']\n",
    "  normalizer_state = args['normalizer_state']\n",
    "  modelName = args['model name']\n",
    "  modelType = args['model']\n",
    "  if seed: #reproducibility\n",
    "      torch.manual_seed(seed)\n",
    "      np.random.seed(seed)\n",
    "  device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")   \n",
    "  \n",
    "  logging.basicConfig(level=logging.INFO, \n",
    "          format='%(asctime)s %(message)s', \n",
    "          datefmt='%Y-%m-%d %H:%M:%S',\n",
    "          )\n",
    "  # define cvs data readers\n",
    "  # this will load data into memory\n",
    "  train_reader = InHospitalMortalityReader(dataset_dir=os.path.join(data, 'train'),\n",
    "                                           notes_dir=notes,\n",
    "                                           listfile=os.path.join(data, 'train_listfile.csv'),\n",
    "                                           period_length=48.0)\n",
    "\n",
    "  val_reader = InHospitalMortalityReader(dataset_dir=os.path.join(data, 'train'),\n",
    "                                         notes_dir=notes,\n",
    "                                       listfile=os.path.join(data, 'val_listfile.csv'),\n",
    "                                       period_length=48.0)\n",
    "\n",
    "  # define discrtizer for imputation, and normalization\n",
    "  discretizer = Discretizer(timestep=float(timestep),\n",
    "                          store_masks=True,\n",
    "                          impute_strategy='previous',\n",
    "                          start_time='zero')\n",
    "\n",
    "  discretizer_header = discretizer.transform(train_reader.read_example(0)[\"X\"])[1].split(',')\n",
    "  cont_channels = [i for (i, x) in enumerate(discretizer_header) if x.find(\"->\") == -1]\n",
    "\n",
    "  normalizer = Normalizer(fields=cont_channels)  # choose here which columns to standardize\n",
    "  normalizer_state = normalizer_state\n",
    "  if normalizer_state is None:\n",
    "      normalizer_state = 'norm_start_time_zero.normalizer'\n",
    "      \n",
    "  \n",
    "  normalizer.load_params(normalizer_state)\n",
    "\n",
    "  # read data for pytorch training data \n",
    "  # here we create the mini batches and load into memory\n",
    "  train_dataset = MIMICDataset(train_reader, discretizer, normalizer, batch_labels=True)\n",
    "  train_dl = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "  #read validation data\n",
    "  val_dataset = MIMICDataset(val_reader, discretizer, normalizer, batch_labels=True)\n",
    "  val_dl = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "  #[B, T, feat_size] batch, time, and num of variables\n",
    "  #print(train_dl[0])\n",
    "  feat_size = train_dataset.data.shape[-1] \n",
    "  \n",
    "  # Define the classification model\n",
    "  model = LSTMClassifier(tag_size=1, #binary\n",
    "                    feat_size= feat_size, \n",
    "                    hidden_size=hidden_size,\n",
    "                    emb_size=emb_size,\n",
    "                    bidirectional=bidirectional_encoder,\n",
    "                    dropout=dropout,\n",
    "                    aggregation_type=aggregation_type,\n",
    "                    modelType = modelType)\n",
    "\n",
    "  model = model.to(device)\n",
    "  logging.info(args)\n",
    "  logging.info(model)\n",
    "\n",
    "  # Define optimizer\n",
    "  optimizer = Adam(model.parameters(), lr=learning_rate) \n",
    "  \n",
    "  # define loss for binary classification\n",
    "  # binary cross-entropy \n",
    "  criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "  # path to save model with extension .pt on disk\n",
    "  save_model = 'rnn_model.pt'\n",
    "  best_val_auc = 0.\n",
    "\n",
    "  results = []\n",
    "\n",
    "  step = 0\n",
    "  num_batches = 0\n",
    "  #training loop\n",
    "  # Your code here\n",
    "  # loop over epochs\n",
    "  #for epoch... \n",
    "  e_loss = []  \n",
    "  for epoch_num in range(num_epochs):\n",
    "    loss_batch_list = []\n",
    "    loss_batch = 0\n",
    "    step = 0\n",
    "    # loop over mini-batches\n",
    "    for x, labels in train_dl: #iterates over train_dl and returns X variables and y labels \n",
    "      num_batches = len(x)\n",
    "      x = x.to(device)\n",
    "      labels = labels.to(device)\n",
    "    \n",
    "      optimizer.zero_grad()  \n",
    "      # (1) Forward  \n",
    "      logits = model(x)\n",
    "        \n",
    "      # (2) Compute loss\n",
    "      loss = criterion(logits, labels) \n",
    "      \n",
    "      ## save losses \n",
    "      if(step % 5 == 0):  \n",
    "          loss_batch_list.append(loss.item())\n",
    "          loss_batch += loss.item()  \n",
    "        \n",
    "      ## (3) Compute gradients  \n",
    "      loss.backward()\n",
    "        \n",
    "      ## (4) Update weights  \n",
    "      optimizer.step()\n",
    "      step +=1\n",
    "\n",
    "      if step % steps == 0:\n",
    "        logging.info(\"epoch (%d) step %d: training loss = %.2f\"% \n",
    "              (epoch_num, step, loss_batch/num_batches))\n",
    "          # Progess on validation data after every epoch \n",
    "    metrics_results, _, _ = eval_model(model,\n",
    "                                    val_dl,\n",
    "                                    device)\n",
    "    metrics_results['epoch'] = epoch_num\n",
    "      # save results of current epoch\n",
    "    results.append(metrics_results)\n",
    "    \n",
    "    e_loss += loss_batch_list\n",
    "  #print(metrics_results)\n",
    "    \n",
    "  plt.plot(e_loss)  \n",
    "  torch.save(model.state_dict(),modelName)\n",
    "\n",
    "      #Model selection \n",
    "      #your code here\n",
    "      # save best model to disk  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "ZbOmZFUhsARx",
    "outputId": "54436578-8bd6-4492-8811-723036354856"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-30 16:16:04 {'dim': 16, 'dropout': 0.3, 'batch_size': 8, 'lr': 0.001, 'epochs': 20, 'emb_size': 16, 'aggregation_type': 'mean', 'bidirectional': False, 'seed': 42, 'steps': 50, 'data': 'test_text_data_2/in-hospital-mortality', 'notes': 'test_text_data_2/train', 'timestep': 1.0, 'imputation': 'previous', 'normalizer_state': None, 'model name': 'meanModel', 'model': 'lstm'}\n",
      "2022-09-30 16:16:04 LSTMClassifier(\n",
      "  (encoder_layer): TransformerEncoderLayer(\n",
      "    (self_attn): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=76, out_features=76, bias=True)\n",
      "    )\n",
      "    (linear1): Linear(in_features=76, out_features=2048, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (linear2): Linear(in_features=2048, out_features=76, bias=True)\n",
      "    (norm1): LayerNorm((76,), eps=1e-05, elementwise_affine=True)\n",
      "    (norm2): LayerNorm((76,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout1): Dropout(p=0.1, inplace=False)\n",
      "    (dropout2): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (hidden2out): Linear(in_features=16, out_features=1, bias=True)\n",
      "  (comb): Linear(in_features=16, out_features=16, bias=True)\n",
      "  (fc1): Linear(in_features=76, out_features=76, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (dout): Dropout(p=0.3, inplace=False)\n",
      "  (lstm): LSTM(76, 16, batch_first=True)\n",
      "  (fc2): Linear(in_features=16, out_features=16, bias=True)\n",
      "  (out): Linear(in_features=16, out_features=1, bias=True)\n",
      ")\n",
      "2022-09-30 16:16:04 epoch (0) step 50: training loss = 0.54\n",
      "2022-09-30 16:16:05 epoch (0) step 100: training loss = 0.92\n",
      "2022-09-30 16:16:05 epoch (0) step 150: training loss = 1.40\n",
      "2022-09-30 16:16:06 epoch (0) step 200: training loss = 1.87\n",
      "2022-09-30 16:16:06 epoch (0) step 250: training loss = 2.22\n",
      "2022-09-30 16:16:07 epoch (0) step 300: training loss = 2.63\n",
      "2022-09-30 16:16:07 epoch (0) step 350: training loss = 3.06\n",
      "2022-09-30 16:16:08 confusion matrix:\n",
      "2022-09-30 16:16:08 [[584   0]\n",
      " [ 88   0]]\n",
      "C:\\Users\\Ramon\\AppData\\Local\\Temp\\ipykernel_20128\\169873742.py:14: RuntimeWarning: invalid value encountered in float_scalars\n",
      "  prec1 = cf[1][1] / (cf[1][1] + cf[0][1])\n",
      "2022-09-30 16:16:08 accuracy = 0.869\n",
      "2022-09-30 16:16:08 precision class 0 = 0.869\n",
      "2022-09-30 16:16:08 precision class 1 = nan\n",
      "2022-09-30 16:16:08 recall class 0 = 1.000\n",
      "2022-09-30 16:16:08 recall class 1 = 0.000\n",
      "2022-09-30 16:16:08 AUC of ROC = 0.826\n",
      "2022-09-30 16:16:08 AUC of PRC = 0.432\n",
      "2022-09-30 16:16:08 epoch (1) step 50: training loss = 0.51\n",
      "2022-09-30 16:16:09 epoch (1) step 100: training loss = 0.80\n",
      "2022-09-30 16:16:09 epoch (1) step 150: training loss = 1.06\n",
      "2022-09-30 16:16:10 epoch (1) step 200: training loss = 1.61\n",
      "2022-09-30 16:16:10 epoch (1) step 250: training loss = 1.97\n",
      "2022-09-30 16:16:11 epoch (1) step 300: training loss = 2.24\n",
      "2022-09-30 16:16:11 epoch (1) step 350: training loss = 2.50\n",
      "2022-09-30 16:16:12 confusion matrix:\n",
      "2022-09-30 16:16:12 [[564  20]\n",
      " [ 60  28]]\n",
      "2022-09-30 16:16:12 accuracy = 0.881\n",
      "2022-09-30 16:16:12 precision class 0 = 0.904\n",
      "2022-09-30 16:16:12 precision class 1 = 0.583\n",
      "2022-09-30 16:16:12 recall class 0 = 0.966\n",
      "2022-09-30 16:16:12 recall class 1 = 0.318\n",
      "2022-09-30 16:16:12 AUC of ROC = 0.837\n",
      "2022-09-30 16:16:12 AUC of PRC = 0.450\n",
      "2022-09-30 16:16:12 epoch (2) step 50: training loss = 0.34\n",
      "2022-09-30 16:16:13 epoch (2) step 100: training loss = 0.93\n",
      "2022-09-30 16:16:13 epoch (2) step 150: training loss = 1.25\n",
      "2022-09-30 16:16:14 epoch (2) step 200: training loss = 1.64\n",
      "2022-09-30 16:16:14 epoch (2) step 250: training loss = 2.04\n",
      "2022-09-30 16:16:15 epoch (2) step 300: training loss = 2.36\n",
      "2022-09-30 16:16:15 epoch (2) step 350: training loss = 2.59\n",
      "2022-09-30 16:16:16 confusion matrix:\n",
      "2022-09-30 16:16:16 [[569  15]\n",
      " [ 71  17]]\n",
      "2022-09-30 16:16:16 accuracy = 0.872\n",
      "2022-09-30 16:16:16 precision class 0 = 0.889\n",
      "2022-09-30 16:16:16 precision class 1 = 0.531\n",
      "2022-09-30 16:16:16 recall class 0 = 0.974\n",
      "2022-09-30 16:16:16 recall class 1 = 0.193\n",
      "2022-09-30 16:16:16 AUC of ROC = 0.845\n",
      "2022-09-30 16:16:16 AUC of PRC = 0.463\n",
      "2022-09-30 16:16:16 epoch (3) step 50: training loss = 0.20\n",
      "2022-09-30 16:16:16 epoch (3) step 100: training loss = 0.59\n",
      "2022-09-30 16:16:17 epoch (3) step 150: training loss = 0.88\n",
      "2022-09-30 16:16:17 epoch (3) step 200: training loss = 1.30\n",
      "2022-09-30 16:16:18 epoch (3) step 250: training loss = 1.75\n",
      "2022-09-30 16:16:18 epoch (3) step 300: training loss = 1.93\n",
      "2022-09-30 16:16:19 epoch (3) step 350: training loss = 2.28\n",
      "2022-09-30 16:16:19 confusion matrix:\n",
      "2022-09-30 16:16:19 [[551  33]\n",
      " [ 51  37]]\n",
      "2022-09-30 16:16:19 accuracy = 0.875\n",
      "2022-09-30 16:16:19 precision class 0 = 0.915\n",
      "2022-09-30 16:16:19 precision class 1 = 0.529\n",
      "2022-09-30 16:16:19 recall class 0 = 0.943\n",
      "2022-09-30 16:16:19 recall class 1 = 0.420\n",
      "2022-09-30 16:16:19 AUC of ROC = 0.838\n",
      "2022-09-30 16:16:19 AUC of PRC = 0.457\n",
      "2022-09-30 16:16:20 epoch (4) step 50: training loss = 0.40\n",
      "2022-09-30 16:16:20 epoch (4) step 100: training loss = 0.67\n",
      "2022-09-30 16:16:20 epoch (4) step 150: training loss = 0.86\n",
      "2022-09-30 16:16:21 epoch (4) step 200: training loss = 1.16\n",
      "2022-09-30 16:16:21 epoch (4) step 250: training loss = 1.51\n",
      "2022-09-30 16:16:22 epoch (4) step 300: training loss = 2.04\n",
      "2022-09-30 16:16:22 epoch (4) step 350: training loss = 2.60\n",
      "2022-09-30 16:16:23 confusion matrix:\n",
      "2022-09-30 16:16:23 [[577   7]\n",
      " [ 80   8]]\n",
      "2022-09-30 16:16:23 accuracy = 0.871\n",
      "2022-09-30 16:16:23 precision class 0 = 0.878\n",
      "2022-09-30 16:16:23 precision class 1 = 0.533\n",
      "2022-09-30 16:16:23 recall class 0 = 0.988\n",
      "2022-09-30 16:16:23 recall class 1 = 0.091\n",
      "2022-09-30 16:16:23 AUC of ROC = 0.841\n",
      "2022-09-30 16:16:23 AUC of PRC = 0.490\n",
      "2022-09-30 16:16:23 epoch (5) step 50: training loss = 0.37\n",
      "2022-09-30 16:16:24 epoch (5) step 100: training loss = 0.78\n",
      "2022-09-30 16:16:24 epoch (5) step 150: training loss = 1.09\n",
      "2022-09-30 16:16:25 epoch (5) step 200: training loss = 1.28\n",
      "2022-09-30 16:16:25 epoch (5) step 250: training loss = 1.57\n",
      "2022-09-30 16:16:25 epoch (5) step 300: training loss = 1.89\n",
      "2022-09-30 16:16:26 epoch (5) step 350: training loss = 2.25\n",
      "2022-09-30 16:16:26 confusion matrix:\n",
      "2022-09-30 16:16:26 [[568  16]\n",
      " [ 63  25]]\n",
      "2022-09-30 16:16:26 accuracy = 0.882\n",
      "2022-09-30 16:16:26 precision class 0 = 0.900\n",
      "2022-09-30 16:16:26 precision class 1 = 0.610\n",
      "2022-09-30 16:16:26 recall class 0 = 0.973\n",
      "2022-09-30 16:16:26 recall class 1 = 0.284\n",
      "2022-09-30 16:16:26 AUC of ROC = 0.840\n",
      "2022-09-30 16:16:26 AUC of PRC = 0.497\n",
      "2022-09-30 16:16:27 epoch (6) step 50: training loss = 0.27\n",
      "2022-09-30 16:16:27 epoch (6) step 100: training loss = 0.47\n",
      "2022-09-30 16:16:28 epoch (6) step 150: training loss = 0.85\n",
      "2022-09-30 16:16:28 epoch (6) step 200: training loss = 1.08\n",
      "2022-09-30 16:16:29 epoch (6) step 250: training loss = 1.41\n",
      "2022-09-30 16:16:29 epoch (6) step 300: training loss = 1.76\n",
      "2022-09-30 16:16:30 epoch (6) step 350: training loss = 2.26\n",
      "2022-09-30 16:16:30 confusion matrix:\n",
      "2022-09-30 16:16:30 [[557  27]\n",
      " [ 59  29]]\n",
      "2022-09-30 16:16:30 accuracy = 0.872\n",
      "2022-09-30 16:16:30 precision class 0 = 0.904\n",
      "2022-09-30 16:16:30 precision class 1 = 0.518\n",
      "2022-09-30 16:16:30 recall class 0 = 0.954\n",
      "2022-09-30 16:16:30 recall class 1 = 0.330\n",
      "2022-09-30 16:16:30 AUC of ROC = 0.833\n",
      "2022-09-30 16:16:30 AUC of PRC = 0.500\n",
      "2022-09-30 16:16:31 epoch (7) step 50: training loss = 0.23\n",
      "2022-09-30 16:16:31 epoch (7) step 100: training loss = 0.62\n",
      "2022-09-30 16:16:32 epoch (7) step 150: training loss = 0.99\n",
      "2022-09-30 16:16:32 epoch (7) step 200: training loss = 1.30\n",
      "2022-09-30 16:16:33 epoch (7) step 250: training loss = 1.69\n",
      "2022-09-30 16:16:33 epoch (7) step 300: training loss = 2.09\n",
      "2022-09-30 16:16:33 epoch (7) step 350: training loss = 2.26\n",
      "2022-09-30 16:16:34 confusion matrix:\n",
      "2022-09-30 16:16:34 [[567  17]\n",
      " [ 65  23]]\n",
      "2022-09-30 16:16:34 accuracy = 0.878\n",
      "2022-09-30 16:16:34 precision class 0 = 0.897\n",
      "2022-09-30 16:16:34 precision class 1 = 0.575\n",
      "2022-09-30 16:16:34 recall class 0 = 0.971\n",
      "2022-09-30 16:16:34 recall class 1 = 0.261\n",
      "2022-09-30 16:16:34 AUC of ROC = 0.852\n",
      "2022-09-30 16:16:34 AUC of PRC = 0.478\n",
      "2022-09-30 16:16:34 epoch (8) step 50: training loss = 0.31\n",
      "2022-09-30 16:16:35 epoch (8) step 100: training loss = 0.67\n",
      "2022-09-30 16:16:35 epoch (8) step 150: training loss = 0.94\n",
      "2022-09-30 16:16:36 epoch (8) step 200: training loss = 1.19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-30 16:16:36 epoch (8) step 250: training loss = 1.45\n",
      "2022-09-30 16:16:37 epoch (8) step 300: training loss = 1.78\n",
      "2022-09-30 16:16:38 epoch (8) step 350: training loss = 2.22\n",
      "2022-09-30 16:16:38 confusion matrix:\n",
      "2022-09-30 16:16:38 [[551  33]\n",
      " [ 53  35]]\n",
      "2022-09-30 16:16:38 accuracy = 0.872\n",
      "2022-09-30 16:16:38 precision class 0 = 0.912\n",
      "2022-09-30 16:16:38 precision class 1 = 0.515\n",
      "2022-09-30 16:16:38 recall class 0 = 0.943\n",
      "2022-09-30 16:16:38 recall class 1 = 0.398\n",
      "2022-09-30 16:16:38 AUC of ROC = 0.828\n",
      "2022-09-30 16:16:38 AUC of PRC = 0.468\n",
      "2022-09-30 16:16:39 epoch (9) step 50: training loss = 0.27\n",
      "2022-09-30 16:16:39 epoch (9) step 100: training loss = 0.54\n",
      "2022-09-30 16:16:39 epoch (9) step 150: training loss = 0.80\n",
      "2022-09-30 16:16:40 epoch (9) step 200: training loss = 1.00\n",
      "2022-09-30 16:16:40 epoch (9) step 250: training loss = 1.31\n",
      "2022-09-30 16:16:41 epoch (9) step 300: training loss = 1.62\n",
      "2022-09-30 16:16:41 epoch (9) step 350: training loss = 1.87\n",
      "2022-09-30 16:16:42 confusion matrix:\n",
      "2022-09-30 16:16:42 [[552  32]\n",
      " [ 56  32]]\n",
      "2022-09-30 16:16:42 accuracy = 0.869\n",
      "2022-09-30 16:16:42 precision class 0 = 0.908\n",
      "2022-09-30 16:16:42 precision class 1 = 0.500\n",
      "2022-09-30 16:16:42 recall class 0 = 0.945\n",
      "2022-09-30 16:16:42 recall class 1 = 0.364\n",
      "2022-09-30 16:16:42 AUC of ROC = 0.821\n",
      "2022-09-30 16:16:42 AUC of PRC = 0.466\n",
      "2022-09-30 16:16:42 epoch (10) step 50: training loss = 0.19\n",
      "2022-09-30 16:16:43 epoch (10) step 100: training loss = 0.41\n",
      "2022-09-30 16:16:43 epoch (10) step 150: training loss = 0.73\n",
      "2022-09-30 16:16:44 epoch (10) step 200: training loss = 0.87\n",
      "2022-09-30 16:16:44 epoch (10) step 250: training loss = 1.05\n",
      "2022-09-30 16:16:45 epoch (10) step 300: training loss = 1.34\n",
      "2022-09-30 16:16:45 epoch (10) step 350: training loss = 1.61\n",
      "2022-09-30 16:16:45 confusion matrix:\n",
      "2022-09-30 16:16:45 [[562  22]\n",
      " [ 63  25]]\n",
      "2022-09-30 16:16:45 accuracy = 0.874\n",
      "2022-09-30 16:16:45 precision class 0 = 0.899\n",
      "2022-09-30 16:16:45 precision class 1 = 0.532\n",
      "2022-09-30 16:16:45 recall class 0 = 0.962\n",
      "2022-09-30 16:16:45 recall class 1 = 0.284\n",
      "2022-09-30 16:16:45 AUC of ROC = 0.814\n",
      "2022-09-30 16:16:45 AUC of PRC = 0.442\n",
      "2022-09-30 16:16:46 epoch (11) step 50: training loss = 0.21\n",
      "2022-09-30 16:16:46 epoch (11) step 100: training loss = 0.51\n",
      "2022-09-30 16:16:47 epoch (11) step 150: training loss = 0.58\n",
      "2022-09-30 16:16:47 epoch (11) step 200: training loss = 0.88\n",
      "2022-09-30 16:16:48 epoch (11) step 250: training loss = 1.12\n",
      "2022-09-30 16:16:48 epoch (11) step 300: training loss = 1.43\n",
      "2022-09-30 16:16:49 epoch (11) step 350: training loss = 1.77\n",
      "2022-09-30 16:16:49 confusion matrix:\n",
      "2022-09-30 16:16:49 [[562  22]\n",
      " [ 59  29]]\n",
      "2022-09-30 16:16:49 accuracy = 0.879\n",
      "2022-09-30 16:16:49 precision class 0 = 0.905\n",
      "2022-09-30 16:16:49 precision class 1 = 0.569\n",
      "2022-09-30 16:16:49 recall class 0 = 0.962\n",
      "2022-09-30 16:16:49 recall class 1 = 0.330\n",
      "2022-09-30 16:16:49 AUC of ROC = 0.831\n",
      "2022-09-30 16:16:49 AUC of PRC = 0.472\n",
      "2022-09-30 16:16:50 epoch (12) step 50: training loss = 0.35\n",
      "2022-09-30 16:16:50 epoch (12) step 100: training loss = 0.62\n",
      "2022-09-30 16:16:50 epoch (12) step 150: training loss = 0.89\n",
      "2022-09-30 16:16:51 epoch (12) step 200: training loss = 1.20\n",
      "2022-09-30 16:16:51 epoch (12) step 250: training loss = 1.41\n",
      "2022-09-30 16:16:52 epoch (12) step 300: training loss = 1.56\n",
      "2022-09-30 16:16:52 epoch (12) step 350: training loss = 1.84\n",
      "2022-09-30 16:16:53 confusion matrix:\n",
      "2022-09-30 16:16:53 [[553  31]\n",
      " [ 58  30]]\n",
      "2022-09-30 16:16:53 accuracy = 0.868\n",
      "2022-09-30 16:16:53 precision class 0 = 0.905\n",
      "2022-09-30 16:16:53 precision class 1 = 0.492\n",
      "2022-09-30 16:16:53 recall class 0 = 0.947\n",
      "2022-09-30 16:16:53 recall class 1 = 0.341\n",
      "2022-09-30 16:16:53 AUC of ROC = 0.836\n",
      "2022-09-30 16:16:53 AUC of PRC = 0.496\n",
      "2022-09-30 16:16:53 epoch (13) step 50: training loss = 0.17\n",
      "2022-09-30 16:16:54 epoch (13) step 100: training loss = 0.37\n",
      "2022-09-30 16:16:54 epoch (13) step 150: training loss = 0.59\n",
      "2022-09-30 16:16:55 epoch (13) step 200: training loss = 0.84\n",
      "2022-09-30 16:16:55 epoch (13) step 250: training loss = 1.06\n",
      "2022-09-30 16:16:55 epoch (13) step 300: training loss = 1.31\n",
      "2022-09-30 16:16:56 epoch (13) step 350: training loss = 1.67\n",
      "2022-09-30 16:16:56 confusion matrix:\n",
      "2022-09-30 16:16:56 [[565  19]\n",
      " [ 63  25]]\n",
      "2022-09-30 16:16:56 accuracy = 0.878\n",
      "2022-09-30 16:16:56 precision class 0 = 0.900\n",
      "2022-09-30 16:16:56 precision class 1 = 0.568\n",
      "2022-09-30 16:16:56 recall class 0 = 0.967\n",
      "2022-09-30 16:16:56 recall class 1 = 0.284\n",
      "2022-09-30 16:16:56 AUC of ROC = 0.807\n",
      "2022-09-30 16:16:56 AUC of PRC = 0.443\n",
      "2022-09-30 16:16:57 epoch (14) step 50: training loss = 0.31\n",
      "2022-09-30 16:16:57 epoch (14) step 100: training loss = 0.48\n",
      "2022-09-30 16:16:58 epoch (14) step 150: training loss = 0.70\n",
      "2022-09-30 16:16:58 epoch (14) step 200: training loss = 0.98\n",
      "2022-09-30 16:16:59 epoch (14) step 250: training loss = 1.22\n",
      "2022-09-30 16:16:59 epoch (14) step 300: training loss = 1.30\n",
      "2022-09-30 16:16:59 epoch (14) step 350: training loss = 1.50\n",
      "2022-09-30 16:17:00 confusion matrix:\n",
      "2022-09-30 16:17:00 [[551  33]\n",
      " [ 53  35]]\n",
      "2022-09-30 16:17:00 accuracy = 0.872\n",
      "2022-09-30 16:17:00 precision class 0 = 0.912\n",
      "2022-09-30 16:17:00 precision class 1 = 0.515\n",
      "2022-09-30 16:17:00 recall class 0 = 0.943\n",
      "2022-09-30 16:17:00 recall class 1 = 0.398\n",
      "2022-09-30 16:17:00 AUC of ROC = 0.815\n",
      "2022-09-30 16:17:00 AUC of PRC = 0.457\n",
      "2022-09-30 16:17:00 epoch (15) step 50: training loss = 0.19\n",
      "2022-09-30 16:17:01 epoch (15) step 100: training loss = 0.56\n",
      "2022-09-30 16:17:01 epoch (15) step 150: training loss = 0.69\n",
      "2022-09-30 16:17:02 epoch (15) step 200: training loss = 0.80\n",
      "2022-09-30 16:17:02 epoch (15) step 250: training loss = 0.97\n",
      "2022-09-30 16:17:03 epoch (15) step 300: training loss = 1.20\n",
      "2022-09-30 16:17:03 epoch (15) step 350: training loss = 1.52\n",
      "2022-09-30 16:17:04 confusion matrix:\n",
      "2022-09-30 16:17:04 [[533  51]\n",
      " [ 46  42]]\n",
      "2022-09-30 16:17:04 accuracy = 0.856\n",
      "2022-09-30 16:17:04 precision class 0 = 0.921\n",
      "2022-09-30 16:17:04 precision class 1 = 0.452\n",
      "2022-09-30 16:17:04 recall class 0 = 0.913\n",
      "2022-09-30 16:17:04 recall class 1 = 0.477\n",
      "2022-09-30 16:17:04 AUC of ROC = 0.823\n",
      "2022-09-30 16:17:04 AUC of PRC = 0.440\n",
      "2022-09-30 16:17:04 epoch (16) step 50: training loss = 0.15\n",
      "2022-09-30 16:17:05 epoch (16) step 100: training loss = 0.29\n",
      "2022-09-30 16:17:05 epoch (16) step 150: training loss = 0.47\n",
      "2022-09-30 16:17:06 epoch (16) step 200: training loss = 0.72\n",
      "2022-09-30 16:17:06 epoch (16) step 250: training loss = 0.84\n",
      "2022-09-30 16:17:06 epoch (16) step 300: training loss = 1.02\n",
      "2022-09-30 16:17:07 epoch (16) step 350: training loss = 1.29\n",
      "2022-09-30 16:17:07 confusion matrix:\n",
      "2022-09-30 16:17:07 [[518  66]\n",
      " [ 37  51]]\n",
      "2022-09-30 16:17:07 accuracy = 0.847\n",
      "2022-09-30 16:17:07 precision class 0 = 0.933\n",
      "2022-09-30 16:17:07 precision class 1 = 0.436\n",
      "2022-09-30 16:17:07 recall class 0 = 0.887\n",
      "2022-09-30 16:17:07 recall class 1 = 0.580\n",
      "2022-09-30 16:17:07 AUC of ROC = 0.818\n",
      "2022-09-30 16:17:07 AUC of PRC = 0.453\n",
      "2022-09-30 16:17:08 epoch (17) step 50: training loss = 0.18\n",
      "2022-09-30 16:17:08 epoch (17) step 100: training loss = 0.36\n",
      "2022-09-30 16:17:09 epoch (17) step 150: training loss = 0.65\n",
      "2022-09-30 16:17:09 epoch (17) step 200: training loss = 0.86\n",
      "2022-09-30 16:17:10 epoch (17) step 250: training loss = 1.15\n",
      "2022-09-30 16:17:10 epoch (17) step 300: training loss = 1.38\n",
      "2022-09-30 16:17:11 epoch (17) step 350: training loss = 1.62\n",
      "2022-09-30 16:17:11 confusion matrix:\n",
      "2022-09-30 16:17:11 [[545  39]\n",
      " [ 52  36]]\n",
      "2022-09-30 16:17:11 accuracy = 0.865\n",
      "2022-09-30 16:17:11 precision class 0 = 0.913\n",
      "2022-09-30 16:17:11 precision class 1 = 0.480\n",
      "2022-09-30 16:17:11 recall class 0 = 0.933\n",
      "2022-09-30 16:17:11 recall class 1 = 0.409\n",
      "2022-09-30 16:17:11 AUC of ROC = 0.813\n",
      "2022-09-30 16:17:11 AUC of PRC = 0.455\n",
      "2022-09-30 16:17:12 epoch (18) step 50: training loss = 0.30\n",
      "2022-09-30 16:17:12 epoch (18) step 100: training loss = 0.40\n",
      "2022-09-30 16:17:13 epoch (18) step 150: training loss = 0.57\n",
      "2022-09-30 16:17:13 epoch (18) step 200: training loss = 0.76\n",
      "2022-09-30 16:17:13 epoch (18) step 250: training loss = 0.94\n",
      "2022-09-30 16:17:14 epoch (18) step 300: training loss = 1.15\n",
      "2022-09-30 16:17:14 epoch (18) step 350: training loss = 1.40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-30 16:17:15 confusion matrix:\n",
      "2022-09-30 16:17:15 [[531  53]\n",
      " [ 52  36]]\n",
      "2022-09-30 16:17:15 accuracy = 0.844\n",
      "2022-09-30 16:17:15 precision class 0 = 0.911\n",
      "2022-09-30 16:17:15 precision class 1 = 0.404\n",
      "2022-09-30 16:17:15 recall class 0 = 0.909\n",
      "2022-09-30 16:17:15 recall class 1 = 0.409\n",
      "2022-09-30 16:17:15 AUC of ROC = 0.810\n",
      "2022-09-30 16:17:15 AUC of PRC = 0.425\n",
      "2022-09-30 16:17:15 epoch (19) step 50: training loss = 0.12\n",
      "2022-09-30 16:17:16 epoch (19) step 100: training loss = 0.32\n",
      "2022-09-30 16:17:16 epoch (19) step 150: training loss = 0.55\n",
      "2022-09-30 16:17:17 epoch (19) step 200: training loss = 0.74\n",
      "2022-09-30 16:17:17 epoch (19) step 250: training loss = 0.97\n",
      "2022-09-30 16:17:18 epoch (19) step 300: training loss = 1.22\n",
      "2022-09-30 16:17:18 epoch (19) step 350: training loss = 1.43\n",
      "2022-09-30 16:17:19 confusion matrix:\n",
      "2022-09-30 16:17:19 [[554  30]\n",
      " [ 60  28]]\n",
      "2022-09-30 16:17:19 accuracy = 0.866\n",
      "2022-09-30 16:17:19 precision class 0 = 0.902\n",
      "2022-09-30 16:17:19 precision class 1 = 0.483\n",
      "2022-09-30 16:17:19 recall class 0 = 0.949\n",
      "2022-09-30 16:17:19 recall class 1 = 0.318\n",
      "2022-09-30 16:17:19 AUC of ROC = 0.800\n",
      "2022-09-30 16:17:19 AUC of PRC = 0.424\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-30 16:17:38 {'dim': 16, 'dropout': 0.3, 'batch_size': 8, 'lr': 0.001, 'epochs': 20, 'emb_size': 16, 'aggregation_type': 'last_state', 'bidirectional': False, 'seed': 42, 'steps': 50, 'data': 'test_text_data_2/in-hospital-mortality', 'notes': 'test_text_data_2/train', 'timestep': 1.0, 'imputation': 'previous', 'normalizer_state': None, 'model name': 'last_stateModel', 'model': 'lstm'}\n",
      "2022-09-30 16:17:38 LSTMClassifier(\n",
      "  (encoder_layer): TransformerEncoderLayer(\n",
      "    (self_attn): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=76, out_features=76, bias=True)\n",
      "    )\n",
      "    (linear1): Linear(in_features=76, out_features=2048, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (linear2): Linear(in_features=2048, out_features=76, bias=True)\n",
      "    (norm1): LayerNorm((76,), eps=1e-05, elementwise_affine=True)\n",
      "    (norm2): LayerNorm((76,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout1): Dropout(p=0.1, inplace=False)\n",
      "    (dropout2): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (hidden2out): Linear(in_features=16, out_features=1, bias=True)\n",
      "  (comb): Linear(in_features=16, out_features=16, bias=True)\n",
      "  (fc1): Linear(in_features=76, out_features=76, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (dout): Dropout(p=0.3, inplace=False)\n",
      "  (lstm): LSTM(76, 16, batch_first=True)\n",
      "  (fc2): Linear(in_features=16, out_features=16, bias=True)\n",
      "  (out): Linear(in_features=16, out_features=1, bias=True)\n",
      ")\n",
      "2022-09-30 16:17:39 epoch (0) step 50: training loss = 0.54\n",
      "2022-09-30 16:17:39 epoch (0) step 100: training loss = 0.93\n",
      "2022-09-30 16:17:40 epoch (0) step 150: training loss = 1.35\n",
      "2022-09-30 16:17:40 epoch (0) step 200: training loss = 1.78\n",
      "2022-09-30 16:17:41 epoch (0) step 250: training loss = 2.09\n",
      "2022-09-30 16:17:41 epoch (0) step 300: training loss = 2.54\n",
      "2022-09-30 16:17:42 epoch (0) step 350: training loss = 2.94\n",
      "2022-09-30 16:17:42 confusion matrix:\n",
      "2022-09-30 16:17:42 [[584   0]\n",
      " [ 87   1]]\n",
      "2022-09-30 16:17:42 accuracy = 0.871\n",
      "2022-09-30 16:17:42 precision class 0 = 0.870\n",
      "2022-09-30 16:17:42 precision class 1 = 1.000\n",
      "2022-09-30 16:17:42 recall class 0 = 1.000\n",
      "2022-09-30 16:17:42 recall class 1 = 0.011\n",
      "2022-09-30 16:17:42 AUC of ROC = 0.848\n",
      "2022-09-30 16:17:42 AUC of PRC = 0.481\n",
      "2022-09-30 16:17:43 epoch (1) step 50: training loss = 0.46\n",
      "2022-09-30 16:17:43 epoch (1) step 100: training loss = 0.73\n",
      "2022-09-30 16:17:44 epoch (1) step 150: training loss = 0.98\n",
      "2022-09-30 16:17:44 epoch (1) step 200: training loss = 1.47\n",
      "2022-09-30 16:17:44 epoch (1) step 250: training loss = 1.78\n",
      "2022-09-30 16:17:45 epoch (1) step 300: training loss = 2.05\n",
      "2022-09-30 16:17:45 epoch (1) step 350: training loss = 2.31\n",
      "2022-09-30 16:17:46 confusion matrix:\n",
      "2022-09-30 16:17:46 [[567  17]\n",
      " [ 58  30]]\n",
      "2022-09-30 16:17:46 accuracy = 0.888\n",
      "2022-09-30 16:17:46 precision class 0 = 0.907\n",
      "2022-09-30 16:17:46 precision class 1 = 0.638\n",
      "2022-09-30 16:17:46 recall class 0 = 0.971\n",
      "2022-09-30 16:17:46 recall class 1 = 0.341\n",
      "2022-09-30 16:17:46 AUC of ROC = 0.855\n",
      "2022-09-30 16:17:46 AUC of PRC = 0.504\n",
      "2022-09-30 16:17:47 epoch (2) step 50: training loss = 0.32\n",
      "2022-09-30 16:17:47 epoch (2) step 100: training loss = 0.86\n",
      "2022-09-30 16:17:48 epoch (2) step 150: training loss = 1.13\n",
      "2022-09-30 16:17:48 epoch (2) step 200: training loss = 1.48\n",
      "2022-09-30 16:17:49 epoch (2) step 250: training loss = 1.86\n",
      "2022-09-30 16:17:49 epoch (2) step 300: training loss = 2.19\n",
      "2022-09-30 16:17:50 epoch (2) step 350: training loss = 2.42\n",
      "2022-09-30 16:17:50 confusion matrix:\n",
      "2022-09-30 16:17:50 [[566  18]\n",
      " [ 59  29]]\n",
      "2022-09-30 16:17:50 accuracy = 0.885\n",
      "2022-09-30 16:17:50 precision class 0 = 0.906\n",
      "2022-09-30 16:17:50 precision class 1 = 0.617\n",
      "2022-09-30 16:17:50 recall class 0 = 0.969\n",
      "2022-09-30 16:17:50 recall class 1 = 0.330\n",
      "2022-09-30 16:17:50 AUC of ROC = 0.860\n",
      "2022-09-30 16:17:50 AUC of PRC = 0.488\n",
      "2022-09-30 16:17:51 epoch (3) step 50: training loss = 0.20\n",
      "2022-09-30 16:17:51 epoch (3) step 100: training loss = 0.49\n",
      "2022-09-30 16:17:51 epoch (3) step 150: training loss = 0.78\n",
      "2022-09-30 16:17:52 epoch (3) step 200: training loss = 1.12\n",
      "2022-09-30 16:17:52 epoch (3) step 250: training loss = 1.53\n",
      "2022-09-30 16:17:53 epoch (3) step 300: training loss = 1.67\n",
      "2022-09-30 16:17:53 epoch (3) step 350: training loss = 2.02\n",
      "2022-09-30 16:17:54 confusion matrix:\n",
      "2022-09-30 16:17:54 [[551  33]\n",
      " [ 48  40]]\n",
      "2022-09-30 16:17:54 accuracy = 0.879\n",
      "2022-09-30 16:17:54 precision class 0 = 0.920\n",
      "2022-09-30 16:17:54 precision class 1 = 0.548\n",
      "2022-09-30 16:17:54 recall class 0 = 0.943\n",
      "2022-09-30 16:17:54 recall class 1 = 0.455\n",
      "2022-09-30 16:17:54 AUC of ROC = 0.862\n",
      "2022-09-30 16:17:54 AUC of PRC = 0.490\n",
      "2022-09-30 16:17:54 epoch (4) step 50: training loss = 0.35\n",
      "2022-09-30 16:17:55 epoch (4) step 100: training loss = 0.59\n",
      "2022-09-30 16:17:55 epoch (4) step 150: training loss = 0.78\n",
      "2022-09-30 16:17:56 epoch (4) step 200: training loss = 1.10\n",
      "2022-09-30 16:17:56 epoch (4) step 250: training loss = 1.35\n",
      "2022-09-30 16:17:57 epoch (4) step 300: training loss = 1.88\n",
      "2022-09-30 16:17:58 epoch (4) step 350: training loss = 2.35\n",
      "2022-09-30 16:17:58 confusion matrix:\n",
      "2022-09-30 16:17:58 [[565  19]\n",
      " [ 65  23]]\n",
      "2022-09-30 16:17:58 accuracy = 0.875\n",
      "2022-09-30 16:17:58 precision class 0 = 0.897\n",
      "2022-09-30 16:17:58 precision class 1 = 0.548\n",
      "2022-09-30 16:17:58 recall class 0 = 0.967\n",
      "2022-09-30 16:17:58 recall class 1 = 0.261\n",
      "2022-09-30 16:17:58 AUC of ROC = 0.865\n",
      "2022-09-30 16:17:58 AUC of PRC = 0.506\n",
      "2022-09-30 16:17:59 epoch (5) step 50: training loss = 0.30\n",
      "2022-09-30 16:17:59 epoch (5) step 100: training loss = 0.74\n",
      "2022-09-30 16:18:00 epoch (5) step 150: training loss = 1.08\n",
      "2022-09-30 16:18:00 epoch (5) step 200: training loss = 1.29\n",
      "2022-09-30 16:18:00 epoch (5) step 250: training loss = 1.60\n",
      "2022-09-30 16:18:01 epoch (5) step 300: training loss = 1.85\n",
      "2022-09-30 16:18:01 epoch (5) step 350: training loss = 2.30\n",
      "2022-09-30 16:18:02 confusion matrix:\n",
      "2022-09-30 16:18:02 [[565  19]\n",
      " [ 58  30]]\n",
      "2022-09-30 16:18:02 accuracy = 0.885\n",
      "2022-09-30 16:18:02 precision class 0 = 0.907\n",
      "2022-09-30 16:18:02 precision class 1 = 0.612\n",
      "2022-09-30 16:18:02 recall class 0 = 0.967\n",
      "2022-09-30 16:18:02 recall class 1 = 0.341\n",
      "2022-09-30 16:18:02 AUC of ROC = 0.870\n",
      "2022-09-30 16:18:02 AUC of PRC = 0.530\n",
      "2022-09-30 16:18:02 epoch (6) step 50: training loss = 0.33\n",
      "2022-09-30 16:18:03 epoch (6) step 100: training loss = 0.49\n",
      "2022-09-30 16:18:03 epoch (6) step 150: training loss = 0.77\n",
      "2022-09-30 16:18:04 epoch (6) step 200: training loss = 0.94\n",
      "2022-09-30 16:18:04 epoch (6) step 250: training loss = 1.20\n",
      "2022-09-30 16:18:05 epoch (6) step 300: training loss = 1.48\n",
      "2022-09-30 16:18:05 epoch (6) step 350: training loss = 2.00\n",
      "2022-09-30 16:18:06 confusion matrix:\n",
      "2022-09-30 16:18:06 [[557  27]\n",
      " [ 58  30]]\n",
      "2022-09-30 16:18:06 accuracy = 0.874\n",
      "2022-09-30 16:18:06 precision class 0 = 0.906\n",
      "2022-09-30 16:18:06 precision class 1 = 0.526\n",
      "2022-09-30 16:18:06 recall class 0 = 0.954\n",
      "2022-09-30 16:18:06 recall class 1 = 0.341\n",
      "2022-09-30 16:18:06 AUC of ROC = 0.859\n",
      "2022-09-30 16:18:06 AUC of PRC = 0.511\n",
      "2022-09-30 16:18:06 epoch (7) step 50: training loss = 0.20\n",
      "2022-09-30 16:18:07 epoch (7) step 100: training loss = 0.46\n",
      "2022-09-30 16:18:07 epoch (7) step 150: training loss = 0.78\n",
      "2022-09-30 16:18:07 epoch (7) step 200: training loss = 1.08\n",
      "2022-09-30 16:18:08 epoch (7) step 250: training loss = 1.47\n",
      "2022-09-30 16:18:08 epoch (7) step 300: training loss = 1.81\n",
      "2022-09-30 16:18:09 epoch (7) step 350: training loss = 1.95\n",
      "2022-09-30 16:18:09 confusion matrix:\n",
      "2022-09-30 16:18:09 [[558  26]\n",
      " [ 52  36]]\n",
      "2022-09-30 16:18:09 accuracy = 0.884\n",
      "2022-09-30 16:18:09 precision class 0 = 0.915\n",
      "2022-09-30 16:18:09 precision class 1 = 0.581\n",
      "2022-09-30 16:18:09 recall class 0 = 0.955\n",
      "2022-09-30 16:18:09 recall class 1 = 0.409\n",
      "2022-09-30 16:18:09 AUC of ROC = 0.861\n",
      "2022-09-30 16:18:09 AUC of PRC = 0.513\n",
      "2022-09-30 16:18:10 epoch (8) step 50: training loss = 0.26\n",
      "2022-09-30 16:18:10 epoch (8) step 100: training loss = 0.55\n",
      "2022-09-30 16:18:11 epoch (8) step 150: training loss = 0.83\n",
      "2022-09-30 16:18:11 epoch (8) step 200: training loss = 1.02\n",
      "2022-09-30 16:18:12 epoch (8) step 250: training loss = 1.24\n",
      "2022-09-30 16:18:12 epoch (8) step 300: training loss = 1.45\n",
      "2022-09-30 16:18:12 epoch (8) step 350: training loss = 1.81\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-30 16:18:13 confusion matrix:\n",
      "2022-09-30 16:18:13 [[552  32]\n",
      " [ 50  38]]\n",
      "2022-09-30 16:18:13 accuracy = 0.878\n",
      "2022-09-30 16:18:13 precision class 0 = 0.917\n",
      "2022-09-30 16:18:13 precision class 1 = 0.543\n",
      "2022-09-30 16:18:13 recall class 0 = 0.945\n",
      "2022-09-30 16:18:13 recall class 1 = 0.432\n",
      "2022-09-30 16:18:13 AUC of ROC = 0.864\n",
      "2022-09-30 16:18:13 AUC of PRC = 0.526\n",
      "2022-09-30 16:18:13 epoch (9) step 50: training loss = 0.23\n",
      "2022-09-30 16:18:14 epoch (9) step 100: training loss = 0.42\n",
      "2022-09-30 16:18:14 epoch (9) step 150: training loss = 0.82\n",
      "2022-09-30 16:18:15 epoch (9) step 200: training loss = 1.01\n",
      "2022-09-30 16:18:15 epoch (9) step 250: training loss = 1.24\n",
      "2022-09-30 16:18:16 epoch (9) step 300: training loss = 1.46\n",
      "2022-09-30 16:18:16 epoch (9) step 350: training loss = 1.63\n",
      "2022-09-30 16:18:17 confusion matrix:\n",
      "2022-09-30 16:18:17 [[546  38]\n",
      " [ 51  37]]\n",
      "2022-09-30 16:18:17 accuracy = 0.868\n",
      "2022-09-30 16:18:17 precision class 0 = 0.915\n",
      "2022-09-30 16:18:17 precision class 1 = 0.493\n",
      "2022-09-30 16:18:17 recall class 0 = 0.935\n",
      "2022-09-30 16:18:17 recall class 1 = 0.420\n",
      "2022-09-30 16:18:17 AUC of ROC = 0.837\n",
      "2022-09-30 16:18:17 AUC of PRC = 0.482\n",
      "2022-09-30 16:18:17 epoch (10) step 50: training loss = 0.19\n",
      "2022-09-30 16:18:17 epoch (10) step 100: training loss = 0.40\n",
      "2022-09-30 16:18:18 epoch (10) step 150: training loss = 0.62\n",
      "2022-09-30 16:18:18 epoch (10) step 200: training loss = 0.77\n",
      "2022-09-30 16:18:19 epoch (10) step 250: training loss = 0.92\n",
      "2022-09-30 16:18:19 epoch (10) step 300: training loss = 1.20\n",
      "2022-09-30 16:18:20 epoch (10) step 350: training loss = 1.49\n",
      "2022-09-30 16:18:20 confusion matrix:\n",
      "2022-09-30 16:18:20 [[541  43]\n",
      " [ 44  44]]\n",
      "2022-09-30 16:18:20 accuracy = 0.871\n",
      "2022-09-30 16:18:20 precision class 0 = 0.925\n",
      "2022-09-30 16:18:20 precision class 1 = 0.506\n",
      "2022-09-30 16:18:20 recall class 0 = 0.926\n",
      "2022-09-30 16:18:20 recall class 1 = 0.500\n",
      "2022-09-30 16:18:20 AUC of ROC = 0.856\n",
      "2022-09-30 16:18:20 AUC of PRC = 0.485\n",
      "2022-09-30 16:18:21 epoch (11) step 50: training loss = 0.11\n",
      "2022-09-30 16:18:21 epoch (11) step 100: training loss = 0.42\n",
      "2022-09-30 16:18:22 epoch (11) step 150: training loss = 0.47\n",
      "2022-09-30 16:18:22 epoch (11) step 200: training loss = 0.72\n",
      "2022-09-30 16:18:22 epoch (11) step 250: training loss = 0.88\n",
      "2022-09-30 16:18:23 epoch (11) step 300: training loss = 1.13\n",
      "2022-09-30 16:18:23 epoch (11) step 350: training loss = 1.50\n",
      "2022-09-30 16:18:24 confusion matrix:\n",
      "2022-09-30 16:18:24 [[554  30]\n",
      " [ 55  33]]\n",
      "2022-09-30 16:18:24 accuracy = 0.874\n",
      "2022-09-30 16:18:24 precision class 0 = 0.910\n",
      "2022-09-30 16:18:24 precision class 1 = 0.524\n",
      "2022-09-30 16:18:24 recall class 0 = 0.949\n",
      "2022-09-30 16:18:24 recall class 1 = 0.375\n",
      "2022-09-30 16:18:24 AUC of ROC = 0.840\n",
      "2022-09-30 16:18:24 AUC of PRC = 0.470\n",
      "2022-09-30 16:18:24 epoch (12) step 50: training loss = 0.29\n",
      "2022-09-30 16:18:25 epoch (12) step 100: training loss = 0.58\n",
      "2022-09-30 16:18:25 epoch (12) step 150: training loss = 0.86\n",
      "2022-09-30 16:18:25 epoch (12) step 200: training loss = 1.06\n",
      "2022-09-30 16:18:26 epoch (12) step 250: training loss = 1.32\n",
      "2022-09-30 16:18:26 epoch (12) step 300: training loss = 1.42\n",
      "2022-09-30 16:18:27 epoch (12) step 350: training loss = 1.64\n",
      "2022-09-30 16:18:27 confusion matrix:\n",
      "2022-09-30 16:18:27 [[553  31]\n",
      " [ 49  39]]\n",
      "2022-09-30 16:18:27 accuracy = 0.881\n",
      "2022-09-30 16:18:27 precision class 0 = 0.919\n",
      "2022-09-30 16:18:27 precision class 1 = 0.557\n",
      "2022-09-30 16:18:27 recall class 0 = 0.947\n",
      "2022-09-30 16:18:27 recall class 1 = 0.443\n",
      "2022-09-30 16:18:27 AUC of ROC = 0.831\n",
      "2022-09-30 16:18:27 AUC of PRC = 0.472\n",
      "2022-09-30 16:18:28 epoch (13) step 50: training loss = 0.13\n",
      "2022-09-30 16:18:28 epoch (13) step 100: training loss = 0.24\n",
      "2022-09-30 16:18:29 epoch (13) step 150: training loss = 0.30\n",
      "2022-09-30 16:18:29 epoch (13) step 200: training loss = 0.46\n",
      "2022-09-30 16:18:30 epoch (13) step 250: training loss = 0.61\n",
      "2022-09-30 16:18:30 epoch (13) step 300: training loss = 0.79\n",
      "2022-09-30 16:18:31 epoch (13) step 350: training loss = 1.08\n",
      "2022-09-30 16:18:31 confusion matrix:\n",
      "2022-09-30 16:18:31 [[551  33]\n",
      " [ 59  29]]\n",
      "2022-09-30 16:18:31 accuracy = 0.863\n",
      "2022-09-30 16:18:31 precision class 0 = 0.903\n",
      "2022-09-30 16:18:31 precision class 1 = 0.468\n",
      "2022-09-30 16:18:31 recall class 0 = 0.943\n",
      "2022-09-30 16:18:31 recall class 1 = 0.330\n",
      "2022-09-30 16:18:31 AUC of ROC = 0.831\n",
      "2022-09-30 16:18:31 AUC of PRC = 0.457\n",
      "2022-09-30 16:18:31 epoch (14) step 50: training loss = 0.11\n",
      "2022-09-30 16:18:32 epoch (14) step 100: training loss = 0.21\n",
      "2022-09-30 16:18:32 epoch (14) step 150: training loss = 0.41\n",
      "2022-09-30 16:18:33 epoch (14) step 200: training loss = 0.63\n",
      "2022-09-30 16:18:33 epoch (14) step 250: training loss = 0.92\n",
      "2022-09-30 16:18:34 epoch (14) step 300: training loss = 1.00\n",
      "2022-09-30 16:18:34 epoch (14) step 350: training loss = 1.14\n",
      "2022-09-30 16:18:35 confusion matrix:\n",
      "2022-09-30 16:18:35 [[553  31]\n",
      " [ 52  36]]\n",
      "2022-09-30 16:18:35 accuracy = 0.876\n",
      "2022-09-30 16:18:35 precision class 0 = 0.914\n",
      "2022-09-30 16:18:35 precision class 1 = 0.537\n",
      "2022-09-30 16:18:35 recall class 0 = 0.947\n",
      "2022-09-30 16:18:35 recall class 1 = 0.409\n",
      "2022-09-30 16:18:35 AUC of ROC = 0.796\n",
      "2022-09-30 16:18:35 AUC of PRC = 0.447\n",
      "2022-09-30 16:18:35 epoch (15) step 50: training loss = 0.23\n",
      "2022-09-30 16:18:36 epoch (15) step 100: training loss = 0.53\n",
      "2022-09-30 16:18:36 epoch (15) step 150: training loss = 0.62\n",
      "2022-09-30 16:18:36 epoch (15) step 200: training loss = 0.73\n",
      "2022-09-30 16:18:37 epoch (15) step 250: training loss = 0.89\n",
      "2022-09-30 16:18:37 epoch (15) step 300: training loss = 1.09\n",
      "2022-09-30 16:18:38 epoch (15) step 350: training loss = 1.31\n",
      "2022-09-30 16:18:38 confusion matrix:\n",
      "2022-09-30 16:18:38 [[543  41]\n",
      " [ 44  44]]\n",
      "2022-09-30 16:18:38 accuracy = 0.874\n",
      "2022-09-30 16:18:38 precision class 0 = 0.925\n",
      "2022-09-30 16:18:38 precision class 1 = 0.518\n",
      "2022-09-30 16:18:38 recall class 0 = 0.930\n",
      "2022-09-30 16:18:38 recall class 1 = 0.500\n",
      "2022-09-30 16:18:38 AUC of ROC = 0.819\n",
      "2022-09-30 16:18:38 AUC of PRC = 0.474\n",
      "2022-09-30 16:18:39 epoch (16) step 50: training loss = 0.15\n",
      "2022-09-30 16:18:39 epoch (16) step 100: training loss = 0.20\n",
      "2022-09-30 16:18:40 epoch (16) step 150: training loss = 0.37\n",
      "2022-09-30 16:18:40 epoch (16) step 200: training loss = 0.53\n",
      "2022-09-30 16:18:40 epoch (16) step 250: training loss = 0.58\n",
      "2022-09-30 16:18:41 epoch (16) step 300: training loss = 0.77\n",
      "2022-09-30 16:18:41 epoch (16) step 350: training loss = 0.97\n",
      "2022-09-30 16:18:42 confusion matrix:\n",
      "2022-09-30 16:18:42 [[533  51]\n",
      " [ 47  41]]\n",
      "2022-09-30 16:18:42 accuracy = 0.854\n",
      "2022-09-30 16:18:42 precision class 0 = 0.919\n",
      "2022-09-30 16:18:42 precision class 1 = 0.446\n",
      "2022-09-30 16:18:42 recall class 0 = 0.913\n",
      "2022-09-30 16:18:42 recall class 1 = 0.466\n",
      "2022-09-30 16:18:42 AUC of ROC = 0.807\n",
      "2022-09-30 16:18:42 AUC of PRC = 0.450\n",
      "2022-09-30 16:18:42 epoch (17) step 50: training loss = 0.11\n",
      "2022-09-30 16:18:43 epoch (17) step 100: training loss = 0.23\n",
      "2022-09-30 16:18:43 epoch (17) step 150: training loss = 0.38\n",
      "2022-09-30 16:18:44 epoch (17) step 200: training loss = 0.56\n",
      "2022-09-30 16:18:44 epoch (17) step 250: training loss = 0.72\n",
      "2022-09-30 16:18:45 epoch (17) step 300: training loss = 0.87\n",
      "2022-09-30 16:18:45 epoch (17) step 350: training loss = 1.01\n",
      "2022-09-30 16:18:45 confusion matrix:\n",
      "2022-09-30 16:18:45 [[552  32]\n",
      " [ 58  30]]\n",
      "2022-09-30 16:18:45 accuracy = 0.866\n",
      "2022-09-30 16:18:45 precision class 0 = 0.905\n",
      "2022-09-30 16:18:45 precision class 1 = 0.484\n",
      "2022-09-30 16:18:45 recall class 0 = 0.945\n",
      "2022-09-30 16:18:45 recall class 1 = 0.341\n",
      "2022-09-30 16:18:45 AUC of ROC = 0.793\n",
      "2022-09-30 16:18:45 AUC of PRC = 0.429\n",
      "2022-09-30 16:18:46 epoch (18) step 50: training loss = 0.22\n",
      "2022-09-30 16:18:46 epoch (18) step 100: training loss = 0.30\n",
      "2022-09-30 16:18:47 epoch (18) step 150: training loss = 0.37\n",
      "2022-09-30 16:18:47 epoch (18) step 200: training loss = 0.44\n",
      "2022-09-30 16:18:48 epoch (18) step 250: training loss = 0.56\n",
      "2022-09-30 16:18:48 epoch (18) step 300: training loss = 0.71\n",
      "2022-09-30 16:18:49 epoch (18) step 350: training loss = 0.99\n",
      "2022-09-30 16:18:49 confusion matrix:\n",
      "2022-09-30 16:18:49 [[537  47]\n",
      " [ 43  45]]\n",
      "2022-09-30 16:18:49 accuracy = 0.866\n",
      "2022-09-30 16:18:49 precision class 0 = 0.926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-30 16:18:49 precision class 1 = 0.489\n",
      "2022-09-30 16:18:49 recall class 0 = 0.920\n",
      "2022-09-30 16:18:49 recall class 1 = 0.511\n",
      "2022-09-30 16:18:49 AUC of ROC = 0.793\n",
      "2022-09-30 16:18:49 AUC of PRC = 0.473\n",
      "2022-09-30 16:18:50 epoch (19) step 50: training loss = 0.04\n",
      "2022-09-30 16:18:50 epoch (19) step 100: training loss = 0.25\n",
      "2022-09-30 16:18:50 epoch (19) step 150: training loss = 0.37\n",
      "2022-09-30 16:18:51 epoch (19) step 200: training loss = 0.48\n",
      "2022-09-30 16:18:51 epoch (19) step 250: training loss = 0.60\n",
      "2022-09-30 16:18:52 epoch (19) step 300: training loss = 0.74\n",
      "2022-09-30 16:18:52 epoch (19) step 350: training loss = 0.85\n",
      "2022-09-30 16:18:53 confusion matrix:\n",
      "2022-09-30 16:18:53 [[553  31]\n",
      " [ 56  32]]\n",
      "2022-09-30 16:18:53 accuracy = 0.871\n",
      "2022-09-30 16:18:53 precision class 0 = 0.908\n",
      "2022-09-30 16:18:53 precision class 1 = 0.508\n",
      "2022-09-30 16:18:53 recall class 0 = 0.947\n",
      "2022-09-30 16:18:53 recall class 1 = 0.364\n",
      "2022-09-30 16:18:53 AUC of ROC = 0.776\n",
      "2022-09-30 16:18:53 AUC of PRC = 0.415\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lastState done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-30 16:19:13 {'dim': 16, 'dropout': 0.3, 'batch_size': 8, 'lr': 0.001, 'epochs': 20, 'emb_size': 16, 'aggregation_type': 'mean', 'bidirectional': False, 'seed': 42, 'steps': 50, 'data': 'test_text_data_2/in-hospital-mortality', 'notes': 'test_text_data_2/train', 'timestep': 1.0, 'imputation': 'previous', 'normalizer_state': None, 'model name': 'gruModel', 'model': 'gru'}\n",
      "2022-09-30 16:19:13 LSTMClassifier(\n",
      "  (encoder_layer): TransformerEncoderLayer(\n",
      "    (self_attn): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=76, out_features=76, bias=True)\n",
      "    )\n",
      "    (linear1): Linear(in_features=76, out_features=2048, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (linear2): Linear(in_features=2048, out_features=76, bias=True)\n",
      "    (norm1): LayerNorm((76,), eps=1e-05, elementwise_affine=True)\n",
      "    (norm2): LayerNorm((76,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout1): Dropout(p=0.1, inplace=False)\n",
      "    (dropout2): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (hidden2out): Linear(in_features=16, out_features=1, bias=True)\n",
      "  (comb): Linear(in_features=16, out_features=16, bias=True)\n",
      "  (fc1): Linear(in_features=76, out_features=76, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (dout): Dropout(p=0.3, inplace=False)\n",
      "  (gru): GRU(76, 16, batch_first=True)\n",
      "  (fc2): Linear(in_features=16, out_features=16, bias=True)\n",
      "  (out): Linear(in_features=16, out_features=1, bias=True)\n",
      ")\n",
      "2022-09-30 16:19:13 epoch (0) step 50: training loss = 0.61\n",
      "2022-09-30 16:19:13 epoch (0) step 100: training loss = 0.90\n",
      "2022-09-30 16:19:14 epoch (0) step 150: training loss = 1.40\n",
      "2022-09-30 16:19:15 epoch (0) step 200: training loss = 1.88\n",
      "2022-09-30 16:19:15 epoch (0) step 250: training loss = 2.34\n",
      "2022-09-30 16:19:15 epoch (0) step 300: training loss = 2.89\n",
      "2022-09-30 16:19:16 epoch (0) step 350: training loss = 3.30\n",
      "2022-09-30 16:19:16 confusion matrix:\n",
      "2022-09-30 16:19:16 [[580   4]\n",
      " [ 85   3]]\n",
      "2022-09-30 16:19:16 accuracy = 0.868\n",
      "2022-09-30 16:19:16 precision class 0 = 0.872\n",
      "2022-09-30 16:19:16 precision class 1 = 0.429\n",
      "2022-09-30 16:19:16 recall class 0 = 0.993\n",
      "2022-09-30 16:19:16 recall class 1 = 0.034\n",
      "2022-09-30 16:19:16 AUC of ROC = 0.822\n",
      "2022-09-30 16:19:16 AUC of PRC = 0.414\n",
      "2022-09-30 16:19:17 epoch (1) step 50: training loss = 0.27\n",
      "2022-09-30 16:19:17 epoch (1) step 100: training loss = 0.63\n",
      "2022-09-30 16:19:18 epoch (1) step 150: training loss = 1.04\n",
      "2022-09-30 16:19:18 epoch (1) step 200: training loss = 1.44\n",
      "2022-09-30 16:19:19 epoch (1) step 250: training loss = 1.86\n",
      "2022-09-30 16:19:19 epoch (1) step 300: training loss = 2.30\n",
      "2022-09-30 16:19:20 epoch (1) step 350: training loss = 2.70\n",
      "2022-09-30 16:19:20 confusion matrix:\n",
      "2022-09-30 16:19:20 [[572  12]\n",
      " [ 72  16]]\n",
      "2022-09-30 16:19:20 accuracy = 0.875\n",
      "2022-09-30 16:19:20 precision class 0 = 0.888\n",
      "2022-09-30 16:19:20 precision class 1 = 0.571\n",
      "2022-09-30 16:19:20 recall class 0 = 0.979\n",
      "2022-09-30 16:19:20 recall class 1 = 0.182\n",
      "2022-09-30 16:19:20 AUC of ROC = 0.837\n",
      "2022-09-30 16:19:20 AUC of PRC = 0.432\n",
      "2022-09-30 16:19:21 epoch (2) step 50: training loss = 0.42\n",
      "2022-09-30 16:19:21 epoch (2) step 100: training loss = 0.77\n",
      "2022-09-30 16:19:22 epoch (2) step 150: training loss = 1.20\n",
      "2022-09-30 16:19:22 epoch (2) step 200: training loss = 1.71\n",
      "2022-09-30 16:19:23 epoch (2) step 250: training loss = 2.11\n",
      "2022-09-30 16:19:23 epoch (2) step 300: training loss = 2.47\n",
      "2022-09-30 16:19:24 epoch (2) step 350: training loss = 2.85\n",
      "2022-09-30 16:19:24 confusion matrix:\n",
      "2022-09-30 16:19:24 [[573  11]\n",
      " [ 69  19]]\n",
      "2022-09-30 16:19:24 accuracy = 0.881\n",
      "2022-09-30 16:19:24 precision class 0 = 0.893\n",
      "2022-09-30 16:19:24 precision class 1 = 0.633\n",
      "2022-09-30 16:19:24 recall class 0 = 0.981\n",
      "2022-09-30 16:19:24 recall class 1 = 0.216\n",
      "2022-09-30 16:19:24 AUC of ROC = 0.840\n",
      "2022-09-30 16:19:24 AUC of PRC = 0.459\n",
      "2022-09-30 16:19:24 epoch (3) step 50: training loss = 0.37\n",
      "2022-09-30 16:19:25 epoch (3) step 100: training loss = 0.74\n",
      "2022-09-30 16:19:25 epoch (3) step 150: training loss = 1.13\n",
      "2022-09-30 16:19:26 epoch (3) step 200: training loss = 1.47\n",
      "2022-09-30 16:19:26 epoch (3) step 250: training loss = 1.79\n",
      "2022-09-30 16:19:27 epoch (3) step 300: training loss = 2.32\n",
      "2022-09-30 16:19:27 epoch (3) step 350: training loss = 2.54\n",
      "2022-09-30 16:19:28 confusion matrix:\n",
      "2022-09-30 16:19:28 [[568  16]\n",
      " [ 63  25]]\n",
      "2022-09-30 16:19:28 accuracy = 0.882\n",
      "2022-09-30 16:19:28 precision class 0 = 0.900\n",
      "2022-09-30 16:19:28 precision class 1 = 0.610\n",
      "2022-09-30 16:19:28 recall class 0 = 0.973\n",
      "2022-09-30 16:19:28 recall class 1 = 0.284\n",
      "2022-09-30 16:19:28 AUC of ROC = 0.847\n",
      "2022-09-30 16:19:28 AUC of PRC = 0.489\n",
      "2022-09-30 16:19:28 epoch (4) step 50: training loss = 0.36\n",
      "2022-09-30 16:19:29 epoch (4) step 100: training loss = 0.65\n",
      "2022-09-30 16:19:29 epoch (4) step 150: training loss = 1.10\n",
      "2022-09-30 16:19:30 epoch (4) step 200: training loss = 1.47\n",
      "2022-09-30 16:19:30 epoch (4) step 250: training loss = 1.83\n",
      "2022-09-30 16:19:31 epoch (4) step 300: training loss = 2.26\n",
      "2022-09-30 16:19:31 epoch (4) step 350: training loss = 2.70\n",
      "2022-09-30 16:19:32 confusion matrix:\n",
      "2022-09-30 16:19:32 [[570  14]\n",
      " [ 72  16]]\n",
      "2022-09-30 16:19:32 accuracy = 0.872\n",
      "2022-09-30 16:19:32 precision class 0 = 0.888\n",
      "2022-09-30 16:19:32 precision class 1 = 0.533\n",
      "2022-09-30 16:19:32 recall class 0 = 0.976\n",
      "2022-09-30 16:19:32 recall class 1 = 0.182\n",
      "2022-09-30 16:19:32 AUC of ROC = 0.846\n",
      "2022-09-30 16:19:32 AUC of PRC = 0.485\n",
      "2022-09-30 16:19:32 epoch (5) step 50: training loss = 0.16\n",
      "2022-09-30 16:19:33 epoch (5) step 100: training loss = 0.46\n",
      "2022-09-30 16:19:33 epoch (5) step 150: training loss = 0.85\n",
      "2022-09-30 16:19:34 epoch (5) step 200: training loss = 1.45\n",
      "2022-09-30 16:19:34 epoch (5) step 250: training loss = 1.85\n",
      "2022-09-30 16:19:35 epoch (5) step 300: training loss = 2.18\n",
      "2022-09-30 16:19:35 epoch (5) step 350: training loss = 2.43\n",
      "2022-09-30 16:19:35 confusion matrix:\n",
      "2022-09-30 16:19:35 [[558  26]\n",
      " [ 53  35]]\n",
      "2022-09-30 16:19:35 accuracy = 0.882\n",
      "2022-09-30 16:19:35 precision class 0 = 0.913\n",
      "2022-09-30 16:19:35 precision class 1 = 0.574\n",
      "2022-09-30 16:19:35 recall class 0 = 0.955\n",
      "2022-09-30 16:19:35 recall class 1 = 0.398\n",
      "2022-09-30 16:19:35 AUC of ROC = 0.846\n",
      "2022-09-30 16:19:35 AUC of PRC = 0.486\n",
      "2022-09-30 16:19:36 epoch (6) step 50: training loss = 0.30\n",
      "2022-09-30 16:19:36 epoch (6) step 100: training loss = 0.59\n",
      "2022-09-30 16:19:37 epoch (6) step 150: training loss = 0.91\n",
      "2022-09-30 16:19:37 epoch (6) step 200: training loss = 1.20\n",
      "2022-09-30 16:19:38 epoch (6) step 250: training loss = 1.47\n",
      "2022-09-30 16:19:38 epoch (6) step 300: training loss = 1.81\n",
      "2022-09-30 16:19:39 epoch (6) step 350: training loss = 2.12\n",
      "2022-09-30 16:19:39 confusion matrix:\n",
      "2022-09-30 16:19:39 [[543  41]\n",
      " [ 53  35]]\n",
      "2022-09-30 16:19:39 accuracy = 0.860\n",
      "2022-09-30 16:19:39 precision class 0 = 0.911\n",
      "2022-09-30 16:19:39 precision class 1 = 0.461\n",
      "2022-09-30 16:19:39 recall class 0 = 0.930\n",
      "2022-09-30 16:19:39 recall class 1 = 0.398\n",
      "2022-09-30 16:19:39 AUC of ROC = 0.835\n",
      "2022-09-30 16:19:39 AUC of PRC = 0.474\n",
      "2022-09-30 16:19:40 epoch (7) step 50: training loss = 0.21\n",
      "2022-09-30 16:19:40 epoch (7) step 100: training loss = 0.65\n",
      "2022-09-30 16:19:41 epoch (7) step 150: training loss = 1.02\n",
      "2022-09-30 16:19:41 epoch (7) step 200: training loss = 1.35\n",
      "2022-09-30 16:19:42 epoch (7) step 250: training loss = 1.60\n",
      "2022-09-30 16:19:42 epoch (7) step 300: training loss = 1.82\n",
      "2022-09-30 16:19:43 epoch (7) step 350: training loss = 2.16\n",
      "2022-09-30 16:19:43 confusion matrix:\n",
      "2022-09-30 16:19:43 [[554  30]\n",
      " [ 52  36]]\n",
      "2022-09-30 16:19:43 accuracy = 0.878\n",
      "2022-09-30 16:19:43 precision class 0 = 0.914\n",
      "2022-09-30 16:19:43 precision class 1 = 0.545\n",
      "2022-09-30 16:19:43 recall class 0 = 0.949\n",
      "2022-09-30 16:19:43 recall class 1 = 0.409\n",
      "2022-09-30 16:19:43 AUC of ROC = 0.853\n",
      "2022-09-30 16:19:43 AUC of PRC = 0.504\n",
      "2022-09-30 16:19:44 epoch (8) step 50: training loss = 0.32\n",
      "2022-09-30 16:19:44 epoch (8) step 100: training loss = 0.70\n",
      "2022-09-30 16:19:45 epoch (8) step 150: training loss = 1.06\n",
      "2022-09-30 16:19:45 epoch (8) step 200: training loss = 1.33\n",
      "2022-09-30 16:19:46 epoch (8) step 250: training loss = 1.54\n",
      "2022-09-30 16:19:46 epoch (8) step 300: training loss = 1.80\n",
      "2022-09-30 16:19:47 epoch (8) step 350: training loss = 1.97\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-30 16:19:47 confusion matrix:\n",
      "2022-09-30 16:19:47 [[564  20]\n",
      " [ 62  26]]\n",
      "2022-09-30 16:19:47 accuracy = 0.878\n",
      "2022-09-30 16:19:47 precision class 0 = 0.901\n",
      "2022-09-30 16:19:47 precision class 1 = 0.565\n",
      "2022-09-30 16:19:47 recall class 0 = 0.966\n",
      "2022-09-30 16:19:47 recall class 1 = 0.295\n",
      "2022-09-30 16:19:47 AUC of ROC = 0.843\n",
      "2022-09-30 16:19:47 AUC of PRC = 0.473\n",
      "2022-09-30 16:19:48 epoch (9) step 50: training loss = 0.42\n",
      "2022-09-30 16:19:48 epoch (9) step 100: training loss = 0.86\n",
      "2022-09-30 16:19:48 epoch (9) step 150: training loss = 1.28\n",
      "2022-09-30 16:19:49 epoch (9) step 200: training loss = 1.60\n",
      "2022-09-30 16:19:49 epoch (9) step 250: training loss = 1.89\n",
      "2022-09-30 16:19:50 epoch (9) step 300: training loss = 2.17\n",
      "2022-09-30 16:19:50 epoch (9) step 350: training loss = 2.57\n",
      "2022-09-30 16:19:51 confusion matrix:\n",
      "2022-09-30 16:19:51 [[555  29]\n",
      " [ 56  32]]\n",
      "2022-09-30 16:19:51 accuracy = 0.874\n",
      "2022-09-30 16:19:51 precision class 0 = 0.908\n",
      "2022-09-30 16:19:51 precision class 1 = 0.525\n",
      "2022-09-30 16:19:51 recall class 0 = 0.950\n",
      "2022-09-30 16:19:51 recall class 1 = 0.364\n",
      "2022-09-30 16:19:51 AUC of ROC = 0.848\n",
      "2022-09-30 16:19:51 AUC of PRC = 0.495\n",
      "2022-09-30 16:19:51 epoch (10) step 50: training loss = 0.35\n",
      "2022-09-30 16:19:52 epoch (10) step 100: training loss = 0.59\n",
      "2022-09-30 16:19:52 epoch (10) step 150: training loss = 0.89\n",
      "2022-09-30 16:19:53 epoch (10) step 200: training loss = 1.20\n",
      "2022-09-30 16:19:53 epoch (10) step 250: training loss = 1.70\n",
      "2022-09-30 16:19:54 epoch (10) step 300: training loss = 2.16\n",
      "2022-09-30 16:19:54 epoch (10) step 350: training loss = 2.46\n",
      "2022-09-30 16:19:55 confusion matrix:\n",
      "2022-09-30 16:19:55 [[548  36]\n",
      " [ 52  36]]\n",
      "2022-09-30 16:19:55 accuracy = 0.869\n",
      "2022-09-30 16:19:55 precision class 0 = 0.913\n",
      "2022-09-30 16:19:55 precision class 1 = 0.500\n",
      "2022-09-30 16:19:55 recall class 0 = 0.938\n",
      "2022-09-30 16:19:55 recall class 1 = 0.409\n",
      "2022-09-30 16:19:55 AUC of ROC = 0.843\n",
      "2022-09-30 16:19:55 AUC of PRC = 0.477\n",
      "2022-09-30 16:19:55 epoch (11) step 50: training loss = 0.25\n",
      "2022-09-30 16:19:56 epoch (11) step 100: training loss = 0.52\n",
      "2022-09-30 16:19:56 epoch (11) step 150: training loss = 0.72\n",
      "2022-09-30 16:19:56 epoch (11) step 200: training loss = 1.23\n",
      "2022-09-30 16:19:57 epoch (11) step 250: training loss = 1.52\n",
      "2022-09-30 16:19:57 epoch (11) step 300: training loss = 1.86\n",
      "2022-09-30 16:19:58 epoch (11) step 350: training loss = 2.19\n",
      "2022-09-30 16:19:58 confusion matrix:\n",
      "2022-09-30 16:19:58 [[559  25]\n",
      " [ 59  29]]\n",
      "2022-09-30 16:19:58 accuracy = 0.875\n",
      "2022-09-30 16:19:58 precision class 0 = 0.905\n",
      "2022-09-30 16:19:58 precision class 1 = 0.537\n",
      "2022-09-30 16:19:58 recall class 0 = 0.957\n",
      "2022-09-30 16:19:58 recall class 1 = 0.330\n",
      "2022-09-30 16:19:58 AUC of ROC = 0.852\n",
      "2022-09-30 16:19:58 AUC of PRC = 0.483\n",
      "2022-09-30 16:19:59 epoch (12) step 50: training loss = 0.15\n",
      "2022-09-30 16:19:59 epoch (12) step 100: training loss = 0.44\n",
      "2022-09-30 16:20:00 epoch (12) step 150: training loss = 0.67\n",
      "2022-09-30 16:20:00 epoch (12) step 200: training loss = 0.95\n",
      "2022-09-30 16:20:01 epoch (12) step 250: training loss = 1.41\n",
      "2022-09-30 16:20:01 epoch (12) step 300: training loss = 1.71\n",
      "2022-09-30 16:20:02 epoch (12) step 350: training loss = 1.95\n",
      "2022-09-30 16:20:02 confusion matrix:\n",
      "2022-09-30 16:20:02 [[553  31]\n",
      " [ 54  34]]\n",
      "2022-09-30 16:20:02 accuracy = 0.874\n",
      "2022-09-30 16:20:02 precision class 0 = 0.911\n",
      "2022-09-30 16:20:02 precision class 1 = 0.523\n",
      "2022-09-30 16:20:02 recall class 0 = 0.947\n",
      "2022-09-30 16:20:02 recall class 1 = 0.386\n",
      "2022-09-30 16:20:02 AUC of ROC = 0.830\n",
      "2022-09-30 16:20:02 AUC of PRC = 0.497\n",
      "2022-09-30 16:20:03 epoch (13) step 50: training loss = 0.26\n",
      "2022-09-30 16:20:03 epoch (13) step 100: training loss = 0.65\n",
      "2022-09-30 16:20:04 epoch (13) step 150: training loss = 0.78\n",
      "2022-09-30 16:20:04 epoch (13) step 200: training loss = 1.09\n",
      "2022-09-30 16:20:05 epoch (13) step 250: training loss = 1.32\n",
      "2022-09-30 16:20:05 epoch (13) step 300: training loss = 1.63\n",
      "2022-09-30 16:20:06 epoch (13) step 350: training loss = 1.92\n",
      "2022-09-30 16:20:06 confusion matrix:\n",
      "2022-09-30 16:20:06 [[548  36]\n",
      " [ 50  38]]\n",
      "2022-09-30 16:20:06 accuracy = 0.872\n",
      "2022-09-30 16:20:06 precision class 0 = 0.916\n",
      "2022-09-30 16:20:06 precision class 1 = 0.514\n",
      "2022-09-30 16:20:06 recall class 0 = 0.938\n",
      "2022-09-30 16:20:06 recall class 1 = 0.432\n",
      "2022-09-30 16:20:06 AUC of ROC = 0.846\n",
      "2022-09-30 16:20:06 AUC of PRC = 0.463\n",
      "2022-09-30 16:20:07 epoch (14) step 50: training loss = 0.44\n",
      "2022-09-30 16:20:08 epoch (14) step 100: training loss = 0.69\n",
      "2022-09-30 16:20:08 epoch (14) step 150: training loss = 1.03\n",
      "2022-09-30 16:20:09 epoch (14) step 200: training loss = 1.39\n",
      "2022-09-30 16:20:09 epoch (14) step 250: training loss = 1.58\n",
      "2022-09-30 16:20:10 epoch (14) step 300: training loss = 1.83\n",
      "2022-09-30 16:20:11 epoch (14) step 350: training loss = 2.05\n",
      "2022-09-30 16:20:11 confusion matrix:\n",
      "2022-09-30 16:20:11 [[547  37]\n",
      " [ 52  36]]\n",
      "2022-09-30 16:20:11 accuracy = 0.868\n",
      "2022-09-30 16:20:11 precision class 0 = 0.913\n",
      "2022-09-30 16:20:11 precision class 1 = 0.493\n",
      "2022-09-30 16:20:11 recall class 0 = 0.937\n",
      "2022-09-30 16:20:11 recall class 1 = 0.409\n",
      "2022-09-30 16:20:11 AUC of ROC = 0.839\n",
      "2022-09-30 16:20:11 AUC of PRC = 0.467\n",
      "2022-09-30 16:20:12 epoch (15) step 50: training loss = 0.28\n",
      "2022-09-30 16:20:12 epoch (15) step 100: training loss = 0.40\n",
      "2022-09-30 16:20:13 epoch (15) step 150: training loss = 0.66\n",
      "2022-09-30 16:20:13 epoch (15) step 200: training loss = 0.87\n",
      "2022-09-30 16:20:14 epoch (15) step 250: training loss = 1.11\n",
      "2022-09-30 16:20:14 epoch (15) step 300: training loss = 1.43\n",
      "2022-09-30 16:20:15 epoch (15) step 350: training loss = 1.69\n",
      "2022-09-30 16:20:15 confusion matrix:\n",
      "2022-09-30 16:20:15 [[537  47]\n",
      " [ 58  30]]\n",
      "2022-09-30 16:20:15 accuracy = 0.844\n",
      "2022-09-30 16:20:15 precision class 0 = 0.903\n",
      "2022-09-30 16:20:15 precision class 1 = 0.390\n",
      "2022-09-30 16:20:15 recall class 0 = 0.920\n",
      "2022-09-30 16:20:15 recall class 1 = 0.341\n",
      "2022-09-30 16:20:15 AUC of ROC = 0.819\n",
      "2022-09-30 16:20:15 AUC of PRC = 0.404\n",
      "2022-09-30 16:20:16 epoch (16) step 50: training loss = 0.23\n",
      "2022-09-30 16:20:16 epoch (16) step 100: training loss = 0.61\n",
      "2022-09-30 16:20:17 epoch (16) step 150: training loss = 0.98\n",
      "2022-09-30 16:20:17 epoch (16) step 200: training loss = 1.27\n",
      "2022-09-30 16:20:18 epoch (16) step 250: training loss = 1.49\n",
      "2022-09-30 16:20:18 epoch (16) step 300: training loss = 1.73\n",
      "2022-09-30 16:20:19 epoch (16) step 350: training loss = 2.03\n",
      "2022-09-30 16:20:19 confusion matrix:\n",
      "2022-09-30 16:20:19 [[535  49]\n",
      " [ 49  39]]\n",
      "2022-09-30 16:20:19 accuracy = 0.854\n",
      "2022-09-30 16:20:19 precision class 0 = 0.916\n",
      "2022-09-30 16:20:19 precision class 1 = 0.443\n",
      "2022-09-30 16:20:19 recall class 0 = 0.916\n",
      "2022-09-30 16:20:19 recall class 1 = 0.443\n",
      "2022-09-30 16:20:19 AUC of ROC = 0.837\n",
      "2022-09-30 16:20:19 AUC of PRC = 0.448\n",
      "2022-09-30 16:20:20 epoch (17) step 50: training loss = 0.33\n",
      "2022-09-30 16:20:20 epoch (17) step 100: training loss = 0.48\n",
      "2022-09-30 16:20:21 epoch (17) step 150: training loss = 0.72\n",
      "2022-09-30 16:20:21 epoch (17) step 200: training loss = 0.84\n",
      "2022-09-30 16:20:22 epoch (17) step 250: training loss = 1.08\n",
      "2022-09-30 16:20:22 epoch (17) step 300: training loss = 1.36\n",
      "2022-09-30 16:20:23 epoch (17) step 350: training loss = 1.66\n",
      "2022-09-30 16:20:23 confusion matrix:\n",
      "2022-09-30 16:20:23 [[564  20]\n",
      " [ 65  23]]\n",
      "2022-09-30 16:20:23 accuracy = 0.874\n",
      "2022-09-30 16:20:23 precision class 0 = 0.897\n",
      "2022-09-30 16:20:23 precision class 1 = 0.535\n",
      "2022-09-30 16:20:23 recall class 0 = 0.966\n",
      "2022-09-30 16:20:23 recall class 1 = 0.261\n",
      "2022-09-30 16:20:23 AUC of ROC = 0.818\n",
      "2022-09-30 16:20:23 AUC of PRC = 0.427\n",
      "2022-09-30 16:20:24 epoch (18) step 50: training loss = 0.20\n",
      "2022-09-30 16:20:24 epoch (18) step 100: training loss = 0.36\n",
      "2022-09-30 16:20:25 epoch (18) step 150: training loss = 0.50\n",
      "2022-09-30 16:20:25 epoch (18) step 200: training loss = 0.60\n",
      "2022-09-30 16:20:26 epoch (18) step 250: training loss = 0.87\n",
      "2022-09-30 16:20:26 epoch (18) step 300: training loss = 1.06\n",
      "2022-09-30 16:20:27 epoch (18) step 350: training loss = 1.34\n",
      "2022-09-30 16:20:27 confusion matrix:\n",
      "2022-09-30 16:20:27 [[540  44]\n",
      " [ 55  33]]\n",
      "2022-09-30 16:20:27 accuracy = 0.853\n",
      "2022-09-30 16:20:27 precision class 0 = 0.908\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-30 16:20:27 precision class 1 = 0.429\n",
      "2022-09-30 16:20:27 recall class 0 = 0.925\n",
      "2022-09-30 16:20:27 recall class 1 = 0.375\n",
      "2022-09-30 16:20:27 AUC of ROC = 0.826\n",
      "2022-09-30 16:20:27 AUC of PRC = 0.451\n",
      "2022-09-30 16:20:28 epoch (19) step 50: training loss = 0.23\n",
      "2022-09-30 16:20:28 epoch (19) step 100: training loss = 0.44\n",
      "2022-09-30 16:20:29 epoch (19) step 150: training loss = 0.68\n",
      "2022-09-30 16:20:29 epoch (19) step 200: training loss = 0.90\n",
      "2022-09-30 16:20:30 epoch (19) step 250: training loss = 1.09\n",
      "2022-09-30 16:20:30 epoch (19) step 300: training loss = 1.23\n",
      "2022-09-30 16:20:31 epoch (19) step 350: training loss = 1.50\n",
      "2022-09-30 16:20:31 confusion matrix:\n",
      "2022-09-30 16:20:31 [[521  63]\n",
      " [ 47  41]]\n",
      "2022-09-30 16:20:31 accuracy = 0.836\n",
      "2022-09-30 16:20:31 precision class 0 = 0.917\n",
      "2022-09-30 16:20:31 precision class 1 = 0.394\n",
      "2022-09-30 16:20:31 recall class 0 = 0.892\n",
      "2022-09-30 16:20:31 recall class 1 = 0.466\n",
      "2022-09-30 16:20:31 AUC of ROC = 0.819\n",
      "2022-09-30 16:20:31 AUC of PRC = 0.433\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gru done\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#execute training \n",
    "#define hyperparameters\n",
    "\n",
    "argsMean = {'dim':16,\n",
    "        'dropout':0.3,\n",
    "        'batch_size':8,\n",
    "        'lr':1e-3,\n",
    "        'epochs':20,\n",
    "        'emb_size':16,\n",
    "        'aggregation_type':'mean', ## 'last_state' , 'mean'\n",
    "        'bidirectional':False, #we are not going to use biRNN \n",
    "        'seed':42,\n",
    "        'steps':50, # print loss every num of steps\n",
    "        'data':'test_text_data_2/in-hospital-mortality', # path to MIMIC data \n",
    "        'notes':'test_text_data_2/train', # code ignores text\n",
    "        'timestep':1.0, # observations every hour\n",
    "        'imputation':'previous', # imputation method\n",
    "        'normalizer_state':None,\n",
    "        'model name': 'meanModel',\n",
    "        'model':'lstm'} # we use normalization config\n",
    "argslastState = {'dim':16,\n",
    "        'dropout':0.3,\n",
    "        'batch_size':8,\n",
    "        'lr':1e-3,\n",
    "        'epochs':20,\n",
    "        'emb_size':16,\n",
    "        'aggregation_type':'last_state', ## 'last_state' , 'mean'\n",
    "        'bidirectional':False, #we are not going to use biRNN \n",
    "        'seed':42,\n",
    "        'steps':50, # print loss every num of steps\n",
    "        'data':'test_text_data_2/in-hospital-mortality', # path to MIMIC data \n",
    "        'notes':'test_text_data_2/train', # code ignores text\n",
    "        'timestep':1.0, # observations every hour\n",
    "        'imputation':'previous', # imputation method\n",
    "        'normalizer_state':None,\n",
    "        'model name': 'last_stateModel',\n",
    "       'model':'lstm'} # we use normalization config\n",
    "argsGru = {'dim':16,\n",
    "        'dropout':0.3,\n",
    "        'batch_size':8,\n",
    "        'lr':1e-3,\n",
    "        'epochs':20,\n",
    "        'emb_size':16,\n",
    "        'aggregation_type':'mean', ## 'last_state' , 'mean'\n",
    "        'bidirectional':False, #we are not going to use biRNN \n",
    "        'seed':42,\n",
    "        'steps':50, # print loss every num of steps\n",
    "        'data':'test_text_data_2/in-hospital-mortality', # path to MIMIC data \n",
    "        'notes':'test_text_data_2/train', # code ignores text\n",
    "        'timestep':1.0, # observations every hour\n",
    "        'imputation':'previous', # imputation method\n",
    "        'normalizer_state':None,\n",
    "        'model name': 'gruModel',\n",
    "        'model':'gru'} # we use normalization config\n",
    "\n",
    "train(argsMean)\n",
    "print('mean done')\n",
    "train(argslastState)\n",
    "print('lastState done')\n",
    "train(argsGru)\n",
    "print('gru done')\n",
    "## input = [batch=8, time=48, features=76]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iYLLKMUtk4zz"
   },
   "source": [
    "## Test\n",
    "\n",
    "Here we use the last trained model or the best validation model   and run in test.\n",
    "\n",
    "In addition to the measures introduced above we also use [calibration curves](https://scikit-learn.org/stable/auto_examples/calibration/plot_calibration_curve.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "G79PLqTHjy9p"
   },
   "outputs": [],
   "source": [
    "#test\n",
    "\n",
    "def test(args):\n",
    "    # define trainning and validation datasets\n",
    "    mode = 'test'\n",
    "    hidden_size = args['dim']\n",
    "    dropout = args['dropout']\n",
    "    batch_size = args['batch_size']\n",
    "    emb_size = args['emb_size']\n",
    "    best_model = args['best_model']\n",
    "    data = args['data']\n",
    "    notes = args['notes']\n",
    "    timestep = args['timestep']\n",
    "    aggregation_type = args['aggregation_type']\n",
    "    modelType = args['model']\n",
    "    bidirectional_encoder = args['bidirectional'] # TODO add into args\n",
    "    device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")   \n",
    "    # 1. Get a unique working directory \n",
    "    \n",
    "    logging.basicConfig(level=logging.INFO, \n",
    "            format='%(asctime)s %(message)s', \n",
    "            datefmt='%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "\n",
    "\n",
    "    test_reader = InHospitalMortalityReader(dataset_dir=os.path.join(data, 'test'),\n",
    "                                           notes_dir=notes,  \n",
    "                                         listfile=os.path.join(data, 'test_listfile.csv'),\n",
    "                                         period_length=48.0)\n",
    "\n",
    "    \n",
    "    discretizer = Discretizer(timestep=float(timestep),\n",
    "                          store_masks=True,\n",
    "                          impute_strategy='previous',\n",
    "                          start_time='zero')\n",
    "\n",
    "    discretizer_header = discretizer.transform(test_reader.read_example(0)[\"X\"])[1].split(',')\n",
    "    cont_channels = [i for (i, x) in enumerate(discretizer_header) if x.find(\"->\") == -1]\n",
    "\n",
    "    normalizer = Normalizer(fields=cont_channels)  # choose here which columns to standardize\n",
    "    normalizer_state = args['normalizer_state']\n",
    "    if normalizer_state is None:\n",
    "        normalizer_state = 'norm_start_time_zero.normalizer'\n",
    "    normalizer.load_params(normalizer_state)\n",
    "\n",
    "    # Read data\n",
    "    test_dataset = MIMICDataset(test_reader, discretizer, normalizer, batch_labels=True)\n",
    "    test_dl = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    #[B, M, feat_size]\n",
    "    feat_size = test_dataset.data.shape[-1] \n",
    "\n",
    "\n",
    "    # Define the classification model.\n",
    "    model = LSTMClassifier(tag_size=1, #binary\n",
    "                    feat_size= feat_size, \n",
    "                    hidden_size=hidden_size,\n",
    "                    emb_size=emb_size,\n",
    "                    bidirectional=bidirectional_encoder,\n",
    "                    dropout=dropout,\n",
    "                    aggregation_type=aggregation_type,\n",
    "                    modelType = modelType)\n",
    "    #load trained model from file\n",
    "    model.load_state_dict(torch.load(best_model))\n",
    "    logging.info(model)\n",
    "    model = model.to(device)\n",
    "\n",
    "    metrics_results, pred_probs, y_true = eval_model(model,\n",
    "                                test_dl,\n",
    "                                device)\n",
    "    return metrics_results, pred_probs, y_true\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "x-e2oIZ8qMP-"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-30 16:12:52 LSTMClassifier(\n",
      "  (encoder_layer): TransformerEncoderLayer(\n",
      "    (self_attn): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=76, out_features=76, bias=True)\n",
      "    )\n",
      "    (linear1): Linear(in_features=76, out_features=2048, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (linear2): Linear(in_features=2048, out_features=76, bias=True)\n",
      "    (norm1): LayerNorm((76,), eps=1e-05, elementwise_affine=True)\n",
      "    (norm2): LayerNorm((76,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout1): Dropout(p=0.1, inplace=False)\n",
      "    (dropout2): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (hidden2out): Linear(in_features=16, out_features=1, bias=True)\n",
      "  (comb): Linear(in_features=16, out_features=16, bias=True)\n",
      "  (fc1): Linear(in_features=76, out_features=76, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (dout): Dropout(p=0.3, inplace=False)\n",
      "  (lstm): LSTM(76, 16, batch_first=True)\n",
      "  (fc2): Linear(in_features=16, out_features=16, bias=True)\n",
      "  (out): Linear(in_features=16, out_features=1, bias=True)\n",
      ")\n",
      "2022-09-30 16:12:52 confusion matrix:\n",
      "2022-09-30 16:12:52 [[576  34]\n",
      " [ 58  15]]\n",
      "2022-09-30 16:12:52 accuracy = 0.865\n",
      "2022-09-30 16:12:52 precision class 0 = 0.909\n",
      "2022-09-30 16:12:52 precision class 1 = 0.306\n",
      "2022-09-30 16:12:52 recall class 0 = 0.944\n",
      "2022-09-30 16:12:52 recall class 1 = 0.205\n",
      "2022-09-30 16:12:52 AUC of ROC = 0.711\n",
      "2022-09-30 16:12:52 AUC of PRC = 0.256\n",
      "2022-09-30 16:12:56 LSTMClassifier(\n",
      "  (encoder_layer): TransformerEncoderLayer(\n",
      "    (self_attn): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=76, out_features=76, bias=True)\n",
      "    )\n",
      "    (linear1): Linear(in_features=76, out_features=2048, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (linear2): Linear(in_features=2048, out_features=76, bias=True)\n",
      "    (norm1): LayerNorm((76,), eps=1e-05, elementwise_affine=True)\n",
      "    (norm2): LayerNorm((76,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout1): Dropout(p=0.1, inplace=False)\n",
      "    (dropout2): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (hidden2out): Linear(in_features=16, out_features=1, bias=True)\n",
      "  (comb): Linear(in_features=16, out_features=16, bias=True)\n",
      "  (fc1): Linear(in_features=76, out_features=76, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (dout): Dropout(p=0.3, inplace=False)\n",
      "  (lstm): LSTM(76, 16, batch_first=True)\n",
      "  (fc2): Linear(in_features=16, out_features=16, bias=True)\n",
      "  (out): Linear(in_features=16, out_features=1, bias=True)\n",
      ")\n",
      "2022-09-30 16:12:56 confusion matrix:\n",
      "2022-09-30 16:12:56 [[575  35]\n",
      " [ 59  14]]\n",
      "2022-09-30 16:12:56 accuracy = 0.862\n",
      "2022-09-30 16:12:56 precision class 0 = 0.907\n",
      "2022-09-30 16:12:56 precision class 1 = 0.286\n",
      "2022-09-30 16:12:56 recall class 0 = 0.943\n",
      "2022-09-30 16:12:56 recall class 1 = 0.192\n",
      "2022-09-30 16:12:56 AUC of ROC = 0.699\n",
      "2022-09-30 16:12:56 AUC of PRC = 0.274\n",
      "2022-09-30 16:12:59 LSTMClassifier(\n",
      "  (encoder_layer): TransformerEncoderLayer(\n",
      "    (self_attn): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=76, out_features=76, bias=True)\n",
      "    )\n",
      "    (linear1): Linear(in_features=76, out_features=2048, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (linear2): Linear(in_features=2048, out_features=76, bias=True)\n",
      "    (norm1): LayerNorm((76,), eps=1e-05, elementwise_affine=True)\n",
      "    (norm2): LayerNorm((76,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout1): Dropout(p=0.1, inplace=False)\n",
      "    (dropout2): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (hidden2out): Linear(in_features=16, out_features=1, bias=True)\n",
      "  (comb): Linear(in_features=16, out_features=16, bias=True)\n",
      "  (fc1): Linear(in_features=76, out_features=76, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (dout): Dropout(p=0.3, inplace=False)\n",
      "  (gru): GRU(76, 16, batch_first=True)\n",
      "  (fc2): Linear(in_features=16, out_features=16, bias=True)\n",
      "  (out): Linear(in_features=16, out_features=1, bias=True)\n",
      ")\n",
      "2022-09-30 16:12:59 confusion matrix:\n",
      "2022-09-30 16:12:59 [[526  84]\n",
      " [ 40  33]]\n",
      "2022-09-30 16:12:59 accuracy = 0.818\n",
      "2022-09-30 16:12:59 precision class 0 = 0.929\n",
      "2022-09-30 16:12:59 precision class 1 = 0.282\n",
      "2022-09-30 16:12:59 recall class 0 = 0.862\n",
      "2022-09-30 16:12:59 recall class 1 = 0.452\n",
      "2022-09-30 16:12:59 AUC of ROC = 0.703\n",
      "2022-09-30 16:12:59 AUC of PRC = 0.301\n"
     ]
    }
   ],
   "source": [
    "# Run test on saved model *.pt\n",
    "# remember to use the same hyperparameters as in training\n",
    "\n",
    "argsMean = {'best_model':'meanModel',\n",
    "        'dim':16,\n",
    "        'dropout':0.3,\n",
    "        'batch_size':16,\n",
    "        'emb_size':16,\n",
    "        'aggregation_type':'mean', ## last_state\n",
    "        'bidirectional':False,\n",
    "        'data':'test_text_data_2/in-hospital-mortality', #path to data\n",
    "        'notes':'test_text_data_2/test', #the code ignores the text\n",
    "        'timestep':1.0,\n",
    "        'imputation':'previous',\n",
    "        'normalizer_state':None,\n",
    "        'model':'lstm'   }\n",
    "argsLastState = {'best_model':'last_stateModel',\n",
    "        'dim':16,\n",
    "        'dropout':0.3,\n",
    "        'batch_size':16,\n",
    "        'emb_size':16,\n",
    "        'aggregation_type':'last_state', ## last_state\n",
    "        'bidirectional':False,\n",
    "        'data':'test_text_data_2/in-hospital-mortality', #path to data\n",
    "        'notes':'test_text_data_2/test', #the code ignores the text\n",
    "        'timestep':1.0,\n",
    "        'imputation':'previous',\n",
    "        'normalizer_state':None,\n",
    "        'model':'lstm'  }\n",
    "argsGRU = {'best_model':'gruModel',\n",
    "        'dim':16,\n",
    "        'dropout':0.3,\n",
    "        'batch_size':16,\n",
    "        'emb_size':16,\n",
    "        'aggregation_type':'last_state', ## last_state\n",
    "        'bidirectional':False,\n",
    "        'data':'test_text_data_2/in-hospital-mortality', #path to data\n",
    "        'notes':'test_text_data_2/test', #the code ignores the text\n",
    "        'timestep':1.0,\n",
    "        'imputation':'previous',\n",
    "        'normalizer_state':None,\n",
    "        'model':'gru'  }\n",
    "metrics_results_mean, pred_probs_mean, y_true_mean = test(argsMean)\n",
    "metrics_results_lastState, pred_probs_lastState, y_true_lastState = test(argsLastState)\n",
    "metrics_results_gru, pred_probs_gru, y_true_gru = test(argsGRU)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CC47QThDyBC-"
   },
   "source": [
    "# Assess predictive performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "_1kDW3kqjgAf"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot roc curve\n",
    "lstm_fpr_mean, lstm_tpr_mean, lstm_thresholds_mean = metrics.roc_curve(y_true_mean, pred_probs_mean)\n",
    "lstm_fpr_last, lstm_tpr_last, lstm_thresholds_last = metrics.roc_curve(y_true_lastState, pred_probs_lastState)\n",
    "lstm_fpr_gru, lstm_tpr_gru, lstm_thresholds_gru = metrics.roc_curve(y_true_gru, pred_probs_gru)\n",
    "\n",
    "\n",
    "# plot the roc curve for the model\n",
    "plt.figure()\n",
    "plt.ylim(0., 1.0)\n",
    "plt.xlim(0.,1.0)\n",
    "plt.plot(lstm_fpr_mean, lstm_tpr_mean, marker='.', label='LSTM Mean', color='darkorange')\n",
    "plt.plot([0, 1], [0, 1], color='navy', linestyle='--')\n",
    "plt.plot(lstm_fpr_last, lstm_tpr_last, marker='.', label='LSTM Last state', color='red')\n",
    "plt.plot(lstm_fpr_gru, lstm_tpr_gru, marker='.', label='GRU model', color='green')\n",
    "# axis labels\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "# show the legend\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "C509pB4Eh7mA"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEKCAYAAAAMzhLIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nO3deZyN9fv48ddlZmyDFGMnaZFlzMSMnUS2hEiRFiGSLOlXH6k+KaovpdJC0qZQfMgyZJlKCJF9G7ssY6eU2Zjl/fvjPjPGmOUYZz/X8/GYxzn3cu77OvfMnOvc9/1+X28xxqCUUkrlpIC7A1BKKeXZNFEopZTKlSYKpZRSudJEoZRSKleaKJRSSuVKE4VSSqlcOS1RiMhXInJaRHbksFxE5CMR2S8i20SkrrNiUUoplX/OPKOYArTLZXl74HbbT3/gUyfGopRSKp+cliiMMSuBv3JZpTPwrbGsBUqKSHlnxaOUUip/At2474rA0UzTsbZ5J7KuKCL9sc46CA4OrnfnnXe6JECllPJ2J07EcfJkHGlpx84aY0Lysw13JgrJZl629USMMZOByQARERFmw4YNzoxLKaW8njEGESEqag/R0QeYMKHD4fxuy52tnmKBypmmKwHH3RSLUkr5hL//TqRv3/m8/fZvAHTqVJ1PPrnvurbpzkQRBTxha/3UEPjHGHPVZSellFL2mTt3FzVrTuSbb7aSnJzmsO067dKTiHwPtABKi0gsMBIIAjDGTAIWAfcB+4EEoLezYlFKKV926lQcgwcvZtasGMLDy/Hjjz2pW9dxbYOcliiMMY/ksdwAzzpiX8nJycTGxpKUlOSIzSkXKly4MJUqVSIoKMjdoSjltY4e/Zcff9zHW2+15MUXGxMUFODQ7bvzZrbDxMbGUrx4capWrYpIdvfIlScyxnDu3DliY2O55ZZb3B2OUl7l8OHzLFiwl0GD6hMRUYEjR56jVKmiTtmXT5TwSEpKolSpUpokvIyIUKpUKT0TVOoapKUZJkz4g9q1P2XEiF84ceICgNOSBPhIogA0SXgp/b0pZb89e85y991TGDRoMU2aVGbHjmcoX7640/frE5eelFLK1yUkJNO06dekpqYxZUpnnngizGVftHzmjMLdihUrdtW8PXv20KJFC8LDw6lRowb9+/dn6dKlhIeHEx4eTrFixahevTrh4eE88cQTLF++HBHhyy+/zNjG5s2bERHGjRt31fZff/11RIT9+/dnzPvggw8QEbRTolK+Ye/ecxhjKFo0iKlTuxAT8yy9eoW79GzcfxNFwhmYeTf8e8RpuxgyZAjDhg1jy5Yt7Nq1i8GDB9O2bVu2bNnCli1biIiIYPr06WzZsoVvv/0WgNDQUGbOnJmxjRkzZhAWFpbjPkJDQ5kxY0bG9OzZs6lZs6bT3pNSyjWSklJ45ZVfqFlzAtOnbwegXbvbKFfu6i+lzua/iWLrJDi2CpYNcdouTpw4QaVKlTKmQ0ND83xNlSpVSEpK4tSpUxhjWLJkCe3bt89x/QceeID58+cDcPDgQW644QZCQi6Xc4mOjqZRo0bUrVuXhx56iLi4OABGjRpFZGQktWvXpn///litlaFFixYMHz6c+vXrc8cdd/Dbb7/l670rpfJv9eojhIdP4u23V/HEE2F06HC7W+Pxz0SRmgybxoNJg8PRcGyNU3YzbNgwWrZsSfv27fnggw84f/68Xa/r1q0bs2bNYs2aNdStW5dChQrluG6JEiWoXLkyO3bs4Pvvv6d79+4Zy86ePcubb77Jzz//zKZNm4iIiOD9998HYNCgQaxfv54dO3aQmJjIwoULM16XkpLCH3/8wfjx43njjTfy+e6VUvkxevQKmjX7mqSkFJYufYyvvurMjTcWcWtM/pko9s+zkgVASiL81M9KGg7Wu3dvdu3axUMPPcTy5ctp2LAhFy9ezPN1Dz/8MLNmzeL777/nkUdy7bcIQI8ePZgxYwbz5s2jS5cuGfPXrl1LTEwMTZo0ITw8nG+++YbDh626YL/++isNGjQgNDSUZcuWsXPnzozXde3aFYB69epx6NCha3zXSqn8SD+rDw8vx+DB9dmxYyBt2tzq5qgs/pko1r0FyRcuT/97GGKmO2VXFSpUoE+fPsyfP5/AwEB27Mh2wL8rlCtXjqCgIH766SdatWqV5/odO3Zk6tSpVKlShRIlSmTMN8bQunXrjHsiMTExfPnllyQlJTFw4EBmz57N9u3b6dev3xV9GdLPYAICAkhJScnHu1ZK2euvvxLp1Wseb765EoCOHavz4YftKVasoJsju8z/EsWpzfD3vivnJcfD8qHWowMtWbKE5GTrzOXkyZOcO3eOihUr2vXaUaNGMXbsWAIC8u6KX6RIEcaOHcsrr7xyxfyGDRuyevXqjFZRCQkJ7N27NyMplC5dmri4OGbPnn0tb0sp5SCzZ8dQo8YEvvtuOybbQRY8g//1o1g/FlKz6QmckgRr34Jmb+drswkJCVfcuH7++eeJjY1l6NChFC5cGIB3332XcuXK2bW9xo0bX9P+e/TocdW8kJAQpkyZwiOPPJJxyevNN9/kjjvuoF+/foSGhlK1alUiIyOvaV9Kqetz4sQFBg1azJw5u6hXrzzR0Y8RFmbfZ4M7iPHkNJaN7AYu2rVrFzVq1LBvA9+EwtkcLv+UrQePaf8DV7um359SPmDjxuPcffcUXnvtbp5/vhGBgc6/uCMiG40xEfl5rf+dUfTa7u4IlFJ+6NCh8yxYsIfBgxtQr14Fjh4d5vbWTPbyv3sUSinlQqmpaXz00Tpq157IK68s4+RJqy+TtyQJ0EShlFJOs2vXGZo3n8LQoUto1uxmduwY6Jae1dfL/y49KaWUCyQkJNO8+RTS0gzffvsAjz1Wx2urJWuiUEopB9q9+yzVq5eiaNEgpk/vSlhYWcqW9b6ziMz00pNSSjlAYmIyw4f/RK1aEzOK+LVpc6vXJwnw10SRmAjffQejR1uPDhhhzV1lxrObn5ctW7awaNEiu9c/f/48EydOdNh6SvmalSsPExY2iXfeWUOfPuHcf/8d7g7JofwvUaxfDxUqwNNPw8iR1mP58tZ8B3NFmfH80EShlOO88cZy7r57Cikpafz88+N8/nknSpYs7O6wHMq/EkViIrRpA+fPQ1wcGGM9nj9vzXfw2M2uKDOe7vPPPycyMpKwsDAefPBBEhISAJg1axa1a9cmLCyM5s2bc+nSJV577TVmzpxJeHj4FUkJYOfOndSvX5/w8HDq1KnDvn37eOmllzhw4ADh4eG8+OKLxMXF0apVK+rWrUtoaGhGmfOs64HVGz0yMpI6deowcuRIu4+dUp4uvbNyREQFhg1ryPbtz9CqVTU3R+Uc/nUze+5cyKnIXUoKzJkDPXs6bHfpZcYbN25MmzZt6N27NyVLlszzdellxu+66648y4yn69q1K/369QPg1Vdf5csvv2Tw4MGMGjWKpUuXUrFiRc6fP0/BggUZNWoUGzZs4JNPPrlqO5MmTWLo0KE8+uijXLp0idTUVMaMGcOOHTvYsmULYJUhnzt3LiVKlODs2bM0bNiQTp06XbVedHQ0+/bt448//sAYQ6dOnVi5ciXNmze/lsOolEc5ezaBYcOWcvvtN/Haa3fTocMddOjgW5easvKvM4oDByA+h8J/8fFw8KBDd+eqMuMAO3bsoFmzZoSGhjJ9+vSMsuFNmjThySef5PPPPyc1NTXP7TRq1Ii3336bsWPHcvjwYYoUubpTkDGGl19+mTp16nDvvfdy7NgxTp06ddV60dHRREdHZyS83bt3s2/fvqvWU8obGGP43/92UrPmBGbM2EGBAt7Z1DU//CtR3HorBAdnvyw4GKo5/rTRFWXGAZ588kk++eQTtm/fzsiRIzMqxE6aNIk333yTo0ePEh4ezrlz53LdTs+ePYmKiqJIkSK0bduWZcuWXbXO9OnTOXPmDBs3bmTLli2ULVv2ijLl6YwxjBgxIuOezP79++nbt69d70cpT3L8+AW6dJlJ9+6zufnmkmzc2J9XX/WfM2P/ShRdu0JgDlfbAgOt5Q7kqjLjABcuXKB8+fIkJyczffrlsTUOHDhAgwYNGDVqFKVLl+bo0aMUL16cCxcuZLudgwcPUq1aNYYMGUKnTp3Ytm3bVev/888/lClThqCgIH799deMwZCyrte2bVu++uqrjOFXjx07xunTp+16P0p5kpMn41i27E/efbc1v//elzp1yro7JJfyr3sUhQtDdLR14zolxbrcFBxsJYnoaGt5Prm7zPjo0aNp0KABN998M6GhoRkf2C+++CL79u3DGEOrVq0ICwujSpUqjBkzhvDwcEaMGHHF8KkzZ85k2rRpBAUFUa5cOV577TVuuukmmjRpQu3atWnfvj3Dhw+nY8eOREREEB4ezp133glAqVKlrljv3XffZdeuXTRq1AiwmhBPmzaNMmXKXNN7U8odDh78m6ioPTz3XEPq1i3PkSPDfK41k738r8w4WK2f5s617klUq2adSVxHklDXR8uMK0+SXsTvlVeWERQUwJ49g7yyPlNWWmb8WhUp4tDWTUop37Bz52n69o1i3bpjdOhwO5Mm3e8TSeJ6+WeiUEqpLBISkrn77imICN9915UePWp7bRE/R9NEoZTyazExZ6hRozRFiwYxY0Y3wsLKEhKSQ+tIP+VfrZ6UUsomISGZF1+MJjT0U6ZN2wbAvfdW0ySRDT2jUEr5neXLD9Gv3wL27/+Lp5+uR6dO1d0dkkfTMwqllF8ZOfJX7rnnG4wxLFv2BJMm3c8NN2irx9zoGYVSyi8YYxAR6tevyP/7f40YNeoeihYNcndYXsGpiUJE2gEfAgHAF8aYMVmW3wBMA6rYYhlnjPnamTGVG1eOU/FX1yUCKBtclpMvnHTavlNSUgjMqWe4UsopzpyJZ+jQJVSvXoqRI1v4RRE/R3Pap5aIBAATgNZALLBeRKKMMTGZVnsWiDHGdBSREGCPiEw3xlxyVlw5JYm8ltlj9OjRTJ8+ncqVK1O6dGnq1avHwoULady4MatXr6ZTp05s376d+++/n27dugFWb+X0EhdKKccxxvD99zsYMmQx//57kTfeaOHukLyWM7/e1gf2G2MOAojIDKAzkDlRGKC4WI2ViwF/ATnUAfdsGzZs4IcffmDz5s2kpKRQt25d6tWrB1gD+qxYsQKwivcppZwrNvZfnnnmRxYu3EuDBhX58stO1KqlpWPyy5mJoiJwNNN0LNAgyzqfAFHAcaA40N0Yk5Z1QyLSH+gP1sA+nmjVqlV07tw5oyx3x44dM5ZlrqWklHK+M2fiWbnyMO+/34YhQxoQEKDtdq6HM49edl0asxaWagtsASoA4cAnIlLiqhcZM9kYE2GMiQgJCXF8pA6QW82s4EylzQMDA0lLS8t4zaVLTrvKppRf2b//Lz744HcA7rqrPEePDmPYsEaaJBzAmUcwFqicaboS1plDZr2BOcayH/gTuNOJMTlN06ZNWbBgAUlJScTFxfHjjz9mu17VqlXZuHEjAPPnz88oQ66Uyp+UlDTGjVtDaOinvPHGCk6dsu75lSiR98iQyj7OTBTrgdtF5BYRKQj0wLrMlNkRoBWAiJQFqgOOHWbORSIjI+nUqRNhYWF07dqViIgIbrjhhqvW69evHytWrKB+/fqsW7fuirMNpdS12b79FI0bf8mLL/5Emza3snPnQMqW1SJ+jubUMuMich8wHqt57FfGmLdEZACAMWaSiFQApgDlsS5VjTHGTMttm9dbZtyZzWPj4uIoVqwYCQkJNG/enMmTJ1O3bt18b89faJlxlR8JCclUqfIBBQoIH3/cnocfrqVF/HLhsWXGjTGLgEVZ5k3K9Pw40MaZMWTlzH4S/fv3JyYmhqSkJHr16qVJQikn2LHjNLVqhVC0aBAzZ3YjLKwcpUsXdXdYPk17fznQd9995+4QlPJZ8fGX+O9/f2X8+LV8880DPP54GK1aOX6ce3U1n0kU6d3zlXfxthEWlXv88stB+vVbwJ9/nmfgwAg6d/bKNi9eyyfajRUuXJhz587ph46XMcZw7ty5jDHFlcrOf/+7jHvvnUpgYAFWrHiSCRM6aIsmF/OJM4pKlSoRGxvLmTNn3B2KukaFCxemUqVK7g5DeaC0NEOBAkLjxpX5z38a8/rrLShSRIv4uYNTWz05Q3atnpRSvuP06XiGDFlM9eqleOONe9wdjs+4nlZPPnHpSSnl/YwxTJu2jRo1JjB37m4tAe5BfOLSk1LKux09+g8DBvzIokX7aNSoEl980YmaNT2zXI8/0kShlHK7c+cSWb36CB9+2I5nn43U+kweRhOFUsot9u49R1TUHl54oTHh4eU4enQYxYtrayZPpGlbKeVSKSlpjB27ijp1PuWtt37LKOKnScJz6RmFUspltm49SZ8+UWzadIIuXe5kwoT7tIifF8gzUYhIE2CLMSZeRB4D6gIfGmMOOz06pZTPSEhIplWrbwkMLMDs2Q/x4IM13R2SspM9l54+BRJEJAz4D3AY+NapUSmlfMa2bacwxlC0aBCzZj1ETMyzmiS8jD2JIsVYvfI6Y51JfIg1bKlSSuUoLu4SQ4cuJjx8ElOnbgPgnntu4aabirg5MnWt7LlHcUFERgCPA81EJADQnjBKqRz99NMB+vdfyKFD5xk0KJIuXbSInzez54yiO3AR6GOMOQlUBN51alRKKa/1yiu/0KbNNAoVCuC333rz8cf3aYsmL5dnorAlhx+A9N/0WWCuM4NSSnmftDSrblzTplUYMaIpW7YMoGnTKm6OSjlCnolCRPoBs4HPbLMqAvOcGZRSynucPBlHt27/4/XXlwPQvv3tvP12KwoX1tb3vsKeS0/PAk2AfwGMMfuAMs4MSinl+YwxTJmyhZo1J7Bw4V4dI8KH2ZPyLxpjLqWPHicigYB31SZXSjnU4cPn6d9/IdHRB2jatApffNGR6tVLuzss5ST2JIoVIvIyUEREWgMDgQXODUsp5cnOn09i/fpjfPJJe555JpICBXQYYl+W58BFIlIA6Au0AQRYCnxh3DTikQ5cpJR77NlzlqioPbz4YhPA6idRrFhBN0el7HU9AxfZc0bRGfjWGPN5fnaglPJuycmpjBu3hjfeWEFwcEF69QqnTJlgTRJ+xJ6b2Z2AvSIyVUQ62O5RKKX8wObNJ2jQ4AtefnkZHTtWJyZmIGXKBLs7LOVieX7oG2N6i0gQ0B7oCUwUkZ+MMU85PTqllNskJCTTuvVUgoIC+OGHh+natYa7Q1JuYtfZgTEmWUQWY7V2KoJ1OUoThVI+aPPmE4SHl6No0SBmz36YsLCy3Hij1mfyZ/Z0uGsnIlOA/UA34AugvJPjUkq52IULFxk0aBF1607OKOLXokVVTRKeKOEMzLwb/j3ikt3Zc0bxJDADeNoYc9G54Sil3GHJkv08/fRCjh79h6FDG+hlJk+3dRIcWwXLhsADzi+UYc89ih5Oj0Ip5TYjRvzMmDGrqVGjNKtX96FRo8ruDknlJjUZNo0HkwaHo+HYGqjY2Km7zDFRiMgqY0xTEbnAlT2xBTDGmBJOjUwp5VSpqWkEBBSgRYuqBAYW4NVXm1OokDZq9Hj751nJAiAlEX7qB722g9jTiDV/ctyyMaap7bG4MaZEpp/imiSU8l4nTlyga9eZGUX82ra9jdGjW2qS8Bbr3oLkC5en/z0MMdOdukt7bmZPtWeeUsqzGWP4+uvN1Kw5kcWL9+tNam90ajP8ve/KecnxsHyo9egk9pyr1Mo8YetwV8854SilnOHQofO0aTONPn2iCA0tw9atA3j++UbuDktdq/VjITXp6vkpSbD2LaftNsdEISIjbPcn6ojIv7afC8ApYL7TIlJKOdw//ySxadMJJk68j+XLn+SOO0q5OySVH+d2Wjexs0pJtG5sO4k9RQH/zxgzIl8bF2kHfAgEYBUSHJPNOi2A8VjjcJ81xtyd2za1KKBS9omJOUNU1B5eeqkpAPHxlwgOdkx9pnLjynEq/lS2y8oGl+XkCycdsh/lOE4pCigidxpjdgOzRKRu1uXGmE15BBUATABaA7HAehGJMsbEZFqnJDARaGeMOSIiOiCSUtfp0qVU3nlnNaNHr6R48YL06XMXZcoEOyxJADkmibyWKe+UWzOH54H+wHvZLDNAyzy2XR/Yb4w5CCAiM7BKf8RkWqcnMMcYcwTAGHPazriVUtnYsOE4fftGsW3bKXr0qM2HH7bTIn7quuWYKIwx/W2P9+Rz2xWBo5mmY4EGWda5AwgSkeVAceBDY8y3WTckIv2xkhZVquhg7UplJz7+Em3bTqNw4UDmz+9Bp07V3R2S8hH2NI99SESK256/KiJzROQuO7ad3ZBXWW+IpLeg6gC0Bf4rIndc9SJjJhtjIowxESEhIXbsWin/sWnTCdLSDMHBBZk7tzs7dw7UJKEcyp7msf81xlwQkaZYH+bfAJPseF0skLkWQCXgeDbrLDHGxBtjzgIrgTA7tq2U3/v334sMHPgj9epNZto0q4hf8+Y3U7JkYTdHpnyNPYki1fbYAfjUGDMfsOeu2HrgdhG5RUQKAj2AqCzrzAeaiUigiBTFujS1y77QlfJfixbto1atiXz22Uaef74hDz6oRfyU89jTZ/+YiHwG3AuMFZFC2JFgjDEpIjIIa4ztAOArY8xOERlgWz7JGLNLRJYA24A0rCa0O/L7ZpTyB8OH/8Q776yhZs0QZs9+iAYNKrk8hrLBZXNtHqt8iz39KIoC7YDtxph9IlIeCDXGOK93Ry60H4XyR8YY0tIMAQEFiI4+wOrVR3j55WZan0nZ7Xr6UeSZKGw7CAOa2SZ/M8Zszc/OHEEThfI3x479y8CBiwgNLcObb+bVKl2p7F1PorCn1dNQYDpQxvYzTUQG52dnSin7GWP4/PON1Kw5kejoA5QuXdTdISk/Zc95a1+ggTEmHkBExgK/Ax87MzCl/Nmff/5N375R/PrrIVq0qMrnn3fktttucndYyk/ZkyiEyy2fsD3Pro+EUspB4uIusW3bKT777H6eeqouBQrov5xyH3sSxdfAOhGZi5UgOgNfOjUqpfzQjh2niYraw8svNyM0tCxHjgyjaNEgd4ellF3NXN8HegN/AeeA3saY8c4OTCl/celSKm+8sZy6dT/jgw/Wcvq0NQCNJgnlKa6lbZ1g9XXQc2ClHGT9+mP06RPFjh2n6dkzlPHj2xISokX8lGfJM1GIyGvAQ8APWEniaxGZZYx509nBKeXL4uMv0a7ddIoUCSQqqgcdO2p9JuWZ7DmjeAS4yxiTBCAiY4BNgCYKpfJhw4bj1K1bnuDggsyf34PQ0DLccIPWZ1Key55aT4eAzH/FhYADTolGKR/2zz9JPP30AiIjP88o4te0aRVNEsrj2XNGcRHYKSI/YZUJbw2sEpGPAIwxQ5wYn1I+YcGCPQwY8CMnT8bxwguN6NatprtDUspu9iSKubafdMudE4pSvunFF6MZN+53QkPLMG9edyIjK7o7JKWuSZ6JwhjzjSsCUcqXGGNITTUEBhagTZtbKVGiEMOHN6VgwQB3h6bUNdPSk0o5WGzsvzzzzI/UqVOGt95qRevWt9K69a3uDkupfLPnZrZSyg5paYbPPttAzZoTWLbsT8qVK+bukJRyiBwThYhMtT0OdV04Snmngwf/pmXLbxgw4Efq16/I9u3PMHhwA3eHpZRD5HbpqZ6I3Az0EZFvydIj2xjzl1MjU8qLxMdfIibmDF980ZE+fe5CRAsYKN+RW6KYBCwBqgEbuTJRGNt8pfzW9u2nmD9/D6++2pzQ0LIcPvwcRYpofSble3K89GSM+cgYUwNrrOtqxphbMv1oklB+6+LFFF577Vfq1p3MRx+tyyjip0lC+Sp7msc+k2Uo1JXGmG3ODUspz7R2bSx9+0YRE3OGxx+vwwcftKVUKR15Tvk2e4ZCHcKVQ6FO16FQlT+Kj79Ehw7fceHCRRYt6sm333bRJKH8gj39KJ5Ch0JVfmzdulgiIysSHFyQBQseITS0DMWLF3J3WEq5jD39KHQoVOWXzp9P4qmnomjY8MuMIn6NG1fWJKH8zrUOhQrwADoUqvJx8+btZuDAHzl9Op7hw5vw0ENaxE/5L3tuZr8vIsuBplhnEr2NMZudHZhS7vL880v54IO1hIWVZcGCR6hXr4K7Q1LKreyq9WSM2YQ1WJFSPilzEb/77rudUqWK8J//NCEoSIv4KaW1npTfO3LkHzp0+I6RI38F4N57q/HKK801SShlo4lC+a20NMPEieupVWsiK1YcpkKF4u4OSSmPpGXGlV/av/8v+vSZz2+/HaF162pMntyRqlVLujss5U0SE2HuXDhwAG69Fbp2hcK+OaxtnolCRLoCY7E624ntxxhjSjg5NqWcJikphb17z/H1153p1StMi/ipa7N+PbRpAykpEB8PwcHw7LMQHQ2Rke6OzuHEGJP7CiL7gY7GmF2uCSl3ERERZsOGDe4OQ3mhLVtOMn/+bkaObAFYyaJwYT2pVtcoMREqVIDz569eVrIknDjhkWcWIrLRGBORn9fac4/ilKckCaXyIykphVde+YWIiMl8+umGjCJ+miRUvsyda51JZCclBebMcW08LmDPf8oGEZkJzAMups80xvje0VA+Z82ao/TtG8Xu3Wfp1SuM999vy003FXF3WMqbHThgXW7KTnw8HDzo2nhcwJ5EUQJIANpkmmcATRTKo8XHX6Jjx+8pVqwgS5Y8Stu2t7k7JOULbr3VuicRF3f1suBgqOZ7ozDkeY/C0+g9CpWX338/SoMGlShQQPj996PUrq1F/JQDJSVB+fJ6jyLLxiuJyFwROS0ip0TkBxGpZGdg7URkj4jsF5GXclkvUkRSRaTbtQSvVGZ//51Inz7zadz4K6ZO3QpAo0ZaxE85WOHCVuumkiWhWDEQsR5LlrTme2CSuF72FgX8DnjINv2YbV7r3F4kIgHABNt6scB6EYkyxsRks95YYOm1ha7UZXPm7OLZZxdx5kw8I0Y0pXv32u4OSfmyyEg4fty6sX3woHW5yZ/7UQAhxpivM01PEZHn7HhdfWC/MeYggIjMADoDMVnWGwz8APhe42PlEsOGLWH8+Ez8V/cAABooSURBVHWEh5dj0aKe3HVXeXeHpPxBkSLQs6e7o3AJexLFWRF5DPjeNv0IcM6O11UEjmaajgUaZF5BRCoCXYCW5JIoRKQ/0B+gSpUqduxa+brMRfzuv/8OypQJ5oUXGmt9JqWcwJ5+FH2Ah4GTwAmgm21eXrLr6pr1zvl4YLgxJjWbdS+/yJjJxpgIY0xESEiIHbtWvuzQofO0azed//53GQCtWlVjxIhmmiSUf0pMhO++g9GjrcekJIfvwp7xKI4AnfKx7VigcqbpSsDxLOtEADNs5RNKA/eJSIoxZl4+9qd8XFqaYcKEPxgx4hdEhC5d7nR3SEq5l4tKieSYKETkP8aYd0TkY64+E8AYMySPba8HbheRW4BjQA/gigt6xphbMu1vCrBQk4TKzr595+jdez6rVx+lXbvbmDSpAzffrEX8lB9LTLSSROZmuul9O9q0cWgz3dzOKNLLduSr04IxJkVEBmG1ZgoAvjLG7BSRAbblk/KzXeWfLl1K5cCBv/n22wd47LE6WsRPKXtKiTjoZnuOicIYs8D2NMEYMyvzMhF5KJuXZLeNRcCiLPOyTRDGmCft2abyH5s3n2D+/D28/noLatUqw6FDQylUSOszKQW4tJSIPTezR9g5TymHSEpKYcSIn4mM/JzPPtvImTPWP4MmCaUySS8lkh0HlxLJ7R5Fe+A+oKKIfJRpUQkgh/Mdpa7PqlVH6Ns3ir17z9G7dzjvvdeGG2/UIn5KXaVrV+vGdXYCA63lDpLbV7TjWPcnOgEbM82/AAxzWARK2cTFXaJz5xmUKFGI6OjHaN36VneHpJTnSi8lkrXVU2Cgw0uJ5HaPYiuwVUTmAvHpfR1sJTe0eI5ymFWrjtC4cWWKFSvIjz/2pHbtMhQrVtDdYSnl+VxUSsSei77RwL1Aek3dIrZ5jR0aiXIdDxnr99y5BIYNW8rUqduYMqUzvXqF07ChXfUmlVLpXFBKxJ5EUdgYk1F43RgTJyJFnRiTciYPGOvXGMPs2TEMGrSYv/5K5L9d/6THfTe4ZN/qGiWcgQXdoP1UKKHlc/yVPa2e4kWkbvqEiNQDEp0XknKazB104uLAGOvx/HlrvhO6/mdn2LClPPzwbCpXLsGGLw2jmkyl0Gq97eWRtk6CY6tgWV79a5Uvs+eM4jlgloikl98oD3R3XkjKaVzYQScrYwwpKWkEBQXQqVN1KlQozvNDIwj8ogKYNDgcDcfWQEW9oukxUpNh03j9/Si7aj2tF5E7gepYhf52G2OSnR6Zr/CQ+wGA28b6/fPPv+nffyH16pVnzJh7adnyFlq2vAX2zLI+jABSEuGnftBrO4g9J7rK6fbP09+PAuy79ARWkqgJ3AU8IiJPOC8kH7J+PVSoAE8/DSNHWo/ly1vz3cGFHXQAUlPT+PDDtdSu/Snr1sVSrdqNV66w7i1IvnB5+t/DEDPdoTGo66C/H2WT55jZIjISaIGVKBYB7YFVxhi3DFvqNWNmJyZaScKTxtV14Vi/e/ee48kn5/H777G0b38bn312P5UrZ7phfWozzGgKKQlXvrDwjdD/KATlkNCUa+jvx+dcz5jZ9tyj6AaEAZuNMb1FpCzwRX525lfceD8gRy7soJOSksbhw/8wbVoXevYMvbqI3/qxkJrNzfOUJFj7FjR722GxqHxw0e+n3LhynIo/le2yssFlOfnCSYfsR10fey49JRpj0oAUESkBnAYce43CF7npfkCe0jvofPYZjBplPZ444ZCmsRs2HM8YTKhmzRAOHhzCo4/mUOn13E7rJmlWKYnWjVPlXi76/eSUJPJaplzLnjOKDSJSEvgcq5RHHPCHU6PyBen3A+Lirl7mhPsB18TBHXQSE5MZOXI57733O+XKFWPIkAaEhATnXsSv13aH7V85gf5+VCa5Jgqxvgr+nzHmPDBJRJYAJYwx21wSnTdzYcEud1qx4hBPPbWA/fv/ol+/urzzTmtKlnRTqy6l/IyrLt3lmiiMMUZE5gH1bNOHHLJXf+DC+wHuEhd3ia5d/0fJkoX55ZcnrCavSimXcdWlO3suPa0VkUhjjJvadHoxFxXscrXffjtMkyZVKFasIIsXP0qtWiEEB3tpET8tUaFUnuxJFPcAA0TkEBCP1enOGGPqODMwn+GCgl2ucvZsAs89t4Tp07dnFPGrX7+iu8O6PplLVDygw7UrlZ3cBi6qYow5gtVvQvkxYwz/+99OBg9ezN9/JzFy5N306FHb3WFdPy1R4XZlg8vmeo1deYbczijmAXWNMYdF5AdjzIOuCkp5lqFDl/Dxx38QGVmBX37pRGioj/wDa4kKt9N+Et4ht0SRufG79pvwM8YYkpPTKFgwgC5d7uTmm2/guecaEhDgQx+iOZWoqPW4+2JSHkM7A16W23+9yeG58nEHDvxFq1bf8uqrVue5e+65hf/3/xr7VpI4tRn+3nflvOR4WD7UelR+zxs6A+Z2ec6Rl+5yO6MIE5F/sc4sitiew+Wb2SUcFoXyCFYRv3W8+uoygoICePTRUHeH5DxaQkT5AFed1eQ2ZnaASyJQHmH37rP06jWPP/44RseOd/Dppx2oWNGHvwvkVaJCE4VSGexpHqv8QFqa4fjxC3z//YN0714r+/pMvkRLVChlN00UfuyPP44xf/5u3nqrFTVrhnDgwBAKFtQTSaXUlXzo7qSyV0JCMi+8EE2jRl/yzTdbOXPGunmrSUIplR1NFH7m11//JDT0U95773f69avLzp0DCQnRQWiUyspVLYq8gV568iNxcZd46KFZlCxZmF9/7UWLFlXdHZLKidagcjt/6ieRFz2j8APLlx8iLc1kFPHbtu0ZTRKeLnMNKqXcTBOFDztzJp5HHvmBe+75hmnTrCFEIiMrUrRokJsjU7nKrgaVUm6kicIHGWP47rvt1KgxgTlzdjF69D2+UcTPX2RXgyq7Ph9KuYgmCh80ePBiHn10DrffXorNm5/m1Veba4smb5JTDSql3ERvZvuItDRDSopVxK9bt5rcdttNDB5c37fqM/mD3GpQ3dEVgvyohVpiojXo14ED1hj0PjDol7dy6qeIiLQTkT0isl9EXspm+aMiss32s0ZEwpwZj6/at+8cLVt+wyuv/AJAixZVfa/Sq7/IqwaVv1i/HipUgKefhpEjrcfy5a35rpRwBmbeDf8ece1+PYzTPklEJACYgDXwUU3gERGpmWW1P4G7baPljQYmOyseX5SSksa4cWuoU2cSW7acpEaNEHeHpK5XXjWo/EFiojXW/PnzEBcHxliP589b85OySaTOoq3PAOdeeqoP7DfGHAQQkRlAZyAmfQVjTObmHGuBSk6Mx6fs2nWGJ56Yx4YNx+ncuToTJ3agQoXi7g5LXS+tQWVdbkpJyX5ZSgrMmeOa4YV1BMQMzrw2URE4mmk61jYvJ32BxdktEJH+IrJBRDacOXPGgSF6t1On4pg5sxtz53bXJKF8x4EDEJ/DmCDx8XDwoGvi0NZnGZx5RpFd+dFsB0ASkXuwEkXT7JYbYyZjuywVERHht4MorV0by/z5u/m//7uXGjWsIn5BQdqayZF0VDMPcOutEBxsXW7KKjgYqrlowE0dATGDM88oYoHKmaYrAcezriQidYAvgM7GmHNOjMdrxcdfYtiwJTRu/CXTp2/PKOKnScLxvGFUM5/XtSsE5vAdNjDQWu5sOgLiFZyZKNYDt4vILSJSEOgBRGVeQUSqAHOAx40xe50Yi9f6+eeD1K79KePHr2PgwEgt4qd8X+HCEB0NJUtCsWIgYj2WLGnNd0UTWW19dgWnXXoyxqSIyCBgKRAAfGWM2SkiA2zLJwGvAaWAibaBclKMMRHOisnbxMVdokeP2dx0UxFWrnySZs1udndISrlGZCQcP27d2D540Lrc5Mp+FDoC4hXEGO+65B8REWE2bNjg7jCcatmyP7n77psJCCjAxo3HqVkzhCJFtD6TK8gbuY/sZ0Z61/+LUulEZGN+v4hrjywPcupUHA8/PItWrb7NKOJXr14FTRJKKbfSEh4ewBjDtGnbeO65pcTFXeKtt1rSs2eou8NSSilAE4VHePbZRXz66QYaNarEl1920h7WblQ2uGyuzWOV8keaKNwkLc2QnJxKoUKBdO9eixo1SjNwYKTWZ3Iz7SeRAy3Q59f0ZrYb7NlzlqeeWkCDBhUZN66Nu8NRKnfr11s1llJSrJ7RwcFWf4boaKt1kvIKejPbSyQnpzJmzCrCwiaxY8dpQkPLuCcQrYip7OVJBfqU2+ilJyfLKAlxOgTmdIWT5aFGDBfvW8TwMx/QC9dd6riqPMXuy/0ytDyFypanFOhTbqWJwskyPpjFQGIReHgm1NxlW5ZNLRtXxHKNy5Qf85QCfcqt9NKTE61ZcxR+uteaCDkLQz7KSBJKeYX0An3ZcWWBPuVWmiicIC7uEkOGLKZp069gR22IL2otCPDPEsXKi3lCgT7ldnrpycGiow/Qv/8Cjhz5h0GD6vNxsQeg0CV3h6VU/qQX6Mup1ZM2kfULmigcKC7uEo8+OodSpYrw22+9adKkCh+/oUlCeTl3F+hTbqeJwgF++ukALVveQrFiBYmOfowaNUIoXFgPrfIhRYpo6yY/pvcorsOJExd48MH/0abNNKZPt8Y6vuuu8lckibIFi+X4eleXhMhtf1qeQimVE/3amw/GGL75ZivDhi0lMTGZMWNaZV/ELzWZkyUKQhIQWAS6/ezWwdm1n4RSKj/0jCIfnnnmR3r3nk/t2mXYunUAw4c3JTAwm0Opg7MrpXyAd59RuLBQWeYifj17hlKnTlkGDIigQIFcBrrRwdmVUj7Ae4sCurBQ2a5dZ3jqqQU0bFiR995ra9+LTm2GGU0hJeHK+YVvhP5HIUjHvVZKuY7/FQV0UaGy5ORU3n77N8LDP2P37rPcdVd5+1+sg7MrpXyEdyYKewqVXaedO09Tv/4XvPLKMjp3rk5MzEAee6yO/RvIa3B2pZTyEt55j8IFhcoCAwvwzz9JzJnzMF261Lj2DfTaft0xKKWUJ/DOM4rcCpUVAm6pmq/N/vbbYV54wfq2X716afbuHZy/JKGUUj7EO29mr1oF5ctT7qnznMqhP9u1jK9w4cJFXnrpZyZO3MAtt5Tkjz/6Ubp0UQdGrZRS7nU9N7O989KTrVDZqUX1c1zF3vEVFi/ex9NPLyQ29l+ee64Bb77ZkuDggo6KVGVx1eBJmejgSUp5Ju9MFGA1gV10fZu4cOEiTzwxjzJlglmzpi8NG1ZyTGwqRzp4klLex3sTRT4ZY1i69ACtW1ejePFC/Pzz49x5Z2kKFfK7Q6GUUnbxzpvZ+XTixAW6dv0f7dtPzyjiFxZWTpOEUkrlwi8+IY0xfP31Fp5/fikXL6byzjv3Zl/ETyml1FX8IlEMGLCQyZM30bz5zXzxRUduv72Uu0NSSimv4dWJomxw2RxvgJYpUo6kpBQKFw7kscfqcNdd5enfv17uRfyUUkpdxasTRU5NKXfuPE3fvlG8fPIX3n+/Lc2a3UyzZje7ODqVndySuw6epJRn8upEkdWlS6mMHbuK0aNXUqJEIYYObeDukFQW2k9CKe/jM4li+/ZTPProHLZvP02PHrX56KN2hIRoKW+llLpePpMoChYMICEhmfnze9CpU3V3h6OUUj7DqxPFihWHiIraw3vvtaV69dLs2TOIgAC/6hriPVw4GqFXxOEtcSmFk4sCikg74EMgAPjCGDMmy3KxLb8PSACeNMZsym2bERERZtmy1Qwf/hOTJm2kWrUbWbfuKS3i58lcOBqhp8eRa62reDg5Ttx3fDxVwhlY0A3aT4USVdwdjde6nqKATksUIhIA7AVaA7HAeuARY0xMpnXuAwZjJYoGwIfGmFzvQN9+e22TlNSX48cv8NxzDRg9uiVFiwY55T0oB0hMhAoVrNEHsypZEk6ccM03Zw+JQ97IvXm2eT3ThCuPjyf7fTT8/jpU6wgPzHN3NF7LU4dCrQ/sN8YcNMZcAmYAnbOs0xn41ljWAiVFJNfxRg8dOs8NNxRizZo+vPdeW00Sns4FoxF6VRzXwlPjcqXUZNg03hot8nA0HFvj7oj8kjPPKLoB7YwxT9mmHwcaGGMGZVpnITDGGLPKNv0LMNwYsyHLtvoD/W2TtYEdTgna+5QGzro7iNxUgvJloUJOy0/B8Vg44YBd5XosXBhH7spTL7fF9bJEkM+4PP7vwl6linJjlRupWkCsL7UXU0jacZKd17AJnzkWDlDdGFM8Py905s3s7M6xs2Yle9bBGDMZmAwgIhvye/rka/RYXKbH4jI9FpfpsbhMRDbkvVb2nHnpKRaonGm6EnA8H+sopZRyI2cmivXA7SJyi4gUBHoAUVnWiQKeEEtD4B9jjPNP/5VSStnNaZeejDEpIjIIWIrVPPYrY8xOERlgWz4Ja4y6+4D9WM1je9ux6clOCtkb6bG4TI/FZXosLtNjcVm+j4VT+1EopZTyftqNWSmlVK40USillMqVxyYKEWknIntEZL+IvJTNchGRj2zLt4lIXXfE6Qp2HItHbcdgm4isEZEwd8TpCnkdi0zrRYpIqq0/j0+y51iISAsR2SIiO0VkhatjdBU7/kduEJEFIrLVdizsuR/qdUTkKxE5LSLZ9jXL9+emMcbjfrBufh8AqgEFga1AzSzr3AcsxuqL0RBY5+643XgsGgM32p639+djkWm9ZViNJbq5O243/l2UBGKAKrbpMu6O243H4mVgrO15CPAXUNDdsTvhWDQH6gI7clier89NTz2jcEr5Dy+V57Ewxqwxxvxtm1yL1R/FF9nzdwFW/bAfgNOuDM7F7DkWPYE5xpgjAMYYXz0e9hwLAxS3FSIthpUocqjp4r2MMSux3ltO8vW56amJoiJwNNN0rG3eta7jC671ffbF+sbgi/I8FiJSEegCTHJhXO5gz9/FHcCNIrJcRDaKyBMui8617DkWnwA1sDr0bgeGGmPSXBOeR8nX56anjkfhsPIfPsDu9yki92AliqZOjch97DkW47HqhaVaXx59lj3HIhCoB7QCigC/i8haY8xeZwfnYvYci7bAFqAlcCvwk4j8Zoz519nBeZh8fW56aqLQ8h+X2fU+RaQO8AXQ3hhzzkWxuZo9xyICmGFLEqWB+0QkxRjja/Wp7f0fOWuMiQfiRWQlEIZV/t+X2HMsemMVIDXAfhH5E7gT+MM1IXqMfH1ueuqlJy3/cVmex0JEqgBzgMd98NtiZnkeC2PMLcaYqsaYqsBsYKAPJgmw739kPtBMRAJFpCjWmC+7XBynK9hzLI5gnVkhImWB6sBBl0bpGfL1uemRZxTGeeU/vI6dx+I1oBQw0fZNOsX4YMVMO4+FX7DnWBhjdonIEmAbkIY1yqTPlei38+9iNDBFRLZjXX4ZbozxufLjIvI90AIoLSKxwEggCK7vc1NLeCillMqVp156Ukop5SE0USillMqVJgqllFK50kShlFIqV5oolFJK5UoThXILETEiMjXTdKCInBGRhe6M61qJyCERKW17viaPdZ8UkQrXuP2qOVUCdcd2lH/SRKHcJR6oLSJFbNOtgWNujCeDiOSrf5ExpnEeqzwJXFOiUMoTaKJQ7rQY6GB7/gjwffoCEQm21dZfLyKbRaSzbX5VEflNRDbZfhrb5rewFb+bLSK7RWS6ZFPsybbOeLHG7dghIvVt818XkckiEg18KyIhIvKDbf/rRaSJbb1SIhJti+kzMtXOEZG4TM//IyLbbeMfjBFrXIwIYLpY40MUEZF6IrLCVrBvaXoVT9v8rSLyO/BsdgdORGaKyH2ZpqeIyIM5HZ8sr31SRD7JNL1QRFrYnrcRkd9tr50lIsVy/Q0q/+Du+un6458/QBxQB6vMRmGsgm0tgIW25W8Dj9mel8SqTxQMFAUK2+bfDmywPW8B/INVu6YA8DvQNJv9Lgc+tz1vjq1uP/A6sBEoYpv+Lv31QBVgl+35R8BrtucdsAqqlU5/T7bH9sAaoKht+qZM+46wPQ+yrRNim+6O1aMYrJ7Ud9uev0s2YwtgVcj9xva8IFZF0CK5HJ+qmd7rk8Anmba10Hb8SgMrgWDb/OHp71V//PvHI0t4KP9gjNkmIlWxziYWZVncBugkIi/YpgtjfWAfBz4RkXAgFauUdro/jDGxACKyBevDcVU2u/7etv+VIlJCREra5kcZYxJtz+8FamY6KSkhIsWxkktX2+t/FJG/udq9wNfGmATbetmND1AdqI1VxRSs0hMnROQGoKQxJn00uqlYiSerxcBHIlIIaAesNMYk2l6f0/HJS0OgJrDaFlNBrISr/JwmCuVuUcA4rG+0pTLNF+BBY8yezCuLyOvAKawqqAWApEyLL2Z6nkrOf99Z69akT8dnmlcAaJQpcaTvP7vXZyV2rrPTGNMoy/ZL2vFajDFJIrIcq3x2dy5fthtGzscnXQpXXnYunCmmn4wxj+S1f+Vf9B6FcrevgFHGmO1Z5i8FBqffZxCRu2zzbwBOGGvQmcexvolfq+62bTbFqp75TzbrRAOD0ids39DBujTzqG1ee+DGHF7bR6yKrYjITbb5F4Ditud7gBARaWRbJ0hEahljzgP/2GIjfV85mIFV1K0Z1vEC+47PISBcRAqISGWsEeLAGh2xiYjcZoupqIhcyxmJ8lGaKJRbGWNijTEfZrNoNNZ1/G22Zp2jbfMnAr1EZC3WZZX4bF6bl79tTVknYQ30lJ0hQIRYA9DHAANs898AmovIJqzLY0eyeU9LsM6UNtgugaVfPpsCTLLNCwC6AWNFZCvWPZr0G8+9gQm2m9lXnNFkEY11KexnYw0BCvYdn9XAn1gjvY0DNtniPoN1/+J7EdmGlTjuzGX/yk9o9VjlV2yXa14wxmxwdyxKeQs9o1BKKZUrPaNQSimVKz2jUEoplStNFEoppXKliUIppVSuNFEopZTKlSYKpZRSufr/6yUUf5eDFxsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot calibration curve\n",
    "lstm_y_mean, lstm_x_mean = calibration_curve(y_true_mean, pred_probs_mean, n_bins=10)\n",
    "lstm_y_last, lstm_x_last = calibration_curve(y_true_lastState, pred_probs_lastState, n_bins=10)\n",
    "lstm_y_gru, lstm_x_gru = calibration_curve(y_true_gru, pred_probs_gru, n_bins=10)\n",
    "plt.figure()\n",
    "plt.ylim(0., 1.0)\n",
    "plt.xlim(0.,1.0)\n",
    "# only these two lines are calibration curves\n",
    "plt.plot(lstm_x_mean,lstm_y_mean, marker='^', linestyle=\"\", markersize=7, label='LSTM Mean', color='darkorange')\n",
    "plt.plot(lstm_x_last,lstm_y_last, marker='o', linestyle=\"\", markersize=7, label='LSTM last state', color='red')\n",
    "plt.plot(lstm_x_gru,lstm_y_gru, marker='s', linestyle=\"\", markersize=7, label='gru', color='green')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], color='navy', linestyle='--')\n",
    "\n",
    "plt.xlabel('Mean predicted value')\n",
    "plt.ylabel('Fraction of positives')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the last hidden state of the LSTM as aggregation type instead of the mean\n",
    "Compare AUC-ROC scores on the test data between LSTM with mean and last hidden state.\n",
    "What do you observe when you use the last hidden state instead of the mean of all the outputs of the LSTM? Which one is better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Judging by the auc and AUC-ROC scores we consider the means model to be better./n What do I notice? hmm\n"
     ]
    }
   ],
   "source": [
    "print('Judging by the auc and AUC-ROC scores we consider the means model to be better.'\n",
    "     '/n What do I notice? hmm')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
